From 900a5511113d9d703b528a4e0392336ef8862b6a Mon Sep 17 00:00:00 2001
From: wuxingyi <wuxy158@chinaunicom.cn>
Date: Tue, 24 Oct 2023 17:42:09 +0800
Subject: [PATCH] add spdk backed rdma rpc and add related bdev rpc
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Signed-off-by: 吴兴义 <wuxy158@chinaunicom.cn>
Signed-off-by: 束桢毅 <shuzy5@chinaunicom.cn>
Signed-off-by: 孙逸方 <sunyf731@chinaunicom.cn>
Signed-off-by: 唐航   <tangh97@chinaunicom.cn>
Signed-off-by: 黄保印 <chenj789@chinaunicom.cn>
Signed-off-by: 陈积   <huangby20@chinaunicom.cn>
---
 include/spdk/event.h                |   12 +
 include/spdk/rdma_client.h          | 3814 ++++++++++++++++++++
 include/spdk/rdma_common.h          |  269 ++
 include/spdk/rdma_server.h          |  799 +++++
 include/spdk_internal/rdma.h        |    8 +
 include/spdk_internal/rdma_client.h | 1952 +++++++++++
 include/spdk_internal/rdma_server.h |   51 +
 lib/Makefile                        |    2 +-
 lib/rdma_server/Makefile            |   62 +
 lib/rdma_server/client.c            | 2296 ++++++++++++
 lib/rdma_server/cmd.c               | 1277 +++++++
 lib/rdma_server/conn.c              | 1122 ++++++
 lib/rdma_server/rdma_c.c            | 3631 +++++++++++++++++++
 lib/rdma_server/rdma_s.c            | 5030 +++++++++++++++++++++++++++
 lib/rdma_server/server.c            |  836 +++++
 lib/rdma_server/transport_c.c       |  764 ++++
 lib/rdma_server/transport_s.c       |  900 +++++
 lib/rdma_server/utils.c             |  487 +++
 python/spdk/rpc/bdev.py             |   85 +
 scripts/rpc.py                      |   44 +
 20 files changed, 23440 insertions(+), 1 deletion(-)
 create mode 100644 include/spdk/rdma_client.h
 create mode 100644 include/spdk/rdma_common.h
 create mode 100644 include/spdk/rdma_server.h
 create mode 100644 include/spdk_internal/rdma_client.h
 create mode 100644 include/spdk_internal/rdma_server.h
 create mode 100644 lib/rdma_server/Makefile
 create mode 100644 lib/rdma_server/client.c
 create mode 100644 lib/rdma_server/cmd.c
 create mode 100644 lib/rdma_server/conn.c
 create mode 100644 lib/rdma_server/rdma_c.c
 create mode 100644 lib/rdma_server/rdma_s.c
 create mode 100644 lib/rdma_server/server.c
 create mode 100644 lib/rdma_server/transport_c.c
 create mode 100644 lib/rdma_server/transport_s.c
 create mode 100644 lib/rdma_server/utils.c

diff --git a/include/spdk/event.h b/include/spdk/event.h
index 444dea778..749313382 100644
--- a/include/spdk/event.h
+++ b/include/spdk/event.h
@@ -316,6 +316,18 @@ void spdk_framework_enable_context_switch_monitor(bool enabled);
  */
 bool spdk_framework_context_switch_monitor_enabled(void);
 
+// void app_start_rpc(int rc, void *arg1);
+
+// void spdk_app_set_start_fn(spdk_msg_fn start_fn, void *arg1);
+
+// int app_setup_trace(struct spdk_app_opts *opts);
+
+// int app_setup_signal_handlers(struct spdk_app_opts *opts);
+
+// void spdk_app_set_g_spdk_app(struct spdk_app_opts *opts);
+
+// void spdk_app_set_g_spdk_thread(struct spdk_thread *thread);
+
 #ifdef __cplusplus
 }
 #endif
diff --git a/include/spdk/rdma_client.h b/include/spdk/rdma_client.h
new file mode 100644
index 000000000..dd12b0d25
--- /dev/null
+++ b/include/spdk/rdma_client.h
@@ -0,0 +1,3814 @@
+/*-
+ *   BSD LICENSE
+ *
+ *   Copyright (c) Intel Corporation. All rights reserved.
+ *   Copyright (c) 2019-2021 Mellanox Technologies LTD. All rights reserved.
+ *   Copyright (c) 2021, 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ *
+ *   Redistribution and use in source and binary forms, with or without
+ *   modification, are permitted provided that the following conditions
+ *   are met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in
+ *       the documentation and/or other materials provided with the
+ *       distribution.
+ *     * Neither the name of Intel Corporation nor the names of its
+ *       contributors may be used to endorse or promote products derived
+ *       from this software without specific prior written permission.
+ *
+ *   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ *   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ *   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ *   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ *   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ *   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ *   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ *   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ *   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ *   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ *   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/** \file
+ * Client driver public API
+ */
+
+#ifndef SPDK_RDMA_CLIENT_H
+#define SPDK_RDMA_CLIENT_H
+
+#include "spdk/stdinc.h"
+
+#ifdef __cplusplus
+extern "C"
+{
+#endif
+
+#include "spdk/env.h"
+#include "spdk/rdma_common.h"
+#define SPDK_CLIENT_TRANSPORT_NAME_FC "FC"
+#define SPDK_CLIENT_TRANSPORT_NAME_PCIE "PCIE"
+#define SPDK_CLIENT_TRANSPORT_NAME_RDMA "RDMA"
+#define SPDK_CLIENT_TRANSPORT_NAME_TCP "TCP"
+#define SPDK_CLIENT_TRANSPORT_NAME_VFIOUSER "VFIOUSER"
+#define SPDK_CLIENT_TRANSPORT_NAME_CUSTOM "CUSTOM"
+
+#define SPDK_SRV_PRIORITY_MAX_LEN 4
+#define SPDK_SRV_MEMORY_POOL_ELEMENT_SIZE 4096
+
+	/**
+	 * Opaque handle to a controller. Returned by spdk_client_probe()'s attach_cb.
+	 */
+	struct spdk_client_ctrlr;
+
+	struct spdk_memory_domain;
+
+	struct rpc_response
+	{
+		struct iovec *iovs; /* array of iovecs to transfer. */
+		int iovcnt;			/* Number of iovecs in iovs array. */
+		uint32_t length;
+	};
+
+	typedef void (*spdk_rpc_request_cb)(void *cb_args, int status, struct iovec *iovs, int iovcnt, int length);
+
+	struct rpc_request
+	{
+		struct spdk_client_qpair *qpair;
+		spdk_rpc_request_cb cb;
+		void *cb_args;
+		char *raw_data;			 // SPDK_CLIENT_SUBMIT_CONTING use this field
+		struct iovec *raw_ioves; // SPDK_CLIENT_SUBMIT_IOVES use this field
+		int raw_iov_cnt;		 // SPDK_CLIENT_SUBMIT_IOVES use this field
+		int submit_type;		 // SPDK_CLIENT_SUBMIT_CONTING or SPDK_CLIENT_SUBMIT_IOVES
+		uint32_t out_length;
+		uint32_t out_payload_length;
+		struct iovec *out_iovs; /* array of iovecs to transfer. */
+		int out_iovcnt;			/* Number of iovecs in iovs array. */
+		int iovpos;				/* Current iovec position. */
+		uint32_t iov_offset;	/* Offset in current iovec. */
+		uint32_t opc;
+		uint32_t in_length;
+		struct iovec *in_iovs;
+		int in_iovcnt;
+		uint32_t in_payload_length;
+		bool check_md5;
+		uint8_t md5sum[SPDK_MD5DIGEST_LEN];
+		uint64_t tsc_last;
+		// uint32_t rpc_receied_index;
+		STAILQ_ENTRY(rpc_request)
+		stailq;
+		uint32_t request_id;
+	};
+
+	struct spdk_client_ctrlr_list
+	{
+		uint16_t ctrlr_count;
+		uint16_t ctrlr_list[2047];
+	};
+	SPDK_STATIC_ASSERT(sizeof(struct spdk_client_ctrlr_list) == 4096, "Incorrect size");
+
+	struct client_poll_group
+	{
+		struct spdk_client_poll_group *group;
+		struct spdk_poller *poller;
+		struct spdk_client_ctrlr *ctrlr;
+	};
+
+	enum spdk_client_cc_css
+	{
+		SPDK_CLIENT_CC_CSS_NVM = 0x0,  /**< NVM command set */
+		SPDK_CLIENT_CC_CSS_IOCS = 0x6, /**< One or more I/O command sets */
+		SPDK_CLIENT_CC_CSS_NOIO = 0x7, /**< No I/O, only admin */
+	};
+
+#define SPDK_CLIENT_CAP_CSS_NVM (1u << SPDK_CLIENT_CC_CSS_NVM)	 /**< NVM command set supported */
+#define SPDK_CLIENT_CAP_CSS_IOCS (1u << SPDK_CLIENT_CC_CSS_IOCS) /**< One or more I/O Command sets supported */
+#define SPDK_CLIENT_CAP_CSS_NOIO (1u << SPDK_CLIENT_CC_CSS_NOIO) /**< No I/O, only admin */
+
+	struct spdk_client_format
+	{
+		uint32_t lbaf : 4;
+		uint32_t ms : 1;
+		uint32_t pi : 3;
+		uint32_t pil : 1;
+		uint32_t ses : 3;
+		uint32_t reserved : 20;
+	};
+	SPDK_STATIC_ASSERT(sizeof(struct spdk_client_format) == 4, "Incorrect size");
+
+	/**
+	 * Arbitration Mechanism Selected to the controller.
+	 *
+	 * Value 0x2 to 0x6 is reserved.
+	 */
+	enum spdk_client_cc_ams
+	{
+		SPDK_CLIENT_CC_AMS_RR = 0x0,  /**< default round robin */
+		SPDK_CLIENT_CC_AMS_WRR = 0x1, /**< weighted round robin */
+		SPDK_CLIENT_CC_AMS_VS = 0x7,  /**< vendor specific */
+	};
+
+	/**
+	 * Client controller initialization options.
+	 *
+	 * A pointer to this structure will be provided for each probe callback from spdk_client_probe() to
+	 * allow the user to request non-default options, and the actual options enabled on the controller
+	 * will be provided during the attach callback.
+	 */
+	struct spdk_client_ctrlr_opts
+	{
+		/**
+		 * Number of I/O queues to request (used to set Number of Queues feature)
+		 */
+		uint32_t num_io_queues;
+
+		/**
+		 * Enable submission queue in controller memory buffer
+		 */
+		bool use_cmb_sqs;
+
+		/**
+		 * Don't initiate shutdown processing
+		 */
+		bool no_shn_notification;
+
+		/**
+		 * Type of arbitration mechanism
+		 */
+		enum spdk_client_cc_ams arb_mechanism;
+
+		/**
+		 * Maximum number of commands that the controller may launch at one time.  The
+		 * value is expressed as a power of two, valid values are from 0-7, and 7 means
+		 * unlimited.
+		 */
+		uint8_t arbitration_burst;
+
+		/**
+		 * Number of commands that may be executed from the low priority queue in each
+		 * arbitration round.  This field is only valid when arb_mechanism is set to
+		 * SPDK_CLIENT_CC_AMS_WRR (weighted round robin).
+		 */
+		uint8_t low_priority_weight;
+
+		/**
+		 * Number of commands that may be executed from the medium priority queue in each
+		 * arbitration round.  This field is only valid when arb_mechanism is set to
+		 * SPDK_CLIENT_CC_AMS_WRR (weighted round robin).
+		 */
+		uint8_t medium_priority_weight;
+
+		/**
+		 * Number of commands that may be executed from the high priority queue in each
+		 * arbitration round.  This field is only valid when arb_mechanism is set to
+		 * SPDK_CLIENT_CC_AMS_WRR (weighted round robin).
+		 */
+		uint8_t high_priority_weight;
+
+		/**
+		 * Keep alive timeout in milliseconds (0 = disabled).
+		 *
+		 * The Client library will set the Keep Alive Timer feature to this value and automatically
+		 * send Keep Alive commands as needed.  The library user must call
+		 * spdk_client_ctrlr_process_admin_completions() periodically to ensure Keep Alive commands
+		 * are sent.
+		 */
+		uint32_t keep_alive_timeout_ms;
+
+		/**
+		 * Specify the retry number when there is issue with the transport
+		 */
+		uint8_t transport_retry_count;
+
+		/**
+		 * The queue depth of each Client I/O queue.
+		 */
+		uint32_t io_queue_size;
+
+		/**
+		 * The host NQN to use when connecting to Client over Fabrics controllers.
+		 *
+		 * If empty, a default value will be used.
+		 */
+		// char hostnqn[SPDK_SRV_NQN_MAX_LEN + 1];
+
+		/**
+		 * The number of requests to allocate for each Client I/O queue.
+		 *
+		 * This should be at least as large as io_queue_size.
+		 *
+		 * A single I/O may allocate more than one request, since splitting may be necessary to
+		 * conform to the device's maximum transfer size, PRP list compatibility requirements,
+		 * or driver-assisted striping.
+		 */
+		uint32_t io_queue_requests;
+
+		/**
+		 * Source address for Client-oF connections.
+		 * Set src_addr and src_svcid to empty strings if no source address should be
+		 * specified.
+		 */
+		char src_addr[SPDK_SRV_TRADDR_MAX_LEN + 1];
+
+		/**
+		 * Source service ID (port) for Client-oF connections.
+		 * Set src_addr and src_svcid to empty strings if no source address should be
+		 * specified.
+		 */
+		char src_svcid[SPDK_SRV_TRSVCID_MAX_LEN + 1];
+
+		/**
+		 * The host identifier to use when connecting to controllers with 64-bit host ID support.
+		 *
+		 * Set to all zeroes to specify that no host ID should be provided to the controller.
+		 */
+		uint8_t host_id[8];
+
+		/**
+		 * The host identifier to use when connecting to controllers with extended (128-bit) host ID support.
+		 *
+		 * Set to all zeroes to specify that no host ID should be provided to the controller.
+		 */
+		uint8_t extended_host_id[16];
+
+		/**
+		 * The I/O command set to select.
+		 *
+		 * If the requested command set is not supported, the controller
+		 * initialization process will not proceed. By default, the NVM
+		 * command set is used.
+		 */
+		enum spdk_client_cc_css command_set;
+
+		/**
+		 * Admin commands timeout in milliseconds (0 = no timeout).
+		 *
+		 * The timeout value is used for admin commands submitted internally
+		 * by the client driver during initialization, before the user is able
+		 * to call spdk_client_ctrlr_register_timeout_callback(). By default,
+		 * this is set to 120 seconds, users can change it in the probing
+		 * callback.
+		 */
+		uint32_t admin_timeout_ms;
+
+		/**
+		 * It is used for TCP transport.
+		 *
+		 * Set to true, means having header digest for the header in the Client/TCP PDU
+		 */
+		bool header_digest;
+
+		/**
+		 * It is used for TCP transport.
+		 *
+		 * Set to true, means having data digest for the data in the Client/TCP PDU
+		 */
+		bool data_digest;
+
+		/**
+		 * Disable logging of requests that are completed with error status.
+		 *
+		 * Defaults to 'false' (errors are logged).
+		 */
+		bool disable_error_logging;
+
+		/**
+		 * It is used for RDMA transport
+		 * Specify the transport ACK timeout. The value should be in range 0-31 where 0 means
+		 * use driver-specific default value. The value is applied to each RDMA qpair
+		 * and affects the time that qpair waits for transport layer acknowledgement
+		 * until it retransmits a packet. The value should be chosen empirically
+		 * to meet the needs of a particular application. A low value means less time
+		 * the qpair waits for ACK which can increase the number of retransmissions.
+		 * A large value can increase the time the connection is closed.
+		 * The value of ACK timeout is calculated according to the formula
+		 * 4.096 * 2^(transport_ack_timeout) usec.
+		 */
+		uint8_t transport_ack_timeout;
+
+		/**
+		 * The queue depth of Client Admin queue.
+		 */
+		uint16_t admin_queue_size;
+
+		/**
+		 * The size of spdk_client_ctrlr_opts according to the caller of this library is used for ABI
+		 * compatibility.  The library uses this field to know how many fields in this
+		 * structure are valid. And the library will populate any remaining fields with default values.
+		 */
+		size_t opts_size;
+
+		/**
+		 * The amount of time to spend before timing out during fabric connect on qpairs associated with
+		 * this controller in microseconds.
+		 */
+		uint64_t fabrics_connect_timeout_us;
+
+		/**
+		 * Disable reading ANA log page. The upper layer should reading ANA log page instead
+		 * if set to true.
+		 *
+		 * Default is `false` (ANA log page is read).
+		 */
+		bool disable_read_ana_log_page;
+
+		uint32_t sector_size;
+
+		// original ns fields
+		/*
+		 * Size of data transferred as part of each block,
+		 * including metadata if FLBAS indicates the metadata is transferred
+		 * as part of the data buffer at the end of each LBA.
+		 */
+		uint32_t extended_lba_size;
+		uint32_t md_size;
+		//	uint32_t			pi_type;
+		uint32_t sectors_per_max_io;
+		//	uint32_t			sectors_per_max_io_no_md;
+		uint32_t sectors_per_stripe;
+	};
+
+	/**
+	 * Client acceleration operation callback.
+	 *
+	 * \param cb_arg The user provided arg which is passed to the corresponding accelerated function call
+	 * defined in struct spdk_client_accel_fn_table.
+	 * \param status 0 if it completed successfully, or negative errno if it failed.
+	 */
+	typedef void (*spdk_client_accel_completion_cb)(void *cb_arg, int status);
+
+	/**
+	 * Function table for the Client accelerator device.
+	 *
+	 * This table provides a set of APIs to allow user to leverage
+	 * accelerator functions.
+	 */
+	struct spdk_client_accel_fn_table
+	{
+		/**
+		 * The size of spdk_client_accel_fun_table according to the caller of
+		 * this library is used for ABI compatibility.  The library uses this
+		 * field to know how many fields in this structure are valid.
+		 * And the library will populate any remaining fields with default values.
+		 * Newly added fields should be put at the end of the struct.
+		 */
+		size_t table_size;
+
+		/** The accelerated crc32c function. */
+		void (*submit_accel_crc32c)(void *ctx, uint32_t *dst, struct iovec *iov,
+									uint32_t iov_cnt, uint32_t seed, spdk_client_accel_completion_cb cb_fn, void *cb_arg);
+	};
+
+	/**
+	 * Indicate whether a ctrlr handle is associated with a Discovery controller.
+	 *
+	 * \param ctrlr Opaque handle to Client controller.
+	 *
+	 * \return true if a discovery controller, else false.
+	 */
+	bool spdk_client_ctrlr_is_discovery(struct spdk_client_ctrlr *ctrlr);
+
+	/**
+	 * Indicate whether a ctrlr handle is associated with a fabrics controller.
+	 *
+	 * \param ctrlr Opaque handle to Client controller.
+	 *
+	 * \return true if a fabrics controller, else false.
+	 */
+	bool spdk_client_ctrlr_is_fabrics(struct spdk_client_ctrlr *ctrlr);
+
+	/**
+	 * Get the default options for the creation of a specific Client controller.
+	 *
+	 * \param[out] opts Will be filled with the default option.
+	 * \param opts_size Must be set to sizeof(struct spdk_client_ctrlr_opts).
+	 */
+	void spdk_client_ctrlr_get_default_ctrlr_opts(struct spdk_client_ctrlr_opts *opts,
+												  size_t opts_size);
+
+	/*
+	 * Get the options in use for a given controller.
+	 *
+	 * \param ctrlr Opaque handle to Client controller.
+	 */
+	const struct spdk_client_ctrlr_opts *spdk_client_ctrlr_get_opts(struct spdk_client_ctrlr *ctrlr);
+
+	/**
+	 * Reason for qpair disconnect at the transport layer.
+	 *
+	 * NONE implies that the qpair is still connected while UNKNOWN means that the
+	 * qpair is disconnected, but the cause was not apparent.
+	 */
+	enum spdk_client_qp_failure_reason
+	{
+		SPDK_CLIENT_QPAIR_FAILURE_NONE = 0,
+		SPDK_CLIENT_QPAIR_FAILURE_LOCAL,
+		SPDK_CLIENT_QPAIR_FAILURE_REMOTE,
+		SPDK_CLIENT_QPAIR_FAILURE_UNKNOWN,
+	};
+
+	typedef enum spdk_client_qp_failure_reason spdk_client_qp_failure_reason;
+
+	/**
+	 * Client library transports
+	 *
+	 * NOTE: These are mapped directly to the Client over Fabrics TRTYPE values, except for PCIe,
+	 * which is a special case since Client over Fabrics does not define a TRTYPE for local PCIe.
+	 *
+	 * Currently, this uses 256 for PCIe which is intentionally outside of the 8-bit range of TRTYPE.
+	 * If the Client-oF specification ever defines a PCIe TRTYPE, this should be updated.
+	 */
+	enum spdk_client_transport_type
+	{
+		/**
+		 * PCIe Transport (locally attached devices)
+		 */
+		SPDK_CLIENT_TRANSPORT_PCIE = 256,
+
+		/**
+		 * RDMA Transport (RoCE, iWARP, etc.)
+		 */
+		SPDK_CLIENT_TRANSPORT_RDMA = SPDK_SRV_TRTYPE_RDMA,
+
+		/**
+		 * TCP Transport
+		 */
+		SPDK_CLIENT_TRANSPORT_TCP = SPDK_SRV_TRTYPE_TCP,
+
+		/**
+		 * Custom VFIO User Transport (Not spec defined)
+		 */
+		SPDK_CLIENT_TRANSPORT_VFIOUSER = 1024,
+
+		/**
+		 * Custom Transport (Not spec defined)
+		 */
+		SPDK_CLIENT_TRANSPORT_CUSTOM = 4096,
+	};
+
+	struct spdk_client_ctrlr *spdk_client_transport_ctrlr_construct(const char *trstring,
+																	const struct spdk_client_ctrlr_opts *opts,
+																	void *devhandle);
+
+	static inline bool spdk_client_trtype_is_fabrics(enum spdk_client_transport_type trtype)
+	{
+		/* We always define non-fabrics trtypes outside of the 8-bit range
+		 * of Client-oF trtype.
+		 */
+		return trtype <= UINT8_MAX;
+	}
+
+	/* typedef added for coding style reasons */
+	typedef enum spdk_client_transport_type spdk_client_transport_type_t;
+
+	/**
+	 * Client transport identifier.
+	 *
+	 * This identifies a unique endpoint on an Client fabric.
+	 *
+	 * A string representation of a transport ID may be converted to this type using
+	 * spdk_client_transport_id_parse().
+	 */
+	struct spdk_client_transport_id
+	{
+		/**
+		 * Client transport string.
+		 */
+		char trstring[SPDK_SRV_TRSTRING_MAX_LEN + 1];
+
+		/**
+		 * Client transport type.
+		 */
+		enum spdk_client_transport_type trtype;
+
+		/**
+		 * Address family of the transport address.
+		 *
+		 * For PCIe, this value is ignored.
+		 */
+		enum spdk_srv_adrfam adrfam;
+
+		/**
+		 * Transport address of the Client-oF endpoint. For transports which use IP
+		 * addressing (e.g. RDMA), this should be an IP address. For PCIe, this
+		 * can either be a zero length string (the whole bus) or a PCI address
+		 * in the format DDDD:BB:DD.FF or DDDD.BB.DD.FF. For FC the string is
+		 * formatted as: nn-0xWWNN:pn-0xWWPN” where WWNN is the Node_Name of the
+		 * target Client_Port and WWPN is the N_Port_Name of the target Client_Port.
+		 */
+		char traddr[SPDK_SRV_TRADDR_MAX_LEN + 1];
+
+		/**
+		 * Transport service id of the Client-oF endpoint.  For transports which use
+		 * IP addressing (e.g. RDMA), this field should be the port number. For PCIe,
+		 * and FC this is always a zero length string.
+		 */
+		char trsvcid[SPDK_SRV_TRSVCID_MAX_LEN + 1];
+
+		/**
+		 * Subsystem NQN of the Client over Fabrics endpoint. May be a zero length string.
+		 */
+		// char subnqn[SPDK_SRV_NQN_MAX_LEN + 1];
+
+		/**
+		 * The Transport connection priority of the Client-oF endpoint. Currently this is
+		 * only supported by posix based sock implementation on Kernel TCP stack. More
+		 * information of this field can be found from the socket(7) man page.
+		 */
+		int priority;
+	};
+
+	/**
+	 * Client host identifier
+	 *
+	 * Used for defining the host identity for an Client-oF connection.
+	 *
+	 * In terms of configuration, this object can be considered a subtype of TransportID
+	 * Please see etc/spdk/srv.conf.in for more details.
+	 *
+	 * A string representation of this type may be converted to this type using
+	 * spdk_client_host_id_parse().
+	 */
+	struct spdk_client_host_id
+	{
+		/**
+		 * Transport address to be used by the host when connecting to the Client-oF endpoint.
+		 * May be an IP address or a zero length string for transports which
+		 * use IP addressing (e.g. RDMA).
+		 * For PCIe and FC this is always a zero length string.
+		 */
+		char hostaddr[SPDK_SRV_TRADDR_MAX_LEN + 1];
+
+		/**
+		 * Transport service ID used by the host when connecting to the Client.
+		 * May be a port number or a zero length string for transports which
+		 * use IP addressing (e.g. RDMA).
+		 * For PCIe and FC this is always a zero length string.
+		 */
+		char hostsvcid[SPDK_SRV_TRSVCID_MAX_LEN + 1];
+	};
+
+	struct spdk_client_rdma_device_stat
+	{
+		const char *name;
+		uint64_t polls;
+		uint64_t idle_polls;
+		uint64_t completions;
+		uint64_t queued_requests;
+		uint64_t total_send_wrs;
+		uint64_t send_doorbell_updates;
+		uint64_t total_recv_wrs;
+		uint64_t recv_doorbell_updates;
+	};
+
+	struct spdk_client_pcie_stat
+	{
+		uint64_t polls;
+		uint64_t idle_polls;
+		uint64_t completions;
+		uint64_t cq_doorbell_updates;
+		uint64_t submitted_requests;
+		uint64_t queued_requests;
+		uint64_t sq_doobell_updates;
+	};
+
+	struct spdk_client_tcp_stat
+	{
+		uint64_t polls;
+		uint64_t idle_polls;
+		uint64_t socket_completions;
+		uint64_t client_completions;
+		uint64_t submitted_requests;
+		uint64_t queued_requests;
+	};
+
+	struct spdk_client_transport_poll_group_stat
+	{
+		spdk_client_transport_type_t trtype;
+		union
+		{
+			struct
+			{
+				uint32_t num_devices;
+				struct spdk_client_rdma_device_stat *device_stats;
+			} rdma;
+			struct spdk_client_pcie_stat pcie;
+			struct spdk_client_tcp_stat tcp;
+		};
+	};
+
+	struct spdk_client_poll_group_stat
+	{
+		uint32_t num_transports;
+		struct spdk_client_transport_poll_group_stat **transport_stat;
+	};
+
+	/*
+	 * Controller support flags
+	 *
+	 * Used for identifying if the controller supports these flags.
+	 */
+	enum spdk_client_ctrlr_flags
+	{
+		SPDK_CLIENT_CTRLR_SGL_SUPPORTED = 1 << 0,				 /**< SGL is supported */
+		SPDK_CLIENT_CTRLR_SECURITY_SEND_RECV_SUPPORTED = 1 << 1, /**< security send/receive is supported */
+		SPDK_CLIENT_CTRLR_WRR_SUPPORTED = 1 << 2,				 /**< Weighted Round Robin is supported */
+		SPDK_CLIENT_CTRLR_COMPARE_AND_WRITE_SUPPORTED = 1 << 3,	 /**< Compare and write fused operations supported */
+		SPDK_CLIENT_CTRLR_SGL_REQUIRES_DWORD_ALIGNMENT = 1 << 4, /**< Dword alignment is required for SGL */
+		SPDK_CLIENT_CTRLR_ZONE_APPEND_SUPPORTED = 1 << 5,		 /**< Zone Append is supported (within Zoned Namespaces) */
+		SPDK_CLIENT_CTRLR_DIRECTIVES_SUPPORTED = 1 << 6,		 /**< The Directives is supported */
+	};
+
+	/**
+	 * Structure with optional IO request parameters
+	 */
+	struct spdk_client_ns_cmd_ext_io_opts
+	{
+		/** size of this structure in bytes */
+		size_t size;
+		/** Memory domain which describes data payload in IO request. The controller must support
+		 * the corresponding memory domain type, refer to \ref spdk_client_ctrlr_get_memory_domains */
+		struct spdk_memory_domain *memory_domain;
+		/** User context to be passed to memory domain operations */
+		void *memory_domain_ctx;
+		/** Flags for this IO, defined in client_spec.h */
+		uint32_t io_flags;
+		/** Virtual address pointer to the metadata payload, the length of metadata is specified by \ref spdk_client_ns_get_md_size */
+		void *metadata;
+		/** Application tag mask to use end-to-end protection information. */
+		uint16_t apptag_mask;
+		/** Application tag to use end-to-end protection information. */
+		uint16_t apptag;
+	};
+
+	/**
+	 * Signature for callback function invoked when a command is completed.
+	 *
+	 * \param ctx Callback context provided when the command was submitted.
+	 * \param cpl Completion queue entry that contains the completion status.
+	 */
+	typedef void (*spdk_client_cmd_cb)(void *ctx, const struct spdk_req_cpl *cpl);
+
+	/**
+	 * Parse the string representation of a transport ID.
+	 *
+	 * \param trid Output transport ID structure (must be allocated and initialized by caller).
+	 * \param str Input string representation of a transport ID to parse.
+	 *
+	 * str must be a zero-terminated C string containing one or more key:value pairs
+	 * separated by whitespace.
+	 *
+	 * Key          | Value
+	 * ------------ | -----
+	 * trtype       | Transport type (e.g. PCIe, RDMA)
+	 * adrfam       | Address family (e.g. IPv4, IPv6)
+	 * traddr       | Transport address (e.g. 0000:04:00.0 for PCIe, 192.168.100.8 for RDMA, or WWN for FC)
+	 * trsvcid      | Transport service identifier (e.g. 4420)
+	 * subnqn       | Subsystem NQN
+	 *
+	 * Unspecified fields of trid are left unmodified, so the caller must initialize
+	 * trid (for example, memset() to 0) before calling this function.
+	 *
+	 * \return 0 if parsing was successful and trid is filled out, or negated errno
+	 * values on failure.
+	 */
+	int spdk_client_transport_id_parse(struct spdk_client_transport_id *trid, const char *str);
+
+	/**
+	 * Fill in the trtype and trstring fields of this trid based on a known transport type.
+	 *
+	 * \param trid The trid to fill out.
+	 * \param trtype The transport type to use for filling the trid fields. Only valid for
+	 * transport types referenced in the Client-oF spec.
+	 */
+	void spdk_client_trid_populate_transport(struct spdk_client_transport_id *trid,
+											 enum spdk_client_transport_type trtype);
+
+	/**
+	 * Parse the string representation of a host ID.
+	 *
+	 * \param hostid Output host ID structure (must be allocated and initialized by caller).
+	 * \param str Input string representation of a transport ID to parse (hostid is a sub-configuration).
+	 *
+	 * str must be a zero-terminated C string containing one or more key:value pairs
+	 * separated by whitespace.
+	 *
+	 * Key            | Value
+	 * -------------- | -----
+	 * hostaddr       | Transport address (e.g. 192.168.100.8 for RDMA)
+	 * hostsvcid      | Transport service identifier (e.g. 4420)
+	 *
+	 * Unspecified fields of trid are left unmodified, so the caller must initialize
+	 * hostid (for example, memset() to 0) before calling this function.
+	 *
+	 * This function should not be used with Fiber Channel or PCIe as these transports
+	 * do not require host information for connections.
+	 *
+	 * \return 0 if parsing was successful and hostid is filled out, or negated errno
+	 * values on failure.
+	 */
+	int spdk_client_host_id_parse(struct spdk_client_host_id *hostid, const char *str);
+
+	/**
+	 * Parse the string representation of a transport ID transport type into the trid struct.
+	 *
+	 * \param trid The trid to write to
+	 * \param trstring Input string representation of transport type (e.g. "PCIe", "RDMA").
+	 *
+	 * \return 0 if parsing was successful and trtype is filled out, or negated errno
+	 * values if the provided string was an invalid transport string.
+	 */
+	int spdk_client_transport_id_populate_trstring(struct spdk_client_transport_id *trid,
+												   const char *trstring);
+
+	/**
+	 * Parse the string representation of a transport ID transport type.
+	 *
+	 * \param trtype Output transport type (allocated by caller).
+	 * \param str Input string representation of transport type (e.g. "PCIe", "RDMA").
+	 *
+	 * \return 0 if parsing was successful and trtype is filled out, or negated errno
+	 * values on failure.
+	 */
+	int spdk_client_transport_id_parse_trtype(enum spdk_client_transport_type *trtype, const char *str);
+
+	/**
+	 * Look up the string representation of a transport ID transport type.
+	 *
+	 * \param trtype Transport type to convert.
+	 *
+	 * \return static string constant describing trtype, or NULL if trtype not found.
+	 */
+	const char *spdk_client_transport_id_trtype_str(enum spdk_client_transport_type trtype);
+
+	/**
+	 * Look up the string representation of a transport ID address family.
+	 *
+	 * \param adrfam Address family to convert.
+	 *
+	 * \return static string constant describing adrfam, or NULL if adrfam not found.
+	 */
+	const char *spdk_client_transport_id_adrfam_str(enum spdk_srv_adrfam adrfam);
+
+	/**
+	 * Parse the string representation of a transport ID address family.
+	 *
+	 * \param adrfam Output address family (allocated by caller).
+	 * \param str Input string representation of address family (e.g. "IPv4", "IPv6").
+	 *
+	 * \return 0 if parsing was successful and adrfam is filled out, or negated errno
+	 * values on failure.
+	 */
+	int spdk_client_transport_id_parse_adrfam(enum spdk_srv_adrfam *adrfam, const char *str);
+
+	/**
+	 * Compare two transport IDs.
+	 *
+	 * The result of this function may be used to sort transport IDs in a consistent
+	 * order; however, the comparison result is not guaranteed to be consistent across
+	 * library versions.
+	 *
+	 * This function uses a case-insensitive comparison for string fields, but it does
+	 * not otherwise normalize the transport ID. It is the caller's responsibility to
+	 * provide the transport IDs in a consistent format.
+	 *
+	 * \param trid1 First transport ID to compare.
+	 * \param trid2 Second transport ID to compare.
+	 *
+	 * \return 0 if trid1 == trid2, less than 0 if trid1 < trid2, greater than 0 if
+	 * trid1 > trid2.
+	 */
+	int spdk_client_transport_id_compare(const struct spdk_client_transport_id *trid1,
+										 const struct spdk_client_transport_id *trid2);
+
+	/**
+	 * Determine whether the Client library can handle a specific Client over Fabrics
+	 * transport type.
+	 *
+	 * \param trtype Client over Fabrics transport type to check.
+	 *
+	 * \return true if trtype is supported or false if it is not supported or if
+	 * SPDK_CLIENT_TRANSPORT_CUSTOM is supplied as trtype since it can represent multiple
+	 * transports.
+	 */
+	bool spdk_client_transport_available(enum spdk_client_transport_type trtype);
+
+	/**
+	 * Determine whether the Client library can handle a specific Client over Fabrics
+	 * transport type.
+	 *
+	 * \param transport_name Name of the Client over Fabrics transport type to check.
+	 *
+	 * \return true if transport_name is supported or false if it is not supported.
+	 */
+	bool spdk_client_transport_available_by_name(const char *transport_name);
+
+	/**
+	 * Callback for spdk_client_probe() enumeration.
+	 *
+	 * \param cb_ctx Opaque value passed to spdk_client_probe().
+	 * \param trid Client transport identifier.
+	 * \param opts Client controller initialization options. This structure will be
+	 * populated with the default values on entry, and the user callback may update
+	 * any options to request a different value. The controller may not support all
+	 * requested parameters, so the final values will be provided during the attach
+	 * callback.
+	 *
+	 * \return true to attach to this device.
+	 */
+	typedef bool (*spdk_client_probe_cb)(void *cb_ctx, const struct spdk_client_transport_id *trid,
+										 struct spdk_client_ctrlr_opts *opts);
+
+	/**
+	 * Callback for spdk_client_attach() to report a device that has been attached to
+	 * the userspace Client driver.
+	 *
+	 * \param cb_ctx Opaque value passed to spdk_client_attach_cb().
+	 * \param trid Client transport identifier.
+	 * \param ctrlr Opaque handle to Client controller.
+	 * \param opts Client controller initialization options that were actually used.
+	 * Options may differ from the requested options from the attach call depending
+	 * on what the controller supports.
+	 */
+	typedef void (*spdk_client_attach_cb)(void *cb_ctx, const struct spdk_client_transport_id *trid,
+										  struct spdk_client_ctrlr *ctrlr,
+										  const struct spdk_client_ctrlr_opts *opts);
+
+	/**
+	 * Callback for spdk_client_remove() to report that a device attached to the userspace
+	 * Client driver has been removed from the system.
+	 *
+	 * The controller will remain in a failed state (any new I/O submitted will fail).
+	 *
+	 * The controller must be detached from the userspace driver by calling spdk_client_detach()
+	 * once the controller is no longer in use. It is up to the library user to ensure
+	 * that no other threads are using the controller before calling spdk_client_detach().
+	 *
+	 * \param cb_ctx Opaque value passed to spdk_client_remove_cb().
+	 * \param ctrlr Client controller instance that was removed.
+	 */
+	typedef void (*spdk_client_remove_cb)(void *cb_ctx, struct spdk_client_ctrlr *ctrlr);
+
+	typedef bool (*spdk_client_pcie_hotplug_filter_cb)(const struct spdk_pci_addr *addr);
+
+	/**
+	 * Enumerate the bus indicated by the transport ID and attach the userspace Client
+	 * driver to each device found if desired.
+	 *
+	 * This function is not thread safe and should only be called from one thread at
+	 * a time while no other threads are actively using any Client devices.
+	 *
+	 * If called from a secondary process, only devices that have been attached to
+	 * the userspace driver in the primary process will be probed.
+	 *
+	 * If called more than once, only devices that are not already attached to the
+	 * SPDK Client driver will be reported.
+	 *
+	 * To stop using the the controller and release its associated resources,
+	 * call spdk_client_detach() with the spdk_client_ctrlr instance from the attach_cb()
+	 * function.
+	 *
+	 * \param trid The transport ID indicating which bus to enumerate. If the trtype
+	 * is PCIe or trid is NULL, this will scan the local PCIe bus. If the trtype is
+	 * RDMA, the traddr and trsvcid must point at the location of an Client-oF discovery
+	 * service.
+	 * \param cb_ctx Opaque value which will be passed back in cb_ctx parameter of
+	 * the callbacks.
+	 * \param probe_cb will be called once per Client device found in the system.
+	 * \param attach_cb will be called for devices for which probe_cb returned true
+	 * once that Client controller has been attached to the userspace driver.
+	 * \param remove_cb will be called for devices that were attached in a previous
+	 * spdk_client_probe() call but are no longer attached to the system. Optional;
+	 * specify NULL if removal notices are not desired.
+	 *
+	 * \return 0 on success, -1 on failure.
+	 */
+	int spdk_client_probe(const struct spdk_client_transport_id *trid,
+						  void *cb_ctx,
+						  spdk_client_probe_cb probe_cb,
+						  spdk_client_attach_cb attach_cb,
+						  spdk_client_remove_cb remove_cb);
+
+	/**
+	 * Connect the Client driver to the device located at the given transport ID.
+	 *
+	 * This function is not thread safe and should only be called from one thread at
+	 * a time while no other threads are actively using this Client device.
+	 *
+	 * If called from a secondary process, only the device that has been attached to
+	 * the userspace driver in the primary process will be connected.
+	 *
+	 * If connecting to multiple controllers, it is suggested to use spdk_client_probe()
+	 * and filter the requested controllers with the probe callback. For PCIe controllers,
+	 * spdk_client_probe() will be more efficient since the controller resets will happen
+	 * in parallel.
+	 *
+	 * To stop using the the controller and release its associated resources, call
+	 * spdk_client_detach() with the spdk_client_ctrlr instance returned by this function.
+	 *
+	 * \param trid The transport ID indicating which device to connect. If the trtype
+	 * is PCIe, this will connect the local PCIe bus. If the trtype is RDMA, the traddr
+	 * and trsvcid must point at the location of an Client-oF service.
+	 * \param opts Client controller initialization options. Default values will be used
+	 * if the user does not specify the options. The controller may not support all
+	 * requested parameters.
+	 * \param opts_size Must be set to sizeof(struct spdk_client_ctrlr_opts), or 0 if
+	 * opts is NULL.
+	 *
+	 * \return pointer to the connected Client controller or NULL if there is any failure.
+	 *
+	 */
+	struct spdk_client_ctrlr *spdk_client_connect(const struct spdk_client_transport_id *trid,
+												  const struct spdk_client_ctrlr_opts *opts,
+												  size_t opts_size);
+
+	struct spdk_client_probe_ctx;
+
+	/**
+	 * Connect the Client driver to the device located at the given transport ID.
+	 *
+	 * The function will return a probe context on success, controller associates with
+	 * the context is not ready for use, user must call spdk_client_probe_poll_async()
+	 * until spdk_client_probe_poll_async() returns 0.
+	 *
+	 * \param trid The transport ID indicating which device to connect. If the trtype
+	 * is PCIe, this will connect the local PCIe bus. If the trtype is RDMA, the traddr
+	 * and trsvcid must point at the location of an Client-oF service.
+	 * \param opts Client controller initialization options. Default values will be used
+	 * if the user does not specify the options. The controller may not support all
+	 * requested parameters.
+	 * \param attach_cb will be called once the Client controller has been attached
+	 * to the userspace driver.
+	 *
+	 * \return probe context on success, NULL on failure.
+	 *
+	 */
+	struct spdk_client_probe_ctx *spdk_client_connect_async(const struct spdk_client_transport_id *trid,
+															const struct spdk_client_ctrlr_opts *opts,
+															spdk_client_attach_cb attach_cb);
+
+	/**
+	 * Probe and add controllers to the probe context list.
+	 *
+	 * Users must call spdk_client_probe_poll_async() to initialize
+	 * controllers in the probe context list to the READY state.
+	 *
+	 * \param trid The transport ID indicating which bus to enumerate. If the trtype
+	 * is PCIe or trid is NULL, this will scan the local PCIe bus. If the trtype is
+	 * RDMA, the traddr and trsvcid must point at the location of an Client-oF discovery
+	 * service.
+	 * \param cb_ctx Opaque value which will be passed back in cb_ctx parameter of
+	 * the callbacks.
+	 * \param probe_cb will be called once per Client device found in the system.
+	 * \param attach_cb will be called for devices for which probe_cb returned true
+	 * once that Client controller has been attached to the userspace driver.
+	 * \param remove_cb will be called for devices that were attached in a previous
+	 * spdk_client_probe() call but are no longer attached to the system. Optional;
+	 * specify NULL if removal notices are not desired.
+	 *
+	 * \return probe context on success, NULL on failure.
+	 */
+	struct spdk_client_probe_ctx *spdk_client_probe_async(const struct spdk_client_transport_id *trid,
+														  void *cb_ctx,
+														  spdk_client_probe_cb probe_cb,
+														  spdk_client_attach_cb attach_cb,
+														  spdk_client_remove_cb remove_cb);
+
+	/**
+	 * Proceed with attaching controllers associated with the probe context.
+	 *
+	 * The probe context is one returned from a previous call to
+	 * spdk_client_probe_async().  Users must call this function on the
+	 * probe context until it returns 0.
+	 *
+	 * If any controllers fail to attach, there is no explicit notification.
+	 * Users can detect attachment failure by comparing attach_cb invocations
+	 * with the number of times where the user returned true for the
+	 * probe_cb.
+	 *
+	 * \param probe_ctx Context used to track probe actions.
+	 *
+	 * \return 0 if all probe operations are complete; the probe_ctx
+	 * is also freed and no longer valid.
+	 * \return -EAGAIN if there are still pending probe operations; user must call
+	 * spdk_client_probe_poll_async again to continue progress.
+	 */
+	int spdk_client_probe_poll_async(struct spdk_client_probe_ctx *probe_ctx);
+
+	/**
+	 * Detach specified device returned by spdk_client_probe()'s attach_cb from the
+	 * Client driver.
+	 *
+	 * On success, the spdk_client_ctrlr handle is no longer valid.
+	 *
+	 * This function should be called from a single thread while no other threads
+	 * are actively using the Client device.
+	 *
+	 * \param ctrlr Opaque handle to Client controller.
+	 *
+	 * \return 0 on success, -1 on failure.
+	 */
+	int spdk_client_detach(struct spdk_client_ctrlr *ctrlr);
+
+	struct spdk_client_detach_ctx;
+
+	/**
+	 * Allocate a context to track detachment of multiple controllers if this call is the
+	 * first successful start of detachment in a sequence, or use the passed context otherwise.
+	 *
+	 * Then, start detaching the specified device returned by spdk_client_probe()'s attach_cb
+	 * from the Client driver, and append this detachment to the context.
+	 *
+	 * User must call spdk_client_detach_poll_async() to complete the detachment.
+	 *
+	 * If the context is not allocated before this call, and if the specified device is detached
+	 * locally from the caller process but any other process still attaches it or failed to be
+	 * detached, context is not allocated.
+	 *
+	 * This function should be called from a single thread while no other threads are
+	 * actively using the Client device.
+	 *
+	 * \param ctrlr Opaque handle to HVMe controller.
+	 * \param detach_ctx Reference to the context in a sequence. An new context is allocated
+	 * if this call is the first successful start of detachment in a sequence, or use the
+	 * passed context.
+	 */
+	int spdk_client_detach_async(struct spdk_client_ctrlr *ctrlr,
+								 struct spdk_client_detach_ctx **detach_ctx);
+
+	/**
+	 * Poll detachment of multiple controllers until they complete.
+	 *
+	 * User must call this function until it returns 0.
+	 *
+	 * \param detach_ctx Context to track the detachment.
+	 *
+	 * \return 0 if all detachments complete; the context is also freed and no longer valid.
+	 * \return -EAGAIN if any detachment is still in progress; users must call
+	 * spdk_client_detach_poll_async() again to continue progress.
+	 */
+	int spdk_client_detach_poll_async(struct spdk_client_detach_ctx *detach_ctx);
+
+	/**
+	 * Continue calling spdk_client_detach_poll_async() internally until it returns 0.
+	 *
+	 * \param detach_ctx Context to track the detachment.
+	 */
+	void spdk_client_detach_poll(struct spdk_client_detach_ctx *detach_ctx);
+
+	/**
+	 * Set the remove callback and context to be invoked if the controller is removed.
+	 *
+	 * This will override any remove_cb and/or ctx specified when the controller was
+	 * probed.
+	 *
+	 * This function may only be called from the primary process.  This function has
+	 * no effect if called from a secondary process.
+	 *
+	 * \param ctrlr Opaque handle to an Client controller.
+	 * \param remove_cb remove callback
+	 * \param remove_ctx remove callback context
+	 */
+	void spdk_client_ctrlr_set_remove_cb(struct spdk_client_ctrlr *ctrlr,
+										 spdk_client_remove_cb remove_cb, void *remove_ctx);
+
+	/**
+	 * Perform a full hardware reset of the Client controller.
+	 *
+	 * This function should be called from a single thread while no other threads
+	 * are actively using the Client device.
+	 *
+	 * Any pointers returned from spdk_client_ctrlr_get_ns(), spdk_client_ns_get_data(),
+	 * spdk_client_zns_ns_get_data(), and spdk_client_zns_ctrlr_get_data()
+	 * may be invalidated by calling this function. The number of namespaces as returned
+	 * by spdk_client_ctrlr_get_num_ns() may also change.
+	 *
+	 * \param ctrlr Opaque handle to Client controller.
+	 *
+	 * \return 0 on success, -1 on failure.
+	 */
+	int spdk_client_ctrlr_reset(struct spdk_client_ctrlr *ctrlr);
+
+	/**
+	 * Inform the driver that the application is preparing to reset the specified Client controller.
+	 *
+	 * This function allows the driver to make decisions knowing that a reset is about to happen.
+	 * For example, the pcie transport in this case could skip sending DELETE_CQ and DELETE_SQ
+	 * commands to the controller if an io qpair is freed after this function is called.
+	 *
+	 * \param ctrlr Opaque handle to Client controller.
+	 */
+	void spdk_client_ctrlr_prepare_for_reset(struct spdk_client_ctrlr *ctrlr);
+
+	struct spdk_client_ctrlr_reset_ctx;
+
+	/**
+	 * Create a context object that can be polled to perform a full hardware reset of the Client controller.
+	 * (Deprecated, please use spdk_client_ctrlr_disconnect(), spdk_client_ctrlr_reconnect_async(), and
+	 * spdk_client_ctrlr_reconnect_poll_async() instead.)
+	 *
+	 * The function will set the controller reset context on success, user must call
+	 * spdk_client_ctrlr_reset_poll_async() until it returns a value other than -EAGAIN.
+	 *
+	 * \param ctrlr Opaque handle to Client controller.
+	 * \param reset_ctx Double pointer to reset context.
+	 *
+	 * \return 0 on success.
+	 * \return -ENOMEM if context could not be allocated.
+	 * \return -EBUSY if controller is already resetting.
+	 * \return -ENXIO if controller has been removed.
+	 *
+	 */
+	int spdk_client_ctrlr_reset_async(struct spdk_client_ctrlr *ctrlr,
+									  struct spdk_client_ctrlr_reset_ctx **reset_ctx);
+
+	/**
+	 * Proceed with resetting controller associated with the controller reset context.
+	 * (Deprecated, please use spdk_client_ctrlr_disconnect(), spdk_client_ctrlr_reconnect_async(), and
+	 * spdk_client_ctrlr_reconnect_poll_async() instead.)
+	 *
+	 * The controller reset context is one returned from a previous call to
+	 * spdk_client_ctrlr_reset_async().  Users must call this function on the
+	 * controller reset context until it returns a value other than -EAGAIN.
+	 *
+	 * \param ctrlr_reset_ctx Context used to track controller reset actions.
+	 *
+	 * \return 0 if all controller reset operations are complete; the ctrlr_reset_ctx
+	 * is also freed and no longer valid.
+	 * \return -EAGAIN if there are still pending controller reset operations; user must call
+	 * spdk_client_ctrlr_reset_poll_async again to continue progress.
+	 */
+	int spdk_client_ctrlr_reset_poll_async(struct spdk_client_ctrlr_reset_ctx *ctrlr_reset_ctx);
+
+	/**
+	 * Disconnect the given Client controller.
+	 *
+	 * This function is used as the first operation of a full reset sequence of the given Client
+	 * controller. The Client controller is ready to reconnect after completing this function.
+	 *
+	 * \param ctrlr Opaque handle to Client controller.
+	 *
+	 * \return 0 on success, -EBUSY if controller is already resetting, or -ENXIO if controller
+	 * has been removed.
+	 */
+	int spdk_client_ctrlr_disconnect(struct spdk_client_ctrlr *ctrlr);
+
+	/**
+	 * Start re-enabling the given Client controller in a full reset sequence
+	 *
+	 * \param ctrlr Opaque handle to Client controller.
+	 */
+	void spdk_client_ctrlr_reconnect_async(struct spdk_client_ctrlr *ctrlr);
+
+	/**
+	 * Proceed with re-enabling the given Client controller.
+	 *
+	 * Users must call this function in a full reset sequence until it returns a value other
+	 * than -EAGAIN.
+	 *
+	 * \return 0 if the given Client controller is enabled, or -EBUSY if there are still
+	 * pending operations to enable it.
+	 */
+	int spdk_client_ctrlr_reconnect_poll_async(struct spdk_client_ctrlr *ctrlr);
+
+	/**
+	 * Perform a Client subsystem reset.
+	 *
+	 * This function should be called from a single thread while no other threads
+	 * are actively using the Client device.
+	 * A subsystem reset is typically seen by the OS as a hot remove, followed by a
+	 * hot add event.
+	 *
+	 * Any pointers returned from spdk_client_ctrlr_get_ns(), spdk_client_ns_get_data(),
+	 * spdk_client_zns_ns_get_data(), and spdk_client_zns_ctrlr_get_data()
+	 * may be invalidated by calling this function. The number of namespaces as returned
+	 * by spdk_client_ctrlr_get_num_ns() may also change.
+	 *
+	 * \param ctrlr Opaque handle to Client controller.
+	 *
+	 * \return 0 on success, -1 on failure, -ENOTSUP if subsystem reset is not supported.
+	 */
+	int spdk_client_ctrlr_reset_subsystem(struct spdk_client_ctrlr *ctrlr);
+
+	/**
+	 * Fail the given Client controller.
+	 *
+	 * This function gives the application the opportunity to fail a controller
+	 * at will. When a controller is failed, any calls to process completions or
+	 * submit I/O on qpairs associated with that controller will fail with an error
+	 * code of -ENXIO.
+	 * The controller can only be taken from the failed state by
+	 * calling spdk_client_ctrlr_reset. After the controller has been successfully
+	 * reset, any I/O pending when the controller was moved to failed will be
+	 * aborted back to the application and can be resubmitted. I/O can then resume.
+	 *
+	 * \param ctrlr Opaque handle to an Client controller.
+	 */
+	void spdk_client_ctrlr_fail(struct spdk_client_ctrlr *ctrlr);
+
+	/**
+	 * This function returns the failed status of a given controller.
+	 *
+	 * \param ctrlr Opaque handle to an Client controller.
+	 *
+	 * \return True if the controller is failed, false otherwise.
+	 */
+	bool spdk_client_ctrlr_is_failed(struct spdk_client_ctrlr *ctrlr);
+
+	/**
+	 * Get the identify controller data as defined by the Client specification.
+	 *
+	 * This function is thread safe and can be called at any point while the controller
+	 * is attached to the SPDK Client driver.
+	 *
+	 * \param ctrlr Opaque handle to Client controller.
+	 *
+	 * \return pointer to the identify controller data.
+	 */
+	const struct spdk_client_ctrlr_data *spdk_client_ctrlr_get_data(struct spdk_client_ctrlr *ctrlr);
+
+	/**
+	 * Get the Client controller CSTS (Status) register.
+	 *
+	 * \param ctrlr Opaque handle to Client controller.
+	 *
+	 * \return the Client controller CSTS (Status) register.
+	 */
+	union spdk_client_csts_register spdk_client_ctrlr_get_regs_csts(struct spdk_client_ctrlr *ctrlr);
+
+	/**
+	 * Get the Client controller CC (Configuration) register.
+	 *
+	 * \param ctrlr Opaque handle to Client controller.
+	 *
+	 * \return the Client controller CC (Configuration) register.
+	 */
+	union spdk_client_cc_register spdk_client_ctrlr_get_regs_cc(struct spdk_client_ctrlr *ctrlr);
+
+	/**
+	 * Get the Client controller CAP (Capabilities) register.
+	 *
+	 * \param ctrlr Opaque handle to Client controller.
+	 *
+	 * \return the Client controller CAP (Capabilities) register.
+	 */
+	union spdk_client_cap_register spdk_client_ctrlr_get_regs_cap(struct spdk_client_ctrlr *ctrlr);
+
+	/**
+	 * Get the Client controller VS (Version) register.
+	 *
+	 * \param ctrlr Opaque handle to Client controller.
+	 *
+	 * \return the Client controller VS (Version) register.
+	 */
+	union spdk_client_vs_register spdk_client_ctrlr_get_regs_vs(struct spdk_client_ctrlr *ctrlr);
+
+	/**
+	 * Get the Client controller CMBSZ (Controller Memory Buffer Size) register
+	 *
+	 * \param ctrlr Opaque handle to Client controller.
+	 *
+	 * \return the Client controller CMBSZ (Controller Memory Buffer Size) register.
+	 */
+	union spdk_client_cmbsz_register spdk_client_ctrlr_get_regs_cmbsz(struct spdk_client_ctrlr *ctrlr);
+
+	/**
+	 * Get the Client controller PMRCAP (Persistent Memory Region Capabilities) register.
+	 *
+	 * \param ctrlr Opaque handle to Client controller.
+	 *
+	 * \return the Client controller PMRCAP (Persistent Memory Region Capabilities) register.
+	 */
+	union spdk_client_pmrcap_register spdk_client_ctrlr_get_regs_pmrcap(struct spdk_client_ctrlr *ctrlr);
+
+	/**
+	 * Get the Client controller BPINFO (Boot Partition Information) register.
+	 *
+	 * \param ctrlr Opaque handle to Client controller.
+	 *
+	 * \return the Client controller BPINFO (Boot Partition Information) register.
+	 */
+	union spdk_client_bpinfo_register spdk_client_ctrlr_get_regs_bpinfo(struct spdk_client_ctrlr *ctrlr);
+
+	/**
+	 * Get the Client controller PMR size.
+	 *
+	 * \param ctrlr Opaque handle to Client controller.
+	 *
+	 * \return the Client controller PMR size or 0 if PMR is not supported.
+	 */
+	uint64_t spdk_client_ctrlr_get_pmrsz(struct spdk_client_ctrlr *ctrlr);
+
+	/**
+	 * Get the maximum NSID value that will ever be used for the given controller
+	 *
+	 * This function is thread safe and can be called at any point while the
+	 * controller is attached to the SPDK Client driver.
+	 *
+	 * This is equivalent to calling spdk_client_ctrlr_get_data() to get the
+	 * spdk_client_ctrlr_data and then reading the nn field.
+	 *
+	 * The NN field in the Client specification represents the maximum value that a
+	 * namespace ID can ever have. Prior to Client 1.2, this was also the number of
+	 * active namespaces, but from 1.2 onward the list of namespaces may be
+	 * sparsely populated. Unfortunately, the meaning of this field is often
+	 * misinterpreted by drive manufacturers and Client-oF implementers so it is
+	 * not considered reliable. AVOID USING THIS FUNCTION WHENEVER POSSIBLE.
+	 *
+	 * \param ctrlr Opaque handle to Client controller.
+	 *
+	 * \return the number of namespaces.
+	 */
+	uint32_t spdk_client_ctrlr_get_num_ns(struct spdk_client_ctrlr *ctrlr);
+
+	/**
+	 * Get the PCI device of a given Client controller.
+	 *
+	 * This only works for local (PCIe-attached) Client controllers; other transports
+	 * will return NULL.
+	 *
+	 * \param ctrlr Opaque handle to Client controller.
+	 *
+	 * \return PCI device of the Client controller, or NULL if not available.
+	 */
+	struct spdk_pci_device *spdk_client_ctrlr_get_pci_device(struct spdk_client_ctrlr *ctrlr);
+
+	/**
+	 * Get the maximum data transfer size of a given Client controller.
+	 *
+	 * \param ctrlr Opaque handle to Client controller.
+	 *
+	 * \return Maximum data transfer size of the Client controller in bytes.
+	 *
+	 * The I/O command helper functions, such as spdk_client_ns_cmd_read(), will split
+	 * large I/Os automatically; however, it is up to the user to obey this limit for
+	 * commands submitted with the raw command functions, such as spdk_client_ctrlr_cmd_io_raw().
+	 */
+	uint32_t spdk_client_ctrlr_get_max_xfer_size(const struct spdk_client_ctrlr *ctrlr);
+
+	/**
+	 * Check whether the nsid is an active nv for the given Client controller.
+	 *
+	 * This function is thread safe and can be called at any point while the controller
+	 * is attached to the SPDK Client driver.
+	 *
+	 * \param ctrlr Opaque handle to Client controller.
+	 * \param nsid Namespace id.
+	 *
+	 * \return true if nsid is an active ns, or false otherwise.
+	 */
+	bool spdk_client_ctrlr_is_active_ns(struct spdk_client_ctrlr *ctrlr, uint32_t nsid);
+
+	/**
+	 * Get the nsid of the first active namespace.
+	 *
+	 * This function is thread safe and can be called at any point while the controller
+	 * is attached to the SPDK Client driver.
+	 *
+	 * \param ctrlr Opaque handle to Client controller.
+	 *
+	 * \return the nsid of the first active namespace, 0 if there are no active namespaces.
+	 */
+	uint32_t spdk_client_ctrlr_get_first_active_ns(struct spdk_client_ctrlr *ctrlr);
+
+	/**
+	 * Get next active namespace given the previous nsid.
+	 *
+	 * This function is thread safe and can be called at any point while the controller
+	 * is attached to the SPDK Client driver.
+	 *
+	 * \param ctrlr Opaque handle to Client controller.
+	 * \param prev_nsid Namespace id.
+	 *
+	 * \return a next active namespace given the previous nsid, 0 when there are no
+	 * more active namespaces.
+	 */
+	uint32_t spdk_client_ctrlr_get_next_active_ns(struct spdk_client_ctrlr *ctrlr, uint32_t prev_nsid);
+
+	/**
+	 * Determine if a particular log page is supported by the given Client controller.
+	 *
+	 * This function is thread safe and can be called at any point while the controller
+	 * is attached to the SPDK Client driver.
+	 *
+	 * \sa spdk_client_ctrlr_cmd_get_log_page().
+	 *
+	 * \param ctrlr Opaque handle to Client controller.
+	 * \param log_page Log page to query.
+	 *
+	 * \return true if supported, or false otherwise.
+	 */
+	bool spdk_client_ctrlr_is_log_page_supported(struct spdk_client_ctrlr *ctrlr, uint8_t log_page);
+
+	/**
+	 * Determine if a particular feature is supported by the given Client controller.
+	 *
+	 * This function is thread safe and can be called at any point while the controller
+	 * is attached to the SPDK Client driver.
+	 *
+	 * \sa spdk_client_ctrlr_cmd_get_feature().
+	 *
+	 * \param ctrlr Opaque handle to Client controller.
+	 * \param feature_code Feature to query.
+	 *
+	 * \return true if supported, or false otherwise.
+	 */
+	bool spdk_client_ctrlr_is_feature_supported(struct spdk_client_ctrlr *ctrlr, uint8_t feature_code);
+
+	/**
+	 * Signature for callback function invoked when a command is completed.
+	 *
+	 * \param ctx Callback context provided when the command was submitted.
+	 * \param cpl Completion queue entry that contains the completion status.
+	 */
+	typedef void (*spdk_req_cmd_cb)(void *ctx, const struct spdk_req_cpl *cpl);
+
+	/**
+	 * Signature for callback function invoked when an asynchronous error request
+	 * command is completed.
+	 *
+	 * \param aer_cb_arg Context specified by spdk_client_register_aer_callback().
+	 * \param cpl Completion queue entry that contains the completion status
+	 * of the asynchronous event request that was completed.
+	 */
+	typedef void (*spdk_client_aer_cb)(void *aer_cb_arg,
+									   const struct spdk_req_cpl *cpl);
+
+	/**
+	 * Register callback function invoked when an AER command is completed for the
+	 * given Client controller.
+	 *
+	 * \param ctrlr Opaque handle to Client controller.
+	 * \param aer_cb_fn Callback function invoked when an asynchronous error request
+	 * command is completed.
+	 * \param aer_cb_arg Argument passed to callback function.
+	 */
+	void spdk_client_ctrlr_register_aer_callback(struct spdk_client_ctrlr *ctrlr,
+												 spdk_client_aer_cb aer_cb_fn,
+												 void *aer_cb_arg);
+
+	/**
+	 * Opaque handle to a queue pair.
+	 *
+	 * I/O queue pairs may be allocated using spdk_client_ctrlr_alloc_io_qpair().
+	 */
+	typedef void (*spdk_connected_cb)(void *cb_args, int status);
+	struct spdk_client_qpair;
+
+	/**
+	 * Signature for the callback function invoked when a timeout is detected on a
+	 * request.
+	 *
+	 * For timeouts detected on the admin queue pair, the qpair returned here will
+	 * be NULL.  If the controller has a serious error condition and is unable to
+	 * communicate with driver via completion queue, the controller can set Controller
+	 * Fatal Status field to 1, then reset is required to recover from such error.
+	 * Users may detect Controller Fatal Status when timeout happens.
+	 *
+	 * \param cb_arg Argument passed to callback function.
+	 * \param ctrlr Opaque handle to Client controller.
+	 * \param qpair Opaque handle to a queue pair.
+	 * \param cid Command ID.
+	 */
+	typedef void (*spdk_client_timeout_cb)(void *cb_arg,
+										   struct spdk_client_ctrlr *ctrlr,
+										   struct spdk_client_qpair *qpair,
+										   uint16_t cid);
+
+	/**
+	 * Register for timeout callback on a controller.
+	 *
+	 * The application can choose to register for timeout callback or not register
+	 * for timeout callback.
+	 *
+	 * \param ctrlr Client controller on which to monitor for timeout.
+	 * \param timeout_io_us Timeout value in microseconds for io commands.
+	 * \param timeout_admin_us Timeout value in microseconds for admin commands.
+	 * \param cb_fn A function pointer that points to the callback function.
+	 * \param cb_arg Argument to the callback function.
+	 */
+	void spdk_client_ctrlr_register_timeout_callback(struct spdk_client_ctrlr *ctrlr,
+													 uint64_t timeout_io_us, uint64_t timeout_admin_us,
+													 spdk_client_timeout_cb cb_fn, void *cb_arg);
+
+	/**
+	 * Get a full discovery log page from the specified controller.
+	 *
+	 * This function will first read the discovery log header to determine the
+	 * total number of valid entries in the discovery log, then it will allocate
+	 * a buffer to hold the entire log and issue multiple GET_LOG_PAGE commands to
+	 * get all of the entries.
+	 *
+	 * The application is responsible for calling
+	 * \ref spdk_client_ctrlr_process_admin_completions to trigger processing of
+	 * completions submitted by this function.
+	 *
+	 * \param ctrlr Pointer to the discovery controller.
+	 * \param cb_fn Function to call when the operation is complete.
+	 * \param cb_arg Argument to pass to cb_fn.
+	 */
+	// int spdk_client_ctrlr_get_discovery_log_page(struct spdk_client_ctrlr *ctrlr,
+	// 		spdk_client_discovery_cb cb_fn, void *cb_arg);
+
+	/**
+	 * Submission queue priority values for Create I/O Submission Queue Command.
+	 *
+	 * Only valid for weighted round robin arbitration method.
+	 */
+	enum spdk_client_qprio
+	{
+		SPDK_CLIENT_QPRIO_URGENT = 0x0,
+		SPDK_CLIENT_QPRIO_HIGH = 0x1,
+		SPDK_CLIENT_QPRIO_MEDIUM = 0x2,
+		SPDK_CLIENT_QPRIO_LOW = 0x3
+	};
+
+	/**
+	 * Client I/O queue pair initialization options.
+	 *
+	 * These options may be passed to spdk_client_ctrlr_alloc_io_qpair() to configure queue pair
+	 * options at queue creation time.
+	 *
+	 * The user may retrieve the default I/O queue pair creation options for a controller using
+	 * spdk_client_ctrlr_get_default_io_qpair_opts().
+	 */
+	struct spdk_client_io_qpair_opts
+	{
+		/**
+		 * Queue priority for weighted round robin arbitration.  If a different arbitration
+		 * method is in use, pass 0.
+		 */
+		enum spdk_client_qprio qprio;
+
+		/**
+		 * The queue depth of this Client I/O queue. Overrides spdk_client_ctrlr_opts::io_queue_size.
+		 */
+		uint32_t io_queue_size;
+
+		/**
+		 * The number of requests to allocate for this Client I/O queue.
+		 *
+		 * Overrides spdk_client_ctrlr_opts::io_queue_requests.
+		 *
+		 * This should be at least as large as io_queue_size.
+		 *
+		 * A single I/O may allocate more than one request, since splitting may be
+		 * necessary to conform to the device's maximum transfer size, PRP list
+		 * compatibility requirements, or driver-assisted striping.
+		 */
+		uint32_t io_queue_requests;
+
+		/**
+		 * When submitting I/O via spdk_client_ns_read/write and similar functions,
+		 * don't immediately submit it to hardware. Instead, queue up new commands
+		 * and submit them to the hardware inside spdk_client_qpair_process_completions().
+		 *
+		 * This results in better batching of I/O commands. Often, it is more efficient
+		 * to submit batches of commands to the underlying hardware than each command
+		 * individually.
+		 *
+		 * This only applies to PCIe and RDMA transports.
+		 *
+		 * The flag was originally named delay_pcie_doorbell. To allow backward compatibility
+		 * both names are kept in unnamed union.
+		 */
+		union
+		{
+			bool delay_cmd_submit;
+			bool delay_pcie_doorbell;
+		};
+
+		/**
+		 * These fields allow specifying the memory buffers for the submission and/or
+		 * completion queues.
+		 * By default, vaddr is set to NULL meaning SPDK will allocate the memory to be used.
+		 * If vaddr is NULL then paddr must be set to 0.
+		 * If vaddr is non-NULL, and paddr is zero, SPDK derives the physical
+		 * address for the Client device, in this case the memory must be registered.
+		 * If a paddr value is non-zero, SPDK uses the vaddr and paddr as passed
+		 * SPDK assumes that the memory passed is both virtually and physically
+		 * contiguous.
+		 * If these fields are used, SPDK will NOT impose any restriction
+		 * on the number of elements in the queues.
+		 * The buffer sizes are in number of bytes, and are used to confirm
+		 * that the buffers are large enough to contain the appropriate queue.
+		 * These fields are only used by PCIe attached Client devices.  They
+		 * are presently ignored for other transports.
+		 */
+		struct
+		{
+			struct spdk_req_cmd *vaddr;
+			uint64_t paddr;
+			uint64_t buffer_size;
+		} sq;
+		struct
+		{
+			struct spdk_req_cpl *vaddr;
+			uint64_t paddr;
+			uint64_t buffer_size;
+		} cq;
+
+		/**
+		 * This flag indicates to the alloc_io_qpair function that it should not perform
+		 * the connect portion on this qpair. This allows the user to add the qpair to a
+		 * poll group and then connect it later.
+		 */
+		bool create_only;
+
+		/**
+		 * This flag if set to true enables the creation of submission and completion queue
+		 * asynchronously. This mode is currently supported at PCIe layer and tracks the
+		 * qpair creation with state machine and returns to the user.Default mode is set to
+		 * false to create io qpair synchronously.
+		 */
+		bool async_mode;
+	};
+
+	/**
+	 * Get the default options for I/O qpair creation for a specific Client controller.
+	 *
+	 * \param ctrlr Client controller to retrieve the defaults from.
+	 * \param[out] opts Will be filled with the default options for
+	 * spdk_client_ctrlr_alloc_io_qpair().
+	 * \param opts_size Must be set to sizeof(struct spdk_client_io_qpair_opts).
+	 */
+	void spdk_client_ctrlr_get_default_io_qpair_opts(struct spdk_client_ctrlr *ctrlr,
+													 struct spdk_client_io_qpair_opts *opts,
+													 size_t opts_size);
+
+	/**
+	 * Allocate an I/O queue pair (submission and completion queue).
+	 *
+	 * This function by default also performs any connection activities required for
+	 * a newly created qpair. To avoid that behavior, the user should set the create_only
+	 * flag in the opts structure to true.
+	 *
+	 * Each queue pair should only be used from a single thread at a time (mutual
+	 * exclusion must be enforced by the user).
+	 *
+	 * \param ctrlr Client controller for which to allocate the I/O queue pair.
+	 * \param opts I/O qpair creation options, or NULL to use the defaults as returned
+	 * by spdk_client_ctrlr_get_default_io_qpair_opts().
+	 * \param opts_size Must be set to sizeof(struct spdk_client_io_qpair_opts), or 0
+	 * if opts is NULL.
+	 *
+	 * \return a pointer to the allocated I/O queue pair.
+	 */
+	struct spdk_client_qpair *spdk_client_ctrlr_alloc_io_qpair(struct spdk_client_ctrlr *ctrlr,
+															   const struct spdk_client_io_qpair_opts *opts,
+															   size_t opts_size, struct spdk_client_transport_id *id, struct spdk_client_poll_group *client_pg);
+
+	struct spdk_client_qpair *
+	spdk_client_ctrlr_alloc_io_qpair_async(struct spdk_client_ctrlr *ctrlr,
+										   const struct spdk_client_io_qpair_opts *user_opts,
+										   size_t opts_size, struct spdk_client_transport_id *id, struct spdk_client_poll_group *client_pg, spdk_connected_cb cb_fn, void *cb_arg);
+	/**
+	 * Connect a newly created I/O qpair.
+	 *
+	 * This function does any connection activities required for a newly created qpair.
+	 * It should be called after spdk_client_ctrlr_alloc_io_qpair has been called with the
+	 * create_only flag set to true in the spdk_client_io_qpair_opts structure.
+	 *
+	 * This call will fail if performed on a qpair that is already connected.
+	 * For reconnecting qpairs, see spdk_client_ctrlr_reconnect_io_qpair.
+	 *
+	 * For fabrics like TCP and RDMA, this function actually sends the commands over the wire
+	 * that connect the qpair. For PCIe, this function performs some internal state machine operations.
+	 *
+	 * \param ctrlr Client controller for which to allocate the I/O queue pair.
+	 * \param qpair Opaque handle to the qpair to connect.
+	 *
+	 * return 0 on success or negated errno on failure. Specifically -EISCONN if the qpair is already connected.
+	 *
+	 */
+	int spdk_client_ctrlr_connect_io_qpair(struct spdk_client_ctrlr *ctrlr, struct spdk_client_qpair *qpair);
+
+	/**
+	 * Disconnect the given I/O qpair.
+	 *
+	 * This function must be called from the same thread as spdk_client_qpair_process_completions
+	 * and the spdk_client_ns_cmd_* functions.
+	 *
+	 * After disconnect, calling spdk_client_qpair_process_completions or one of the
+	 * spdk_client_ns_cmd* on a qpair will result in a return value of -ENXIO. A
+	 * disconnected qpair may be reconnected with either the spdk_client_ctrlr_connect_io_qpair
+	 * or spdk_client_ctrlr_reconnect_io_qpair APIs.
+	 *
+	 * \param qpair The qpair to disconnect.
+	 */
+	void spdk_client_ctrlr_disconnect_io_qpair(struct spdk_client_qpair *qpair);
+
+    bool spdk_client_ctrlr_has_free_memory(struct spdk_client_qpair *qpair, size_t size);
+
+	/**
+	 * Attempt to reconnect the given qpair.
+	 *
+	 * This function is intended to be called on qpairs that have already been connected,
+	 * but have since entered a failed state as indicated by a return value of -ENXIO from
+	 * either spdk_client_qpair_process_completions or one of the spdk_client_ns_cmd_* functions.
+	 * This function must be called from the same thread as spdk_client_qpair_process_completions
+	 * and the spdk_client_ns_cmd_* functions.
+	 *
+	 * Calling this function has the same effect as calling spdk_client_ctrlr_disconnect_io_qpair
+	 * followed by spdk_client_ctrlr_connect_io_qpair.
+	 *
+	 * This function may be called on newly created qpairs, but it does extra checks and attempts
+	 * to disconnect the qpair before connecting it. The recommended API for newly created qpairs
+	 * is spdk_client_ctrlr_connect_io_qpair.
+	 *
+	 * \param qpair The qpair to reconnect.
+	 *
+	 * \return 0 on success, or if the qpair was already connected.
+	 * -EAGAIN if the driver was unable to reconnect during this call,
+	 * but the controller is still connected and is either resetting or enabled.
+	 * -ENODEV if the controller is removed. In this case, the controller cannot be recovered
+	 * and the application will have to destroy it and the associated qpairs.
+	 * -ENXIO if the controller is in a failed state but is not yet resetting. In this case,
+	 * the application should call spdk_client_ctrlr_reset to reset the entire controller.
+	 */
+	int spdk_client_ctrlr_reconnect_io_qpair(struct spdk_client_qpair *qpair);
+
+	/**
+	 * Returns the reason the admin qpair for a given controller is disconnected.
+	 *
+	 * \param ctrlr The controller to check.
+	 *
+	 * \return a valid spdk_client_qp_failure_reason.
+	 */
+	spdk_client_qp_failure_reason spdk_client_ctrlr_get_admin_qp_failure_reason(
+		struct spdk_client_ctrlr *ctrlr);
+
+	/**
+	 * Free an I/O queue pair that was allocated by spdk_client_ctrlr_alloc_io_qpair().
+	 *
+	 * The qpair must not be accessed after calling this function.
+	 *
+	 * \param qpair I/O queue pair to free.
+	 *
+	 * \return 0 on success.  This function will never return any value other than 0.
+	 */
+	int spdk_client_ctrlr_free_io_qpair(struct spdk_client_qpair *qpair);
+
+	/**
+	 * Send the given NVM I/O command, I/O buffers, lists and all to the Client controller.
+	 *
+	 * This is a low level interface for submitting I/O commands directly.
+	 *
+	 * This function allows a caller to submit an I/O request that is
+	 * COMPLETELY pre-defined, right down to the "physical" memory buffers.
+	 * It is intended for testing hardware, specifying exact buffer location,
+	 * alignment, and offset.  It also allows for specific choice of PRP
+	 * and SGLs.
+	 *
+	 * The driver sets the CID.  EVERYTHING else is assumed set by the caller.
+	 * Needless to say, this is potentially extremely dangerous for both the host
+	 * (accidental/malicious storage usage/corruption), and the device.
+	 * Thus its intent is for very specific hardware testing and environment
+	 * reproduction.
+	 *
+	 * The command is submitted to a qpair allocated by spdk_client_ctrlr_alloc_io_qpair().
+	 * The user must ensure that only one thread submits I/O on a given qpair at any
+	 * given time.
+	 *
+	 * This function can only be used on PCIe controllers and qpairs.
+	 *
+	 * \param ctrlr Opaque handle to Client controller.
+	 * \param qpair I/O qpair to submit command.
+	 * \param cmd NVM I/O command to submit.
+	 * \param cb_fn Callback function invoked when the I/O command completes.
+	 * \param cb_arg Argument passed to callback function.
+	 *
+	 * \return 0 if successfully submitted, negated errnos on the following error conditions:
+	 * -ENOMEM: The request cannot be allocated.
+	 * -ENXIO: The qpair is failed at the transport level.
+	 */
+
+	int spdk_client_ctrlr_io_cmd_raw_no_payload_build(struct spdk_client_ctrlr *ctrlr,
+													  struct spdk_client_qpair *qpair,
+													  struct spdk_req_cmd *cmd,
+													  spdk_req_cmd_cb cb_fn, void *cb_arg);
+
+	/**
+	 * Send the given NVM I/O command to the Client controller.
+	 *
+	 * This is a low level interface for submitting I/O commands directly. Prefer
+	 * the spdk_client_ns_cmd_* functions instead. The validity of the command will
+	 * not be checked!
+	 *
+	 * When constructing the client_command it is not necessary to fill out the PRP
+	 * list/SGL or the CID. The driver will handle both of those for you.
+	 *
+	 * The command is submitted to a qpair allocated by spdk_client_ctrlr_alloc_io_qpair().
+	 * The user must ensure that only one thread submits I/O on a given qpair at any
+	 * given time.
+	 *
+	 * \param ctrlr Opaque handle to Client controller.
+	 * \param qpair I/O qpair to submit command.
+	 * \param cmd NVM I/O command to submit.
+	 * \param buf Virtual memory address of a single physically contiguous buffer.
+	 * \param len Size of buffer.
+	 * \param cb_fn Callback function invoked when the I/O command completes.
+	 * \param cb_arg Argument passed to callback function.
+	 *
+	 * \return 0 if successfully submitted, negated errnos on the following error conditions:
+	 * -ENOMEM: The request cannot be allocated.
+	 * -ENXIO: The qpair is failed at the transport level.
+	 */
+	int spdk_client_ctrlr_cmd_io_raw(struct spdk_client_ctrlr *ctrlr,
+									 struct spdk_client_qpair *qpair,
+									 struct spdk_req_cmd *cmd,
+									 void *buf, uint32_t len,
+									 spdk_req_cmd_cb cb_fn, void *cb_arg);
+
+	/**
+	 * Send the given NVM I/O command with metadata to the Client controller.
+	 *
+	 * This is a low level interface for submitting I/O commands directly. Prefer
+	 * the spdk_client_ns_cmd_* functions instead. The validity of the command will
+	 * not be checked!
+	 *
+	 * When constructing the client_command it is not necessary to fill out the PRP
+	 * list/SGL or the CID. The driver will handle both of those for you.
+	 *
+	 * The command is submitted to a qpair allocated by spdk_client_ctrlr_alloc_io_qpair().
+	 * The user must ensure that only one thread submits I/O on a given qpair at any
+	 * given time.
+	 *
+	 * \param ctrlr Opaque handle to Client controller.
+	 * \param qpair I/O qpair to submit command.
+	 * \param cmd NVM I/O command to submit.
+	 * \param buf Virtual memory address of a single physically contiguous buffer.
+	 * \param len Size of buffer.
+	 * \param md_buf Virtual memory address of a single physically contiguous metadata
+	 * buffer.
+	 * \param cb_fn Callback function invoked when the I/O command completes.
+	 * \param cb_arg Argument passed to callback function.
+	 *
+	 * \return 0 if successfully submitted, negated errnos on the following error conditions:
+	 * -ENOMEM: The request cannot be allocated.
+	 * -ENXIO: The qpair is failed at the transport level.
+	 */
+	int spdk_client_ctrlr_cmd_io_raw_with_md(struct spdk_client_ctrlr *ctrlr,
+											 struct spdk_client_qpair *qpair,
+											 struct spdk_req_cmd *cmd,
+											 void *buf, uint32_t len, void *md_buf,
+											 spdk_req_cmd_cb cb_fn, void *cb_arg);
+
+	/**
+	 * Process any outstanding completions for I/O submitted on a queue pair.
+	 *
+	 * This call is non-blocking, i.e. it only processes completions that are ready
+	 * at the time of this function call. It does not wait for outstanding commands
+	 * to finish.
+	 *
+	 * For each completed command, the request's callback function will be called if
+	 * specified as non-NULL when the request was submitted.
+	 *
+	 * The caller must ensure that each queue pair is only used from one thread at a
+	 * time.
+	 *
+	 * This function may be called at any point while the controller is attached to
+	 * the SPDK Client driver.
+	 *
+	 * \sa spdk_req_cmd_cb
+	 *
+	 * \param qpair Queue pair to check for completions.
+	 * \param max_completions Limit the number of completions to be processed in one
+	 * call, or 0 for unlimited.
+	 *
+	 * \return number of completions processed (may be 0) or negated on error. -ENXIO
+	 * in the special case that the qpair is failed at the transport layer.
+	 */
+	int32_t spdk_client_qpair_process_completions(struct spdk_client_qpair *qpair,
+												  uint32_t max_completions);
+
+	/**
+	 * Returns the reason the qpair is disconnected.
+	 *
+	 * \param qpair The qpair to check.
+	 *
+	 * \return a valid spdk_client_qp_failure_reason.
+	 */
+	spdk_client_qp_failure_reason spdk_client_qpair_get_failure_reason(struct spdk_client_qpair *qpair);
+
+	/**
+	 * Send the given admin command to the Client controller.
+	 *
+	 * This is a low level interface for submitting admin commands directly. Prefer
+	 * the spdk_client_ctrlr_cmd_* functions instead. The validity of the command will
+	 * not be checked!
+	 *
+	 * When constructing the client_command it is not necessary to fill out the PRP
+	 * list/SGL or the CID. The driver will handle both of those for you.
+	 *
+	 * This function is thread safe and can be called at any point while the controller
+	 * is attached to the SPDK Client driver.
+	 *
+	 * Call spdk_client_ctrlr_process_admin_completions() to poll for completion
+	 * of commands submitted through this function.
+	 *
+	 * \param ctrlr Opaque handle to Client controller.
+	 * \param cmd NVM admin command to submit.
+	 * \param buf Virtual memory address of a single physically contiguous buffer.
+	 * \param len Size of buffer.
+	 * \param cb_fn Callback function invoked when the admin command completes.
+	 * \param cb_arg Argument passed to callback function.
+	 *
+	 * \return 0 if successfully submitted, negated errno if resources could not be
+	 * allocated for this request, -ENXIO if the admin qpair is failed at the transport layer.
+	 */
+	int spdk_client_ctrlr_cmd_admin_raw(struct spdk_client_ctrlr *ctrlr,
+										struct spdk_req_cmd *cmd,
+										void *buf, uint32_t len,
+										spdk_req_cmd_cb cb_fn, void *cb_arg);
+
+	/**
+	 * Process any outstanding completions for admin commands.
+	 *
+	 * This will process completions for admin commands submitted on any thread.
+	 *
+	 * This call is non-blocking, i.e. it only processes completions that are ready
+	 * at the time of this function call. It does not wait for outstanding commands
+	 * to finish.
+	 *
+	 * This function is thread safe and can be called at any point while the controller
+	 * is attached to the SPDK Client driver.
+	 *
+	 * \param ctrlr Opaque handle to Client controller.
+	 *
+	 * \return number of completions processed (may be 0) or negated on error. -ENXIO
+	 * in the special case that the qpair is failed at the transport layer.
+	 */
+	// int32_t spdk_client_ctrlr_process_admin_completions(struct spdk_client_ctrlr *ctrlr);
+
+	/**
+	 * Opaque handle to a namespace. Obtained by calling spdk_client_ctrlr_get_ns().
+	 */
+	struct spdk_client_ns;
+
+	/**
+	 * Get a handle to a namespace for the given controller.
+	 *
+	 * Namespaces are numbered from 1 to the total number of namespaces. There will
+	 * never be any gaps in the numbering. The number of namespaces is obtained by
+	 * calling spdk_client_ctrlr_get_num_ns().
+	 *
+	 * This function is thread safe and can be called at any point while the controller
+	 * is attached to the SPDK Client driver.
+	 *
+	 * \param ctrlr Opaque handle to Client controller.
+	 * \param ns_id Namespace id.
+	 *
+	 * \return a pointer to the namespace.
+	 */
+	struct spdk_client_ns *spdk_client_ctrlr_get_ns(struct spdk_client_ctrlr *ctrlr, uint32_t ns_id);
+
+	/**
+	 * Get a specific log page from the Client controller.
+	 *
+	 * This function is thread safe and can be called at any point while the controller
+	 * is attached to the SPDK Client driver.
+	 *
+	 * Call spdk_client_ctrlr_process_admin_completions() to poll for completion of
+	 * commands submitted through this function.
+	 *
+	 * \sa spdk_client_ctrlr_is_log_page_supported()
+	 *
+	 * \param ctrlr Opaque handle to Client controller.
+	 * \param log_page The log page identifier.
+	 * \param nsid Depending on the log page, this may be 0, a namespace identifier,
+	 * or SPDK_CLIENT_GLOBAL_NS_TAG.
+	 * \param payload The pointer to the payload buffer.
+	 * \param payload_size The size of payload buffer.
+	 * \param offset Offset in bytes within the log page to start retrieving log page
+	 * data. May only be non-zero if the controller supports extended data for Get Log
+	 * Page as reported in the controller data log page attributes.
+	 * \param cb_fn Callback function to invoke when the log page has been retrieved.
+	 * \param cb_arg Argument to pass to the callback function.
+	 *
+	 * \return 0 if successfully submitted, negated errno if resources could not be
+	 * allocated for this request, -ENXIO if the admin qpair is failed at the transport layer.
+	 */
+	int spdk_client_ctrlr_cmd_get_log_page(struct spdk_client_ctrlr *ctrlr,
+										   uint8_t log_page, uint32_t nsid,
+										   void *payload, uint32_t payload_size,
+										   uint64_t offset,
+										   spdk_req_cmd_cb cb_fn, void *cb_arg);
+
+	/**
+	 * Get a specific log page from the Client controller.
+	 *
+	 * This function is thread safe and can be called at any point while the controller
+	 * is attached to the SPDK Client driver.
+	 *
+	 * This function allows specifying extra fields in cdw10 and cdw11 such as
+	 * Retain Asynchronous Event and Log Specific Field.
+	 *
+	 * Call spdk_client_ctrlr_process_admin_completions() to poll for completion of
+	 * commands submitted through this function.
+	 *
+	 * \sa spdk_client_ctrlr_is_log_page_supported()
+	 *
+	 * \param ctrlr Opaque handle to Client controller.
+	 * \param log_page The log page identifier.
+	 * \param nsid Depending on the log page, this may be 0, a namespace identifier,
+	 * or SPDK_CLIENT_GLOBAL_NS_TAG.
+	 * \param payload The pointer to the payload buffer.
+	 * \param payload_size The size of payload buffer.
+	 * \param offset Offset in bytes within the log page to start retrieving log page
+	 * data. May only be non-zero if the controller supports extended data for Get Log
+	 * Page as reported in the controller data log page attributes.
+	 * \param cdw10 Value to specify for cdw10.  Specify 0 for numdl - it will be
+	 * set by this function based on the payload_size parameter.  Specify 0 for lid -
+	 * it will be set by this function based on the log_page parameter.
+	 * \param cdw11 Value to specify for cdw11.  Specify 0 for numdu - it will be
+	 * set by this function based on the payload_size.
+	 * \param cdw14 Value to specify for cdw14.
+	 * \param cb_fn Callback function to invoke when the log page has been retrieved.
+	 * \param cb_arg Argument to pass to the callback function.
+	 *
+	 * \return 0 if successfully submitted, negated errno if resources could not be
+	 * allocated for this request, -ENXIO if the admin qpair is failed at the transport layer.
+	 */
+	int spdk_client_ctrlr_cmd_get_log_page_ext(struct spdk_client_ctrlr *ctrlr, uint8_t log_page,
+											   uint32_t nsid, void *payload, uint32_t payload_size,
+											   uint64_t offset, uint32_t cdw10, uint32_t cdw11,
+											   uint32_t cdw14, spdk_req_cmd_cb cb_fn, void *cb_arg);
+
+	/**
+	 * Abort a specific previously-submitted Client command.
+	 *
+	 * \sa spdk_client_ctrlr_register_timeout_callback()
+	 *
+	 * \param ctrlr Client controller to which the command was submitted.
+	 * \param qpair Client queue pair to which the command was submitted. For admin
+	 *  commands, pass NULL for the qpair.
+	 * \param cid Command ID of the command to abort.
+	 * \param cb_fn Callback function to invoke when the abort has completed.
+	 * \param cb_arg Argument to pass to the callback function.
+	 *
+	 * \return 0 if successfully submitted, negated errno if resources could not be
+	 * allocated for this request, -ENXIO if the admin qpair is failed at the transport layer.
+	 */
+	int spdk_client_ctrlr_cmd_abort(struct spdk_client_ctrlr *ctrlr,
+									struct spdk_client_qpair *qpair,
+									uint16_t cid,
+									spdk_req_cmd_cb cb_fn,
+									void *cb_arg);
+
+	/**
+	 * Abort previously submitted commands which have cmd_cb_arg as its callback argument.
+	 *
+	 * \param ctrlr Client controller to which the commands were submitted.
+	 * \param qpair Client queue pair to which the commands were submitted. For admin
+	 * commands, pass NULL for the qpair.
+	 * \param cmd_cb_arg Callback argument for the Client commands which this function
+	 * attempts to abort.
+	 * \param cb_fn Callback function to invoke when this function has completed.
+	 * \param cb_arg Argument to pass to the callback function.
+	 *
+	 * \return 0 if successfully submitted, negated errno otherwise.
+	 */
+	int spdk_client_ctrlr_cmd_abort_ext(struct spdk_client_ctrlr *ctrlr,
+										struct spdk_client_qpair *qpair,
+										void *cmd_cb_arg,
+										spdk_req_cmd_cb cb_fn,
+										void *cb_arg);
+
+	/**
+	 * Set specific feature for the given Client controller.
+	 *
+	 * This function is thread safe and can be called at any point while the controller
+	 * is attached to the SPDK Client driver.
+	 *
+	 * Call spdk_client_ctrlr_process_admin_completions() to poll for completion of
+	 * commands submitted through this function.
+	 *
+	 * \sa spdk_client_ctrlr_cmd_get_feature().
+	 *
+	 * \param ctrlr Client controller to manipulate.
+	 * \param feature The feature identifier.
+	 * \param cdw11 as defined by the specification for this command.
+	 * \param cdw12 as defined by the specification for this command.
+	 * \param payload The pointer to the payload buffer.
+	 * \param payload_size The size of payload buffer.
+	 * \param cb_fn Callback function to invoke when the feature has been set.
+	 * \param cb_arg Argument to pass to the callback function.
+	 *
+	 * \return 0 if successfully submitted, negated errno if resources could not be
+	 * allocated for this request, -ENXIO if the admin qpair is failed at the transport layer.
+	 */
+	int spdk_client_ctrlr_cmd_set_feature(struct spdk_client_ctrlr *ctrlr,
+										  uint8_t feature, uint32_t cdw11, uint32_t cdw12,
+										  void *payload, uint32_t payload_size,
+										  spdk_req_cmd_cb cb_fn, void *cb_arg);
+
+	/**
+	 * Get specific feature from given Client controller.
+	 *
+	 * This function is thread safe and can be called at any point while the controller
+	 * is attached to the SPDK Client driver.
+	 *
+	 * Call spdk_client_ctrlr_process_admin_completions() to poll for completion of
+	 * commands submitted through this function.
+	 *
+	 * \sa spdk_client_ctrlr_cmd_set_feature()
+	 *
+	 * \param ctrlr Client controller to query.
+	 * \param feature The feature identifier.
+	 * \param cdw11 as defined by the specification for this command.
+	 * \param payload The pointer to the payload buffer.
+	 * \param payload_size The size of payload buffer.
+	 * \param cb_fn Callback function to invoke when the feature has been retrieved.
+	 * \param cb_arg Argument to pass to the callback function.
+	 *
+	 * \return 0 if successfully submitted, -ENOMEM if resources could not be allocated
+	 * for this request, -ENXIO if the admin qpair is failed at the transport layer.
+	 */
+	int spdk_client_ctrlr_cmd_get_feature(struct spdk_client_ctrlr *ctrlr,
+										  uint8_t feature, uint32_t cdw11,
+										  void *payload, uint32_t payload_size,
+										  spdk_req_cmd_cb cb_fn, void *cb_arg);
+
+	/**
+	 * Get specific feature from given Client controller.
+	 *
+	 * \param ctrlr Client controller to query.
+	 * \param feature The feature identifier.
+	 * \param cdw11 as defined by the specification for this command.
+	 * \param payload The pointer to the payload buffer.
+	 * \param payload_size The size of payload buffer.
+	 * \param cb_fn Callback function to invoke when the feature has been retrieved.
+	 * \param cb_arg Argument to pass to the callback function.
+	 * \param ns_id The namespace identifier.
+	 *
+	 * \return 0 if successfully submitted, -ENOMEM if resources could not be allocated
+	 * for this request, -ENXIO if the admin qpair is failed at the transport layer.
+	 *
+	 * This function is thread safe and can be called at any point while the controller
+	 * is attached to the SPDK Client driver.
+	 *
+	 * Call \ref spdk_client_ctrlr_process_admin_completions() to poll for completion
+	 * of commands submitted through this function.
+	 *
+	 * \sa spdk_client_ctrlr_cmd_set_feature_ns()
+	 */
+	int spdk_client_ctrlr_cmd_get_feature_ns(struct spdk_client_ctrlr *ctrlr, uint8_t feature,
+											 uint32_t cdw11, void *payload, uint32_t payload_size,
+											 spdk_req_cmd_cb cb_fn, void *cb_arg, uint32_t ns_id);
+
+	/**
+	 * Set specific feature for the given Client controller and namespace ID.
+	 *
+	 * \param ctrlr Client controller to manipulate.
+	 * \param feature The feature identifier.
+	 * \param cdw11 as defined by the specification for this command.
+	 * \param cdw12 as defined by the specification for this command.
+	 * \param payload The pointer to the payload buffer.
+	 * \param payload_size The size of payload buffer.
+	 * \param cb_fn Callback function to invoke when the feature has been set.
+	 * \param cb_arg Argument to pass to the callback function.
+	 * \param ns_id The namespace identifier.
+	 *
+	 * \return 0 if successfully submitted, -ENOMEM if resources could not be allocated
+	 * for this request, -ENXIO if the admin qpair is failed at the transport layer.
+	 *
+	 * This function is thread safe and can be called at any point while the controller
+	 * is attached to the SPDK Client driver.
+	 *
+	 * Call \ref spdk_client_ctrlr_process_admin_completions() to poll for completion
+	 * of commands submitted through this function.
+	 *
+	 * \sa spdk_client_ctrlr_cmd_get_feature_ns()
+	 */
+	int spdk_client_ctrlr_cmd_set_feature_ns(struct spdk_client_ctrlr *ctrlr, uint8_t feature,
+											 uint32_t cdw11, uint32_t cdw12, void *payload,
+											 uint32_t payload_size, spdk_req_cmd_cb cb_fn,
+											 void *cb_arg, uint32_t ns_id);
+
+	/**
+	 * Receive security protocol data from controller.
+	 *
+	 * This function is thread safe and can be called at any point after spdk_client_probe().
+	 *
+	 * \param ctrlr Client controller to use for security receive command submission.
+	 * \param secp Security Protocol that is used.
+	 * \param spsp Security Protocol Specific field.
+	 * \param nssf Client Security Specific field. Indicate RPMB target when using Security
+	 * Protocol EAh.
+	 * \param payload The pointer to the payload buffer.
+	 * \param payload_size The size of payload buffer.
+	 * \param cb_fn Callback function to invoke when the command has been completed.
+	 * \param cb_arg Argument to pass to the callback function.
+	 *
+	 * \return 0 if successfully submitted, negated errno if resources could not be allocated
+	 * for this request.
+	 */
+	int spdk_client_ctrlr_cmd_security_receive(struct spdk_client_ctrlr *ctrlr, uint8_t secp,
+											   uint16_t spsp, uint8_t nssf, void *payload,
+											   uint32_t payload_size,
+											   spdk_req_cmd_cb cb_fn, void *cb_arg);
+
+	/**
+	 * Send security protocol data to controller.
+	 *
+	 * This function is thread safe and can be called at any point after spdk_client_probe().
+	 *
+	 * \param ctrlr Client controller to use for security send command submission.
+	 * \param secp Security Protocol that is used.
+	 * \param spsp Security Protocol Specific field.
+	 * \param nssf Client Security Specific field. Indicate RPMB target when using Security
+	 * Protocol EAh.
+	 * \param payload The pointer to the payload buffer.
+	 * \param payload_size The size of payload buffer.
+	 * \param cb_fn Callback function to invoke when the command has been completed.
+	 * \param cb_arg Argument to pass to the callback function.
+	 *
+	 * \return 0 if successfully submitted, negated errno if resources could not be allocated
+	 * for this request.
+	 */
+	int spdk_client_ctrlr_cmd_security_send(struct spdk_client_ctrlr *ctrlr, uint8_t secp,
+											uint16_t spsp, uint8_t nssf, void *payload,
+											uint32_t payload_size, spdk_req_cmd_cb cb_fn, void *cb_arg);
+
+	/**
+	 * Receive security protocol data from controller.
+	 *
+	 * This function is thread safe and can be called at any point after spdk_client_probe().
+	 *
+	 * \param ctrlr Client controller to use for security receive command submission.
+	 * \param secp Security Protocol that is used.
+	 * \param spsp Security Protocol Specific field.
+	 * \param nssf Client Security Specific field. Indicate RPMB target when using Security
+	 * Protocol EAh.
+	 * \param payload The pointer to the payload buffer.
+	 * \param size The size of payload buffer.
+	 *
+	 * \return 0 if successfully submitted, negated errno if resources could not be allocated
+	 * for this request.
+	 */
+	int spdk_client_ctrlr_security_receive(struct spdk_client_ctrlr *ctrlr, uint8_t secp,
+										   uint16_t spsp, uint8_t nssf, void *payload, size_t size);
+
+	/**
+	 * Send security protocol data to controller.
+	 *
+	 * This function is thread safe and can be called at any point after spdk_client_probe().
+	 *
+	 * \param ctrlr Client controller to use for security send command submission.
+	 * \param secp Security Protocol that is used.
+	 * \param spsp Security Protocol Specific field.
+	 * \param nssf Client Security Specific field. Indicate RPMB target when using Security
+	 * Protocol EAh.
+	 * \param payload The pointer to the payload buffer.
+	 * \param size The size of payload buffer.
+	 *
+	 * \return 0 if successfully submitted, negated errno if resources could not be allocated
+	 * for this request.
+	 */
+	int spdk_client_ctrlr_security_send(struct spdk_client_ctrlr *ctrlr, uint8_t secp,
+										uint16_t spsp, uint8_t nssf, void *payload, size_t size);
+
+	/**
+	 * Receive data related to a specific Directive Type from the controller.
+	 *
+	 * This function is thread safe and can be called at any point after spdk_client_probe().
+	 *
+	 * Call spdk_client_ctrlr_process_admin_completions() to poll for completion of
+	 * commands submitted through this function.
+	 *
+	 * \param ctrlr Client controller to use for directive receive command submission.
+	 * \param nsid Specific Namespace Identifier.
+	 * \param doper Directive Operation defined in client_spec.h.
+	 * \param dtype Directive Type defined in client_spec.h.
+	 * \param dspec Directive Specific defined in client_spec.h.
+	 * \param payload The pointer to the payload buffer.
+	 * \param payload_size The size of payload buffer.
+	 * \param cdw12 Command dword 12.
+	 * \param cdw13 Command dword 13.
+	 * \param cb_fn Callback function to invoke when the command has been completed.
+	 * \param cb_arg Argument to pass to the callback function.
+	 *
+	 * \return 0 if successfully submitted, negated errno if resources could not be allocated
+	 * for this request.
+	 */
+	int spdk_client_ctrlr_cmd_directive_receive(struct spdk_client_ctrlr *ctrlr, uint32_t nsid,
+												uint32_t doper, uint32_t dtype, uint32_t dspec,
+												void *payload, uint32_t payload_size, uint32_t cdw12,
+												uint32_t cdw13, spdk_req_cmd_cb cb_fn, void *cb_arg);
+
+	/**
+	 * Send data related to a specific Directive Type to the controller.
+	 *
+	 * This function is thread safe and can be called at any point after spdk_client_probe().
+	 *
+	 * Call spdk_client_ctrlr_process_admin_completions() to poll for completion of
+	 * commands submitted through this function.
+	 *
+	 * \param ctrlr Client controller to use for directive send command submission.
+	 * \param nsid Specific Namespace Identifier.
+	 * \param doper Directive Operation defined in client_spec.h.
+	 * \param dtype Directive Type defined in client_spec.h.
+	 * \param dspec Directive Specific defined in client_spec.h.
+	 * \param payload The pointer to the payload buffer.
+	 * \param payload_size The size of payload buffer.
+	 * \param cdw12 Command dword 12.
+	 * \param cdw13 Command dword 13.
+	 * \param cb_fn Callback function to invoke when the command has been completed.
+	 * \param cb_arg Argument to pass to the callback function.
+	 *
+	 * \return 0 if successfully submitted, negated errno if resources could not be allocated
+	 * for this request.
+	 */
+	int spdk_client_ctrlr_cmd_directive_send(struct spdk_client_ctrlr *ctrlr, uint32_t nsid,
+											 uint32_t doper, uint32_t dtype, uint32_t dspec,
+											 void *payload, uint32_t payload_size, uint32_t cdw12,
+											 uint32_t cdw13, spdk_req_cmd_cb cb_fn, void *cb_arg);
+
+	/**
+	 * Get supported flags of the controller.
+	 *
+	 * \param ctrlr Client controller to get flags.
+	 *
+	 * \return supported flags of this controller.
+	 */
+	uint64_t spdk_client_ctrlr_get_flags(struct spdk_client_ctrlr *ctrlr);
+
+	/**
+	 * Attach the specified namespace to controllers.
+	 *
+	 * This function is thread safe and can be called at any point after spdk_client_probe().
+	 *
+	 * \param ctrlr Client controller to use for command submission.
+	 * \param nsid Namespace identifier for namespace to attach.
+	 * \param payload The pointer to the controller list.
+	 *
+	 * \return 0 if successfully submitted, ENOMEM if resources could not be allocated
+	 * for this request.
+	 */
+	int spdk_client_ctrlr_attach_ns(struct spdk_client_ctrlr *ctrlr, uint32_t nsid,
+									struct spdk_client_ctrlr_list *payload);
+
+	/**
+	 * Detach the specified namespace from controllers.
+	 *
+	 * This function is thread safe and can be called at any point after spdk_client_probe().
+	 *
+	 * \param ctrlr Client controller to use for command submission.
+	 * \param nsid Namespace ID to detach.
+	 * \param payload The pointer to the controller list.
+	 *
+	 * \return 0 if successfully submitted, ENOMEM if resources could not be allocated
+	 * for this request
+	 */
+	int spdk_client_ctrlr_detach_ns(struct spdk_client_ctrlr *ctrlr, uint32_t nsid,
+									struct spdk_client_ctrlr_list *payload);
+
+	/**
+	 * Delete a namespace.
+	 *
+	 * This function is thread safe and can be called at any point after spdk_client_probe().
+	 *
+	 * \param ctrlr Client controller to delete namespace from.
+	 * \param nsid The namespace identifier.
+	 *
+	 * \return 0 if successfully submitted, negated errno if resources could not be
+	 * allocated
+	 * for this request
+	 */
+	int spdk_client_ctrlr_delete_ns(struct spdk_client_ctrlr *ctrlr, uint32_t nsid);
+
+	/**
+	 * Format NVM.
+	 *
+	 * This function requests a low-level format of the media.
+	 *
+	 * This function is thread safe and can be called at any point after spdk_client_probe().
+	 *
+	 * \param ctrlr Client controller to format.
+	 * \param nsid The namespace identifier. May be SPDK_CLIENT_GLOBAL_NS_TAG to format
+	 * all namespaces.
+	 * \param format The format information for the command.
+	 *
+	 * \return 0 if successfully submitted, negated errno if resources could not be
+	 * allocated for this request
+	 */
+	int spdk_client_ctrlr_format(struct spdk_client_ctrlr *ctrlr, uint32_t nsid,
+								 struct spdk_client_format *format);
+
+	/**
+	 * Start the Read from a Boot Partition.
+	 *
+	 * This function is thread safe and can be called at any point after spdk_client_probe().
+	 *
+	 * \param ctrlr Client controller to perform the Boot Partition read.
+	 * \param payload The data buffer for Boot Partition read.
+	 * \param bprsz Read size in multiples of 4 KiB to copy into the Boot Partition Memory Buffer.
+	 * \param bprof Boot Partition offset to read from in 4 KiB units.
+	 * \param bpid Boot Partition identifier for the Boot Partition read operation.
+	 *
+	 * \return 0 if Boot Partition read is successful. Negated errno on the following error conditions:
+	 * -ENOMEM: if resources could not be allocated.
+	 * -ENOTSUP: Boot Partition is not supported by the Controller.
+	 * -EIO: Registers access failure.
+	 * -EINVAL: Parameters are invalid.
+	 * -EFAULT: Invalid address was specified as part of payload.
+	 * -EALREADY: Boot Partition read already initiated.
+	 */
+	int spdk_client_ctrlr_read_boot_partition_start(struct spdk_client_ctrlr *ctrlr, void *payload,
+													uint32_t bprsz, uint32_t bprof, uint32_t bpid);
+
+	/**
+	 * Poll the status of the Read from a Boot Partition.
+	 *
+	 * This function is thread safe and can be called at any point after spdk_client_probe().
+	 *
+	 * \param ctrlr Client controller to perform the Boot Partition read.
+	 *
+	 * \return 0 if Boot Partition read is successful. Negated errno on the following error conditions:
+	 * -EIO: Registers access failure.
+	 * -EINVAL: Invalid read status or the Boot Partition read is not initiated yet.
+	 * -EAGAIN: If the read is still in progress; users must call
+	 * spdk_client_ctrlr_read_boot_partition_poll again to check the read status.
+	 */
+	int spdk_client_ctrlr_read_boot_partition_poll(struct spdk_client_ctrlr *ctrlr);
+
+	/**
+	 * Write to a Boot Partition.
+	 *
+	 * This function is thread safe and can be called at any point after spdk_client_probe().
+	 * Users will get the completion after the data is downloaded, image is replaced and
+	 * Boot Partition is activated or when the sequence encounters an error.
+	 *
+	 * \param ctrlr Client controller to perform the Boot Partition write.
+	 * \param payload The data buffer for Boot Partition write.
+	 * \param size Data size to write to the Boot Partition.
+	 * \param bpid Boot Partition identifier for the Boot Partition write operation.
+	 * \param cb_fn Callback function to invoke when the operation is completed.
+	 * \param cb_arg Argument to pass to the callback function.
+	 *
+	 * \return 0 if Boot Partition write submit is successful. Negated errno on the following error conditions:
+	 * -ENOMEM: if resources could not be allocated.
+	 * -ENOTSUP: Boot Partition is not supported by the Controller.
+	 * -EIO: Registers access failure.
+	 * -EINVAL: Parameters are invalid.
+	 */
+	int spdk_client_ctrlr_write_boot_partition(struct spdk_client_ctrlr *ctrlr, void *payload,
+											   uint32_t size, uint32_t bpid, spdk_req_cmd_cb cb_fn, void *cb_arg);
+
+	/**
+	 * Return virtual address of PCIe NVM I/O registers
+	 *
+	 * This function returns a pointer to the PCIe I/O registers for a controller
+	 * or NULL if unsupported for this transport.
+	 *
+	 * \param ctrlr Controller whose registers are to be accessed.
+	 *
+	 * \return Pointer to virtual address of register bank, or NULL.
+	 */
+	volatile struct spdk_client_registers *spdk_client_ctrlr_get_registers(struct spdk_client_ctrlr *ctrlr);
+
+	/**
+	 * Reserve the controller memory buffer for data transfer use.
+	 *
+	 * This function reserves the full size of the controller memory buffer
+	 * for use in data transfers. If submission queues or completion queues are
+	 * already placed in the controller memory buffer, this call will fail.
+	 *
+	 * \param ctrlr Controller from which to allocate memory buffer
+	 *
+	 * \return The size of the controller memory buffer on success. Negated errno
+	 * on failure.
+	 */
+	int spdk_client_ctrlr_reserve_cmb(struct spdk_client_ctrlr *ctrlr);
+
+	/**
+	 * Map a previously reserved controller memory buffer so that it's data is
+	 * visible from the CPU. This operation is not always possible.
+	 *
+	 * \param ctrlr Controller that contains the memory buffer
+	 * \param size Size of buffer that was mapped.
+	 *
+	 * \return Pointer to controller memory buffer, or NULL on failure.
+	 */
+	void *spdk_client_ctrlr_map_cmb(struct spdk_client_ctrlr *ctrlr, size_t *size);
+
+	/**
+	 * Free a controller memory I/O buffer.
+	 *
+	 * \param ctrlr Controller from which to unmap the memory buffer.
+	 */
+	void spdk_client_ctrlr_unmap_cmb(struct spdk_client_ctrlr *ctrlr);
+
+	/**
+	 * Enable the Persistent Memory Region
+	 *
+	 * \param ctrlr Controller that contains the Persistent Memory Region
+	 *
+	 * \return 0 on success. Negated errno on the following error conditions:
+	 * -ENOTSUP: PMR is not supported by the Controller.
+	 * -EIO: Registers access failure.
+	 * -EINVAL: PMR Time Units Invalid or PMR is already enabled.
+	 * -ETIMEDOUT: Timed out to Enable PMR.
+	 * -ENOSYS: Transport does not support Enable PMR function.
+	 */
+	int spdk_client_ctrlr_enable_pmr(struct spdk_client_ctrlr *ctrlr);
+
+	/**
+	 * Disable the Persistent Memory Region
+	 *
+	 * \param ctrlr Controller that contains the Persistent Memory Region
+	 *
+	 * \return 0 on success. Negated errno on the following error conditions:
+	 * -ENOTSUP: PMR is not supported by the Controller.
+	 * -EIO: Registers access failure.
+	 * -EINVAL: PMR Time Units Invalid or PMR is already disabled.
+	 * -ETIMEDOUT: Timed out to Disable PMR.
+	 * -ENOSYS: Transport does not support Disable PMR function.
+	 */
+	int spdk_client_ctrlr_disable_pmr(struct spdk_client_ctrlr *ctrlr);
+
+	/**
+	 * Map the Persistent Memory Region so that it's data is
+	 * visible from the CPU.
+	 *
+	 * \param ctrlr Controller that contains the Persistent Memory Region
+	 * \param size Size of the region that was mapped.
+	 *
+	 * \return Pointer to Persistent Memory Region, or NULL on failure.
+	 */
+	void *spdk_client_ctrlr_map_pmr(struct spdk_client_ctrlr *ctrlr, size_t *size);
+
+	/**
+	 * Free the Persistent Memory Region.
+	 *
+	 * \param ctrlr Controller from which to unmap the Persistent Memory Region.
+	 *
+	 * \return 0 on success, negative errno on failure.
+	 * -ENXIO: Either PMR is not supported by the Controller or the PMR is already unmapped.
+	 * -ENOSYS: Transport does not support Unmap PMR function.
+	 */
+	int spdk_client_ctrlr_unmap_pmr(struct spdk_client_ctrlr *ctrlr);
+
+	/**
+	 * \brief Alloc Client I/O queue identifier.
+	 *
+	 * This function is only needed for the non-standard case of allocating queues using the raw
+	 * command interface. In most cases \ref spdk_client_ctrlr_alloc_io_qpair should be sufficient.
+	 *
+	 * \param ctrlr Opaque handle to Client controller.
+	 * \return qid on success, -1 on failure.
+	 */
+	int32_t spdk_client_ctrlr_alloc_qid(struct spdk_client_ctrlr *ctrlr);
+
+	/**
+	 * \brief Free Client I/O queue identifier.
+	 *
+	 * This function must only be called with qids previously allocated with \ref spdk_client_ctrlr_alloc_qid.
+	 *
+	 * \param ctrlr Opaque handle to Client controller.
+	 * \param qid Client Queue Identifier.
+	 */
+	void spdk_client_ctrlr_free_qid(struct spdk_client_ctrlr *ctrlr, uint16_t qid);
+
+	/**
+	 * Opaque handle for a poll group. A poll group is a collection of spdk_client_qpair
+	 * objects that are polled for completions as a unit.
+	 *
+	 * Returned by spdk_client_poll_group_create().
+	 */
+	struct spdk_client_poll_group;
+
+	/**
+	 * This function alerts the user to disconnected qpairs when calling
+	 * spdk_client_poll_group_process_completions.
+	 */
+	typedef void (*spdk_client_disconnected_qpair_cb)(struct spdk_client_qpair *qpair,
+													  void *poll_group_ctx);
+
+	/**
+	 * Create a new poll group.
+	 *
+	 * \param ctx A user supplied context that can be retrieved later with spdk_client_poll_group_get_ctx
+	 * \param table The call back table defined by users which contains the accelerated functions
+	 * which can be used to accelerate some operations such as crc32c.
+	 *
+	 * \return Pointer to the new poll group, or NULL on error.
+	 */
+	struct spdk_client_poll_group *spdk_client_poll_group_create(void *ctx,
+																 struct spdk_client_accel_fn_table *table);
+
+	/**
+	 * Get a optimal poll group.
+	 *
+	 * \param qpair The qpair to get the optimal poll group.
+	 *
+	 * \return Pointer to the optimal poll group, or NULL if not found.
+	 */
+	struct spdk_client_poll_group *spdk_client_qpair_get_optimal_poll_group(struct spdk_client_qpair *qpair);
+
+	/**
+	 * Add an spdk_client_qpair to a poll group. qpairs may only be added to
+	 * a poll group if they are in the disconnected state; i.e. either they were
+	 * just allocated and not yet connected or they have been disconnected with a call
+	 * to spdk_client_ctrlr_disconnect_io_qpair.
+	 *
+	 * \param group The group to which the qpair will be added.
+	 * \param qpair The qpair to add to the poll group.
+	 *
+	 * return 0 on success, -EINVAL if the qpair is not in the disabled state, -ENODEV if the transport
+	 * doesn't exist, -ENOMEM on memory allocation failures, or -EPROTO on a protocol (transport) specific failure.
+	 */
+	int spdk_client_poll_group_add(struct spdk_client_poll_group *group, struct spdk_client_qpair *qpair);
+
+	/**
+	 * Remove a disconnected spdk_client_qpair from a poll group.
+	 *
+	 * \param group The group from which to remove the qpair.
+	 * \param qpair The qpair to remove from the poll group.
+	 *
+	 * return 0 on success, -ENOENT if the qpair is not found in the group, -EINVAL if the qpair is not
+	 * disconnected in the group, or -EPROTO on a protocol (transport) specific failure.
+	 */
+	int spdk_client_poll_group_remove(struct spdk_client_poll_group *group, struct spdk_client_qpair *qpair);
+
+	/**
+	 * Destroy an empty poll group.
+	 *
+	 * \param group The group to destroy.
+	 *
+	 * return 0 on success, -EBUSY if the poll group is not empty.
+	 */
+	int spdk_client_poll_group_destroy(struct spdk_client_poll_group *group);
+
+	/**
+	 * Poll for completions on all qpairs in this poll group.
+	 *
+	 * the disconnected_qpair_cb will be called for all disconnected qpairs in the poll group
+	 * including qpairs which fail within the context of this call.
+	 * The user is responsible for trying to reconnect or destroy those qpairs.
+	 *
+	 * \param group The group on which to poll for completions.
+	 * \param completions_per_qpair The maximum number of completions per qpair.
+	 * \param disconnected_qpair_cb A callback function of type spdk_client_disconnected_qpair_cb. Must be non-NULL.
+	 *
+	 * return The number of completions across all qpairs, -EINVAL if no disconnected_qpair_cb is passed, or
+	 * -EIO if the shared completion queue cannot be polled for the RDMA transport.
+	 */
+	int64_t spdk_client_poll_group_process_completions(struct spdk_client_poll_group *group,
+													   uint32_t completions_per_qpair, spdk_client_disconnected_qpair_cb disconnected_qpair_cb);
+
+	/**
+	 * Retrieve the user context for this specific poll group.
+	 *
+	 * \param group The poll group from which to retrieve the context.
+	 *
+	 * \return A pointer to the user provided poll group context.
+	 */
+	void *spdk_client_poll_group_get_ctx(struct spdk_client_poll_group *group);
+
+	/**
+	 * Retrieves transport statistics for the given poll group.
+	 *
+	 * Note: the structure returned by this function should later be freed with
+	 * @b spdk_client_poll_group_free_stats function
+	 *
+	 * \param group Pointer to CLIENT poll group
+	 * \param stats Double pointer to statistics to be filled by this function
+	 * \return 0 on success or negated errno on failure
+	 */
+	int spdk_client_poll_group_get_stats(struct spdk_client_poll_group *group,
+										 struct spdk_client_poll_group_stat **stats);
+
+	/**
+	 * Frees poll group statistics retrieved using @b spdk_client_poll_group_get_stats function
+	 *
+	 * @param group Pointer to a poll group
+	 * @param stat Pointer to statistics to be released
+	 */
+	void spdk_client_poll_group_free_stats(struct spdk_client_poll_group *group,
+										   struct spdk_client_poll_group_stat *stat);
+
+	/**
+	 * Get the identify namespace data as defined by the Client specification.
+	 *
+	 * This function is thread safe and can be called at any point while the controller
+	 * is attached to the SPDK Client driver.
+	 *
+	 * \param ns Namespace.
+	 *
+	 * \return a pointer to the namespace data.
+	 */
+	const struct spdk_client_ns_data *spdk_client_ns_get_data(struct spdk_client_ns *ns);
+
+	/**
+	 * Get the namespace id (index number) from the given namespace handle.
+	 *
+	 * This function is thread safe and can be called at any point while the controller
+	 * is attached to the SPDK Client driver.
+	 *
+	 * \param ns Namespace.
+	 *
+	 * \return namespace id.
+	 */
+	uint32_t spdk_client_ns_get_id(struct spdk_client_ns *ns);
+
+	/**
+	 * Get the controller with which this namespace is associated.
+	 *
+	 * This function is thread safe and can be called at any point while the controller
+	 * is attached to the SPDK Client driver.
+	 *
+	 * \param ns Namespace.
+	 *
+	 * \return a pointer to the controller.
+	 */
+	struct spdk_client_ctrlr *spdk_client_ns_get_ctrlr(struct spdk_client_ns *ns);
+
+	/**
+	 * Determine whether a namespace is active.
+	 *
+	 * Inactive namespaces cannot be the target of I/O commands.
+	 *
+	 * \param ns Namespace to query.
+	 *
+	 * \return true if active, or false if inactive.
+	 */
+	bool spdk_client_ns_is_active(struct spdk_client_ns *ns);
+
+	/**
+	 * Get the maximum transfer size, in bytes, for an I/O sent to the given namespace.
+	 *
+	 * This function is thread safe and can be called at any point while the controller
+	 * is attached to the SPDK Client driver.
+	 *
+	 * \param ns Namespace to query.
+	 *
+	 * \return the maximum transfer size in bytes.
+	 */
+	uint32_t spdk_client_ns_get_max_io_xfer_size(struct spdk_client_ns *ns);
+
+	/**
+	 * Get the sector size, in bytes, of the given namespace.
+	 *
+	 * This function returns the size of the data sector only.  It does not
+	 * include metadata size.
+	 *
+	 * This function is thread safe and can be called at any point while the controller
+	 * is attached to the SPDK Client driver.
+	 *
+	 * \param ns Namespace to query.
+	 *
+	 * /return the sector size in bytes.
+	 */
+	uint32_t spdk_client_ns_get_sector_size(struct spdk_client_ns *ns);
+
+	/**
+	 * Get the extended sector size, in bytes, of the given namespace.
+	 *
+	 * This function returns the size of the data sector plus metadata.
+	 *
+	 * This function is thread safe and can be called at any point while the controller
+	 * is attached to the SPDK Client driver.
+	 *
+	 * \param ns Namespace to query.
+	 *
+	 * /return the extended sector size in bytes.
+	 */
+	uint32_t spdk_client_ns_get_extended_sector_size(struct spdk_client_ns *ns);
+
+	/**
+	 * Get the number of sectors for the given namespace.
+	 *
+	 * This function is thread safe and can be called at any point while the controller
+	 * is attached to the SPDK Client driver.
+	 *
+	 * \param ns Namespace to query.
+	 *
+	 * \return the number of sectors.
+	 */
+	uint64_t spdk_client_ns_get_num_sectors(struct spdk_client_ns *ns);
+
+	/**
+	 * Get the size, in bytes, of the given namespace.
+	 *
+	 * This function is thread safe and can be called at any point while the controller
+	 * is attached to the SPDK Client driver.
+	 *
+	 * \param ns Namespace to query.
+	 *
+	 * \return the size of the given namespace in bytes.
+	 */
+	uint64_t spdk_client_ns_get_size(struct spdk_client_ns *ns);
+
+	/**
+	 * Get the metadata size, in bytes, of the given namespace.
+	 *
+	 * This function is thread safe and can be called at any point while the controller
+	 * is attached to the SPDK Client driver.
+	 *
+	 * \param ns Namespace to query.
+	 *
+	 * \return the metadata size of the given namespace in bytes.
+	 */
+	uint32_t spdk_client_ns_get_md_size(struct spdk_client_ns *ns);
+
+	/**
+	 * Check whether if the namespace can support extended LBA when end-to-end data
+	 * protection enabled.
+	 *
+	 * This function is thread safe and can be called at any point while the controller
+	 * is attached to the SPDK Client driver.
+	 *
+	 * \param ns Namespace to query.
+	 *
+	 * \return true if the namespace can support extended LBA when end-to-end data
+	 * protection enabled, or false otherwise.
+	 */
+	bool spdk_client_ns_supports_extended_lba(struct spdk_client_ns *ns);
+
+	/**
+	 * Check whether if the namespace supports compare operation
+	 *
+	 * This function is thread safe and can be called at any point while the controller
+	 * is attached to the SPDK Client driver.
+	 *
+	 * \param ns Namespace to query.
+	 *
+	 * \return true if the namespace supports compare operation, or false otherwise.
+	 */
+	bool spdk_client_ns_supports_compare(struct spdk_client_ns *ns);
+
+	/**
+	 * Get the optimal I/O boundary, in blocks, for the given namespace.
+	 *
+	 * Read and write commands should not cross the optimal I/O boundary for best
+	 * performance.
+	 *
+	 * \param ns Namespace to query.
+	 *
+	 * \return Optimal granularity of I/O commands, in blocks, or 0 if no optimal
+	 * granularity is reported.
+	 */
+	uint32_t spdk_client_ns_get_optimal_io_boundary(struct spdk_client_ns *ns);
+
+	/**
+	 * Get the NGUID for the given namespace.
+	 *
+	 * \param ns Namespace to query.
+	 *
+	 * \return a pointer to namespace NGUID, or NULL if ns does not have a NGUID.
+	 */
+	const uint8_t *spdk_client_ns_get_nguid(const struct spdk_client_ns *ns);
+
+	/**
+	 * Get the UUID for the given namespace.
+	 *
+	 * \param ns Namespace to query.
+	 *
+	 * \return a pointer to namespace UUID, or NULL if ns does not have a UUID.
+	 */
+	const struct spdk_uuid *spdk_client_ns_get_uuid(const struct spdk_client_ns *ns);
+
+	/**
+	 * \brief Namespace command support flags.
+	 */
+	enum spdk_client_ns_flags
+	{
+		SPDK_CLIENT_NS_DEALLOCATE_SUPPORTED = 1 << 0,		   /**< The deallocate command is supported */
+		SPDK_CLIENT_NS_FLUSH_SUPPORTED = 1 << 1,			   /**< The flush command is supported */
+		SPDK_CLIENT_NS_RESERVATION_SUPPORTED = 1 << 2,		   /**< The reservation command is supported */
+		SPDK_CLIENT_NS_WRITE_ZEROES_SUPPORTED = 1 << 3,		   /**< The write zeroes command is supported */
+		SPDK_CLIENT_NS_DPS_PI_SUPPORTED = 1 << 4,			   /**< The end-to-end data protection is supported */
+		SPDK_CLIENT_NS_EXTENDED_LBA_SUPPORTED = 1 << 5,		   /**< The extended lba format is supported,
+										   metadata is transferred as a contiguous
+										   part of the logical block that it is associated with */
+		SPDK_CLIENT_NS_WRITE_UNCORRECTABLE_SUPPORTED = 1 << 6, /**< The write uncorrectable command is supported */
+		SPDK_CLIENT_NS_COMPARE_SUPPORTED = 1 << 7,			   /**< The compare command is supported */
+	};
+
+	/**
+	 * Get the flags for the given namespace.
+	 *
+	 * See spdk_client_ns_flags for the possible flags returned.
+	 *
+	 * This function is thread safe and can be called at any point while the controller
+	 * is attached to the SPDK Client driver.
+	 *
+	 * \param ns Namespace to query.
+	 *
+	 * \return the flags for the given namespace.
+	 */
+	uint32_t spdk_client_ns_get_flags(struct spdk_client_ns *ns);
+
+	/**
+	 * Get the ANA group ID for the given namespace.
+	 *
+	 * This function should be called only if spdk_client_ctrlr_is_log_page_supported() returns
+	 * true for the controller and log page ID SPDK_CLIENT_LOG_ASYMMETRIC_NAMESPACE_ACCESS.
+	 *
+	 * This function is thread safe and can be called at any point while the controller
+	 * is attached to the SPDK Client driver.
+	 *
+	 * \param ns Namespace to query.
+	 *
+	 * \return the ANA group ID for the given namespace.
+	 */
+	uint32_t spdk_client_ns_get_ana_group_id(const struct spdk_client_ns *ns);
+
+	/**
+	 * Restart the SGL walk to the specified offset when the command has scattered payloads.
+	 *
+	 * \param cb_arg Argument passed to readv/writev.
+	 * \param offset Offset for SGL.
+	 */
+	typedef void (*spdk_client_req_reset_sgl_cb)(void *cb_arg, uint32_t offset);
+
+	/**
+	 * Fill out *address and *length with the current SGL entry and advance to the next
+	 * entry for the next time the callback is invoked.
+	 *
+	 * The described segment must be physically contiguous.
+	 *
+	 * \param cb_arg Argument passed to readv/writev.
+	 * \param address Virtual address of this segment, a value of UINT64_MAX
+	 * means the segment should be described via Bit Bucket SGL.
+	 * \param length Length of this physical segment.
+	 */
+	typedef int (*spdk_client_req_next_sge_cb)(void *cb_arg, void **address, uint32_t *length);
+
+	/**
+	 * Submit a write I/O to the specified Client namespace.
+	 *
+	 * The command is submitted to a qpair allocated by spdk_client_ctrlr_alloc_io_qpair().
+	 * The user must ensure that only one thread submits I/O on a given qpair at any
+	 * given time.
+	 *
+	 * \param ns Client namespace to submit the write I/O.
+	 * \param qpair I/O queue pair to submit the request.
+	 * \param payload Virtual address pointer to the data payload.
+	 * \param lba Starting LBA to write the data.
+	 * \param lba_count Length (in sectors) for the write operation.
+	 * \param cb_fn Callback function to invoke when the I/O is completed.
+	 * \param cb_arg Argument to pass to the callback function.
+	 * \param io_flags Set flags, defined by the SPDK_CLIENT_IO_FLAGS_* entries in
+	 * spdk/client_spec.h, for this I/O.
+	 *
+	 * \return 0 if successfully submitted, negated errnos on the following error conditions:
+	 * -EINVAL: The request is malformed.
+	 * -ENOMEM: The request cannot be allocated.
+	 * -ENXIO: The qpair is failed at the transport level.
+	 */
+	int spdk_client_ns_cmd_write(struct spdk_client_qpair *qpair, void *payload,
+								 uint64_t lba, uint32_t lba_count, spdk_req_cmd_cb cb_fn,
+								 void *cb_arg, uint32_t io_flags);
+
+	/**
+	 * Submit a write I/O to the specified Client namespace.
+	 *
+	 * The command is submitted to a qpair allocated by spdk_client_ctrlr_alloc_io_qpair().
+	 * The user must ensure that only one thread submits I/O on a given qpair at any
+	 * given time.
+	 *
+	 * \param ns Client namespace to submit the write I/O.
+	 * \param qpair I/O queue pair to submit the request.
+	 * \param lba Starting LBA to write the data.
+	 * \param lba_count Length (in sectors) for the write operation.
+	 * \param cb_fn Callback function to invoke when the I/O is completed.
+	 * \param cb_arg Argument to pass to the callback function.
+	 * \param io_flags Set flags, defined in client_spec.h, for this I/O.
+	 * \param reset_sgl_fn Callback function to reset scattered payload.
+	 * \param next_sge_fn Callback function to iterate each scattered payload memory
+	 * segment.
+	 *
+	 * \return 0 if successfully submitted, negated errnos on the following error conditions:
+	 * -EINVAL: The request is malformed.
+	 * -ENOMEM: The request cannot be allocated.
+	 * -ENXIO: The qpair is failed at the transport level.
+	 */
+	int spdk_client_ns_cmd_writev(struct spdk_client_qpair *qpair,
+								  uint64_t lba, uint32_t lba_count,
+								  spdk_req_cmd_cb cb_fn, void *cb_arg, uint32_t io_flags,
+								  spdk_client_req_reset_sgl_cb reset_sgl_fn,
+								  spdk_client_req_next_sge_cb next_sge_fn);
+
+	/**
+	 * Submit a write I/O to the specified Client namespace.
+	 *
+	 * The command is submitted to a qpair allocated by spdk_client_ctrlr_alloc_io_qpair().
+	 * The user must ensure that only one thread submits I/O on a given qpair at any
+	 * given time.
+	 *
+	 * \param ns Client namespace to submit the write I/O
+	 * \param qpair I/O queue pair to submit the request
+	 * \param lba starting LBA to write the data
+	 * \param lba_count length (in sectors) for the write operation
+	 * \param cb_fn callback function to invoke when the I/O is completed
+	 * \param cb_arg argument to pass to the callback function
+	 * \param io_flags set flags, defined in client_spec.h, for this I/O
+	 * \param reset_sgl_fn callback function to reset scattered payload
+	 * \param next_sge_fn callback function to iterate each scattered
+	 * payload memory segment
+	 * \param metadata virtual address pointer to the metadata payload, the length
+	 * of metadata is specified by spdk_client_ns_get_md_size()
+	 * \param apptag_mask application tag mask.
+	 * \param apptag application tag to use end-to-end protection information.
+	 *
+	 * \return 0 if successfully submitted, negated errnos on the following error conditions:
+	 * -EINVAL: The request is malformed.
+	 * -ENOMEM: The request cannot be allocated.
+	 * -ENXIO: The qpair is failed at the transport level.
+	 */
+	int spdk_client_ns_cmd_writev_with_md(struct spdk_client_qpair *qpair,
+										  uint64_t lba, uint32_t lba_count,
+										  spdk_req_cmd_cb cb_fn, void *cb_arg, uint32_t io_flags,
+										  spdk_client_req_reset_sgl_cb reset_sgl_fn,
+										  spdk_client_req_next_sge_cb next_sge_fn, void *metadata,
+										  uint16_t apptag_mask, uint16_t apptag);
+
+	/**
+	 * Submit a write I/O to the specified Client namespace.
+	 *
+	 * The command is submitted to a qpair allocated by spdk_client_ctrlr_alloc_io_qpair().
+	 * The user must ensure that only one thread submits I/O on a given qpair at any
+	 * given time.
+	 *
+	 * \param ns Client namespace to submit the write I/O
+	 * \param qpair I/O queue pair to submit the request
+	 * \param lba starting LBA to write the data
+	 * \param lba_count length (in sectors) for the write operation
+	 * \param cb_fn callback function to invoke when the I/O is completed
+	 * \param cb_arg argument to pass to the callback function
+	 * \param reset_sgl_fn callback function to reset scattered payload
+	 * \param next_sge_fn callback function to iterate each scattered
+	 * payload memory segment
+	 * \param opts Optional structure with extended IO request options. If provided, the caller must
+	 * guarantee that this structure is accessible until IO completes
+	 *
+	 * \return 0 if successfully submitted, negated errnos on the following error conditions:
+	 * -EINVAL: The request is malformed.
+	 * -ENOMEM: The request cannot be allocated.
+	 * -ENXIO: The qpair is failed at the transport level.
+	 * -EFAULT: Invalid address was specified as part of payload.  cb_fn is also called
+	 *          with error status including dnr=1 in this case.
+	 */
+	int spdk_client_ns_cmd_writev_ext(struct spdk_client_qpair *qpair,
+									  uint64_t lba, uint32_t lba_count,
+									  spdk_req_cmd_cb cb_fn, void *cb_arg,
+									  spdk_client_req_reset_sgl_cb reset_sgl_fn,
+									  spdk_client_req_next_sge_cb next_sge_fn,
+									  struct spdk_client_ns_cmd_ext_io_opts *opts);
+
+	/**
+	 * Submit a write I/O to the specified Client namespace.
+	 *
+	 * The command is submitted to a qpair allocated by spdk_client_ctrlr_alloc_io_qpair().
+	 * The user must ensure that only one thread submits I/O on a given qpair at any
+	 * given time.
+	 *
+	 * \param ns Client namespace to submit the write I/O.
+	 * \param qpair I/O queue pair to submit the request.
+	 * \param payload Virtual address pointer to the data payload.
+	 * \param metadata Virtual address pointer to the metadata payload, the length
+	 * of metadata is specified by spdk_client_ns_get_md_size().
+	 * \param lba Starting LBA to write the data.
+	 * \param lba_count Length (in sectors) for the write operation.
+	 * \param cb_fn Callback function to invoke when the I/O is completed.
+	 * \param cb_arg Argument to pass to the callback function.
+	 * \param io_flags Set flags, defined by the SPDK_CLIENT_IO_FLAGS_* entries in
+	 * spdk/client_spec.h, for this I/O.
+	 * \param apptag_mask Application tag mask.
+	 * \param apptag Application tag to use end-to-end protection information.
+	 *
+	 * \return 0 if successfully submitted, negated errnos on the following error conditions:
+	 * -EINVAL: The request is malformed.
+	 * -ENOMEM: The request cannot be allocated.
+	 * -ENXIO: The qpair is failed at the transport level.
+	 */
+	int spdk_client_ns_cmd_write_with_md(struct spdk_client_qpair *qpair,
+										 void *payload, void *metadata,
+										 uint64_t lba, uint32_t lba_count, spdk_req_cmd_cb cb_fn,
+										 void *cb_arg, uint32_t io_flags,
+										 uint16_t apptag_mask, uint16_t apptag);
+
+	/**
+	 * Submit a write zeroes I/O to the specified Client namespace.
+	 *
+	 * The command is submitted to a qpair allocated by spdk_client_ctrlr_alloc_io_qpair().
+	 * The user must ensure that only one thread submits I/O on a given qpair at any
+	 * given time.
+	 *
+	 * \param ns Client namespace to submit the write zeroes I/O.
+	 * \param qpair I/O queue pair to submit the request.
+	 * \param lba Starting LBA for this command.
+	 * \param lba_count Length (in sectors) for the write zero operation.
+	 * \param cb_fn Callback function to invoke when the I/O is completed.
+	 * \param cb_arg Argument to pass to the callback function.
+	 * \param io_flags Set flags, defined by the SPDK_CLIENT_IO_FLAGS_* entries in
+	 * spdk/client_spec.h, for this I/O.
+	 *
+	 * \return 0 if successfully submitted, negated errnos on the following error conditions:
+	 * -EINVAL: The request is malformed.
+	 * -ENOMEM: The request cannot be allocated.
+	 * -ENXIO: The qpair is failed at the transport level.
+	 */
+	int spdk_client_ns_cmd_write_zeroes(struct spdk_client_qpair *qpair,
+										uint64_t lba, uint32_t lba_count,
+										spdk_req_cmd_cb cb_fn, void *cb_arg,
+										uint32_t io_flags);
+
+	/**
+	 * Submit a write uncorrectable I/O to the specified Client namespace.
+	 *
+	 * The command is submitted to a qpair allocated by spdk_client_ctrlr_alloc_io_qpair().
+	 * The user must ensure that only one thread submits I/O on a given qpair at any
+	 * given time.
+	 *
+	 * \param ns Client namespace to submit the write uncorrectable I/O.
+	 * \param qpair I/O queue pair to submit the request.
+	 * \param lba Starting LBA for this command.
+	 * \param lba_count Length (in sectors) for the write uncorrectable operation.
+	 * \param cb_fn Callback function to invoke when the I/O is completed.
+	 * \param cb_arg Argument to pass to the callback function.
+	 *
+	 * \return 0 if successfully submitted, negated errnos on the following error conditions:
+	 * -EINVAL: The request is malformed.
+	 * -ENOMEM: The request cannot be allocated.
+	 * -ENXIO: The qpair is failed at the transport level.
+	 */
+	int spdk_client_ns_cmd_write_uncorrectable(struct spdk_client_qpair *qpair,
+											   uint64_t lba, uint32_t lba_count,
+											   spdk_req_cmd_cb cb_fn, void *cb_arg);
+
+	/**
+	 * \brief Submits a read I/O to the specified Client namespace.
+	 *
+	 * The command is submitted to a qpair allocated by spdk_client_ctrlr_alloc_io_qpair().
+	 * The user must ensure that only one thread submits I/O on a given qpair at any
+	 * given time.
+	 *
+	 * \param ns Client namespace to submit the read I/O.
+	 * \param qpair I/O queue pair to submit the request.
+	 * \param payload Virtual address pointer to the data payload.
+	 * \param lba Starting LBA to read the data.
+	 * \param lba_count Length (in sectors) for the read operation.
+	 * \param cb_fn Callback function to invoke when the I/O is completed.
+	 * \param cb_arg Argument to pass to the callback function.
+	 * \param io_flags Set flags, defined in client_spec.h, for this I/O.
+	 *
+	 * \return 0 if successfully submitted, negated errnos on the following error conditions:
+	 * -EINVAL: The request is malformed.
+	 * -ENOMEM: The request cannot be allocated.
+	 * -ENXIO: The qpair is failed at the transport level.
+	 */
+	int spdk_client_ns_cmd_read(struct spdk_client_qpair *qpair, void *payload,
+								uint64_t lba, uint32_t lba_count, spdk_req_cmd_cb cb_fn,
+								void *cb_arg, uint32_t io_flags);
+
+	/**
+	 * Submit a read I/O to the specified Client namespace.
+	 *
+	 * The command is submitted to a qpair allocated by spdk_client_ctrlr_alloc_io_qpair().
+	 * The user must ensure that only one thread submits I/O on a given qpair at any
+	 * given time.
+	 *
+	 * \param ns Client namespace to submit the read I/O.
+	 * \param qpair I/O queue pair to submit the request.
+	 * \param lba Starting LBA to read the data.
+	 * \param lba_count Length (in sectors) for the read operation.
+	 * \param cb_fn Callback function to invoke when the I/O is completed.
+	 * \param cb_arg Argument to pass to the callback function.
+	 * \param io_flags Set flags, defined in client_spec.h, for this I/O.
+	 * \param reset_sgl_fn Callback function to reset scattered payload.
+	 * \param next_sge_fn Callback function to iterate each scattered payload memory
+	 * segment.
+	 *
+	 * \return 0 if successfully submitted, negated errnos on the following error conditions:
+	 * -EINVAL: The request is malformed.
+	 * -ENOMEM: The request cannot be allocated.
+	 * -ENXIO: The qpair is failed at the transport level.
+	 */
+	int spdk_client_ns_cmd_readv(struct spdk_client_qpair *qpair,
+								 uint64_t lba, uint32_t lba_count,
+								 spdk_req_cmd_cb cb_fn, void *cb_arg, uint32_t io_flags,
+								 spdk_client_req_reset_sgl_cb reset_sgl_fn,
+								 spdk_client_req_next_sge_cb next_sge_fn);
+
+	/**
+	 * Submit a read I/O to the specified Client namespace.
+	 *
+	 * The command is submitted to a qpair allocated by spdk_client_ctrlr_alloc_io_qpair().
+	 * The user must ensure that only one thread submits I/O on a given qpair at any given time.
+	 *
+	 * \param ns Client namespace to submit the read I/O
+	 * \param qpair I/O queue pair to submit the request
+	 * \param lba starting LBA to read the data
+	 * \param lba_count length (in sectors) for the read operation
+	 * \param cb_fn callback function to invoke when the I/O is completed
+	 * \param cb_arg argument to pass to the callback function
+	 * \param io_flags set flags, defined in client_spec.h, for this I/O
+	 * \param reset_sgl_fn callback function to reset scattered payload
+	 * \param next_sge_fn callback function to iterate each scattered
+	 * payload memory segment
+	 * \param metadata virtual address pointer to the metadata payload, the length
+	 *	           of metadata is specified by spdk_client_ns_get_md_size()
+	 * \param apptag_mask application tag mask.
+	 * \param apptag application tag to use end-to-end protection information.
+	 *
+	 * \return 0 if successfully submitted, negated errnos on the following error conditions:
+	 * -EINVAL: The request is malformed.
+	 * -ENOMEM: The request cannot be allocated.
+	 * -ENXIO: The qpair is failed at the transport level.
+	 */
+	int spdk_client_ns_cmd_readv_with_md(struct spdk_client_qpair *qpair,
+										 uint64_t lba, uint32_t lba_count,
+										 spdk_req_cmd_cb cb_fn, void *cb_arg, uint32_t io_flags,
+										 spdk_client_req_reset_sgl_cb reset_sgl_fn,
+										 spdk_client_req_next_sge_cb next_sge_fn, void *metadata,
+										 uint16_t apptag_mask, uint16_t apptag);
+
+	/**
+	 * Submit a read I/O to the specified Client namespace.
+	 *
+	 * The command is submitted to a qpair allocated by spdk_client_ctrlr_alloc_io_qpair().
+	 * The user must ensure that only one thread submits I/O on a given qpair at any given time.
+	 *
+	 * \param ns Client namespace to submit the read I/O
+	 * \param qpair I/O queue pair to submit the request
+	 * \param lba starting LBA to read the data
+	 * \param lba_count length (in sectors) for the read operation
+	 * \param cb_fn callback function to invoke when the I/O is completed
+	 * \param cb_arg argument to pass to the callback function
+	 * \param reset_sgl_fn callback function to reset scattered payload
+	 * \param next_sge_fn callback function to iterate each scattered
+	 * payload memory segment
+	 * \param opts Optional structure with extended IO request options. If provided, the caller must
+	 * guarantee that this structure is accessible until IO completes
+	 *
+	 * \return 0 if successfully submitted, negated errnos on the following error conditions:
+	 * -EINVAL: The request is malformed.
+	 * -ENOMEM: The request cannot be allocated.
+	 * -ENXIO: The qpair is failed at the transport level.
+	 * -EFAULT: Invalid address was specified as part of payload.  cb_fn is also called
+	 *          with error status including dnr=1 in this case.
+	 */
+	int spdk_client_ns_cmd_readv_ext(struct spdk_client_qpair *qpair,
+									 uint64_t lba, uint32_t lba_count, spdk_req_cmd_cb cb_fn,
+									 void *cb_arg, spdk_client_req_reset_sgl_cb reset_sgl_fn,
+									 spdk_client_req_next_sge_cb next_sge_fn,
+									 struct spdk_client_ns_cmd_ext_io_opts *opts);
+
+	/**
+	 * Submits a read I/O to the specified Client namespace.
+	 *
+	 * The command is submitted to a qpair allocated by spdk_client_ctrlr_alloc_io_qpair().
+	 * The user must ensure that only one thread submits I/O on a given qpair at any
+	 * given time.
+	 *
+	 * \param ns Client namespace to submit the read I/O
+	 * \param qpair I/O queue pair to submit the request
+	 * \param payload virtual address pointer to the data payload
+	 * \param metadata virtual address pointer to the metadata payload, the length
+	 * of metadata is specified by spdk_client_ns_get_md_size().
+	 * \param lba starting LBA to read the data.
+	 * \param lba_count Length (in sectors) for the read operation.
+	 * \param cb_fn Callback function to invoke when the I/O is completed.
+	 * \param cb_arg Argument to pass to the callback function.
+	 * \param io_flags Set flags, defined in client_spec.h, for this I/O.
+	 * \param apptag_mask Application tag mask.
+	 * \param apptag Application tag to use end-to-end protection information.
+	 *
+	 * \return 0 if successfully submitted, negated errnos on the following error conditions:
+	 * -EINVAL: The request is malformed.
+	 * -ENOMEM: The request cannot be allocated.
+	 * -ENXIO: The qpair is failed at the transport level.
+	 */
+	int spdk_client_ns_cmd_read_with_md(struct spdk_client_qpair *qpair,
+										void *payload, void *metadata,
+										uint64_t lba, uint32_t lba_count, spdk_req_cmd_cb cb_fn,
+										void *cb_arg, uint32_t io_flags,
+										uint16_t apptag_mask, uint16_t apptag);
+
+	/**
+	 * Submit a flush request to the specified Client namespace.
+	 *
+	 * The command is submitted to a qpair allocated by spdk_client_ctrlr_alloc_io_qpair().
+	 * The user must ensure that only one thread submits I/O on a given qpair at any
+	 * given time.
+	 *
+	 * \param ns Client namespace to submit the flush request.
+	 * \param qpair I/O queue pair to submit the request.
+	 * \param cb_fn Callback function to invoke when the I/O is completed.
+	 * \param cb_arg Argument to pass to the callback function.
+	 *
+	 * \return 0 if successfully submitted, negated errnos on the following error conditions:
+	 * -ENOMEM: The request cannot be allocated.
+	 * -ENXIO: The qpair is failed at the transport level.
+	 */
+	int spdk_client_ns_cmd_flush(struct spdk_client_qpair *qpair,
+								 spdk_req_cmd_cb cb_fn, void *cb_arg);
+
+	/**
+	 * Submit a reservation report to the specified Client namespace.
+	 *
+	 * The command is submitted to a qpair allocated by spdk_client_ctrlr_alloc_io_qpair().
+	 * The user must ensure that only one thread submits I/O on a given qpair at any
+	 * given time.
+	 *
+	 * \param ns Client namespace to submit the reservation report request.
+	 * \param qpair I/O queue pair to submit the request.
+	 * \param payload Virtual address pointer for reservation status data.
+	 * \param len Length bytes for reservation status data structure.
+	 * \param cb_fn Callback function to invoke when the I/O is completed.
+	 * \param cb_arg Argument to pass to the callback function.
+	 *
+	 * \return 0 if successfully submitted, negated errnos on the following error conditions:
+	 * -ENOMEM: The request cannot be allocated.
+	 * -ENXIO: The qpair is failed at the transport level.
+	 */
+	int spdk_client_ns_cmd_reservation_report(struct spdk_client_ns *ns,
+											  struct spdk_client_qpair *qpair,
+											  void *payload, uint32_t len,
+											  spdk_req_cmd_cb cb_fn, void *cb_arg);
+
+	/**
+	 * Submit a compare I/O to the specified Client namespace.
+	 *
+	 * The command is submitted to a qpair allocated by spdk_client_ctrlr_alloc_io_qpair().
+	 * The user must ensure that only one thread submits I/O on a given qpair at any
+	 * given time.
+	 *
+	 * \param ns Client namespace to submit the compare I/O.
+	 * \param qpair I/O queue pair to submit the request.
+	 * \param payload Virtual address pointer to the data payload.
+	 * \param lba Starting LBA to compare the data.
+	 * \param lba_count Length (in sectors) for the compare operation.
+	 * \param cb_fn Callback function to invoke when the I/O is completed.
+	 * \param cb_arg Argument to pass to the callback function.
+	 * \param io_flags Set flags, defined in client_spec.h, for this I/O.
+	 *
+	 * \return 0 if successfully submitted, negated errnos on the following error conditions:
+	 * -EINVAL: The request is malformed.
+	 * -ENOMEM: The request cannot be allocated.
+	 * -ENXIO: The qpair is failed at the transport level.
+	 */
+	int spdk_client_ns_cmd_compare(struct spdk_client_ns *ns, struct spdk_client_qpair *qpair, void *payload,
+								   uint64_t lba, uint32_t lba_count, spdk_req_cmd_cb cb_fn,
+								   void *cb_arg, uint32_t io_flags);
+
+	/**
+	 * Submit a compare I/O to the specified Client namespace.
+	 *
+	 * The command is submitted to a qpair allocated by spdk_client_ctrlr_alloc_io_qpair().
+	 * The user must ensure that only one thread submits I/O on a given qpair at any
+	 * given time.
+	 *
+	 * \param ns Client namespace to submit the compare I/O.
+	 * \param qpair I/O queue pair to submit the request.
+	 * \param lba Starting LBA to compare the data.
+	 * \param lba_count Length (in sectors) for the compare operation.
+	 * \param cb_fn Callback function to invoke when the I/O is completed.
+	 * \param cb_arg Argument to pass to the callback function.
+	 * \param io_flags Set flags, defined in client_spec.h, for this I/O.
+	 * \param reset_sgl_fn Callback function to reset scattered payload.
+	 * \param next_sge_fn Callback function to iterate each scattered payload memory
+	 * segment.
+	 *
+	 * \return 0 if successfully submitted, negated errnos on the following error conditions:
+	 * -EINVAL: The request is malformed.
+	 * -ENOMEM: The request cannot be allocated.
+	 * -ENXIO: The qpair is failed at the transport level.
+	 */
+	int spdk_client_ns_cmd_comparev(struct spdk_client_ns *ns, struct spdk_client_qpair *qpair,
+									uint64_t lba, uint32_t lba_count,
+									spdk_req_cmd_cb cb_fn, void *cb_arg, uint32_t io_flags,
+									spdk_client_req_reset_sgl_cb reset_sgl_fn,
+									spdk_client_req_next_sge_cb next_sge_fn);
+
+	/**
+	 * Submit a compare I/O to the specified Client namespace.
+	 *
+	 * The command is submitted to a qpair allocated by spdk_client_ctrlr_alloc_io_qpair().
+	 * The user must ensure that only one thread submits I/O on a given qpair at any
+	 * given time.
+	 *
+	 * \param ns Client namespace to submit the compare I/O.
+	 * \param qpair I/O queue pair to submit the request.
+	 * \param lba Starting LBA to compare the data.
+	 * \param lba_count Length (in sectors) for the compare operation.
+	 * \param cb_fn Callback function to invoke when the I/O is completed.
+	 * \param cb_arg Argument to pass to the callback function.
+	 * \param io_flags Set flags, defined in client_spec.h, for this I/O.
+	 * \param reset_sgl_fn Callback function to reset scattered payload.
+	 * \param next_sge_fn Callback function to iterate each scattered payload memory
+	 * segment.
+	 * \param metadata Virtual address pointer to the metadata payload, the length
+	 * of metadata is specified by spdk_client_ns_get_md_size()
+	 * \param apptag_mask Application tag mask.
+	 * \param apptag Application tag to use end-to-end protection information.
+	 *
+	 * \return 0 if successfully submitted, negated errnos on the following error conditions:
+	 * -EINVAL: The request is malformed.
+	 * -ENOMEM: The request cannot be allocated.
+	 * -ENXIO: The qpair is failed at the transport level.
+	 */
+	int
+	spdk_client_ns_cmd_comparev_with_md(struct spdk_client_ns *ns, struct spdk_client_qpair *qpair,
+										uint64_t lba, uint32_t lba_count,
+										spdk_req_cmd_cb cb_fn, void *cb_arg, uint32_t io_flags,
+										spdk_client_req_reset_sgl_cb reset_sgl_fn,
+										spdk_client_req_next_sge_cb next_sge_fn, void *metadata,
+										uint16_t apptag_mask, uint16_t apptag);
+
+	/**
+	 * Submit a compare I/O to the specified Client namespace.
+	 *
+	 * The command is submitted to a qpair allocated by spdk_client_ctrlr_alloc_io_qpair().
+	 * The user must ensure that only one thread submits I/O on a given qpair at any
+	 * given time.
+	 *
+	 * \param ns Client namespace to submit the compare I/O.
+	 * \param qpair I/O queue pair to submit the request.
+	 * \param payload Virtual address pointer to the data payload.
+	 * \param metadata Virtual address pointer to the metadata payload, the length
+	 * of metadata is specified by spdk_client_ns_get_md_size().
+	 * \param lba Starting LBA to compare the data.
+	 * \param lba_count Length (in sectors) for the compare operation.
+	 * \param cb_fn Callback function to invoke when the I/O is completed.
+	 * \param cb_arg Argument to pass to the callback function.
+	 * \param io_flags Set flags, defined in client_spec.h, for this I/O.
+	 * \param apptag_mask Application tag mask.
+	 * \param apptag Application tag to use end-to-end protection information.
+	 *
+	 * \return 0 if successfully submitted, negated errnos on the following error conditions:
+	 * -EINVAL: The request is malformed.
+	 * -ENOMEM: The request cannot be allocated.
+	 * -ENXIO: The qpair is failed at the transport level.
+	 */
+	int spdk_client_ns_cmd_compare_with_md(struct spdk_client_ns *ns, struct spdk_client_qpair *qpair,
+										   void *payload, void *metadata,
+										   uint64_t lba, uint32_t lba_count, spdk_req_cmd_cb cb_fn,
+										   void *cb_arg, uint32_t io_flags,
+										   uint16_t apptag_mask, uint16_t apptag);
+
+	/**
+	 * \brief Inject an error for the next request with a given opcode.
+	 *
+	 * \param ctrlr Client controller.
+	 * \param qpair I/O queue pair to add the error command,
+	 *              NULL for Admin queue pair.
+	 * \param opc Opcode for Admin or I/O commands.
+	 * \param do_not_submit True if matching requests should not be submitted
+	 *                      to the controller, but instead completed manually
+	 *                      after timeout_in_us has expired.  False if matching
+	 *                      requests should be submitted to the controller and
+	 *                      have their completion status modified after the
+	 *                      controller completes the request.
+	 * \param timeout_in_us Wait specified microseconds when do_not_submit is true.
+	 * \param err_count Number of matching requests to inject errors.
+	 * \param sct Status code type.
+	 * \param sc Status code.
+	 *
+	 * \return 0 if successfully enabled, ENOMEM if an error command
+	 *	     structure cannot be allocated.
+	 *
+	 * The function can be called multiple times to inject errors for different
+	 * commands.  If the opcode matches an existing entry, the existing entry
+	 * will be updated with the values specified.
+	 */
+	int spdk_client_qpair_add_cmd_error_injection(struct spdk_client_ctrlr *ctrlr,
+												  struct spdk_client_qpair *qpair,
+												  uint8_t opc,
+												  bool do_not_submit,
+												  uint64_t timeout_in_us,
+												  uint32_t err_count,
+												  uint8_t sct, uint8_t sc);
+
+	/**
+	 * \brief Clear the specified Client command with error status.
+	 *
+	 * \param ctrlr Client controller.
+	 * \param qpair I/O queue pair to remove the error command,
+	 * \            NULL for Admin queue pair.
+	 * \param opc Opcode for Admin or I/O commands.
+	 *
+	 * The function will remove specified command in the error list.
+	 */
+	void spdk_client_qpair_remove_cmd_error_injection(struct spdk_client_ctrlr *ctrlr,
+													  struct spdk_client_qpair *qpair,
+													  uint8_t opc);
+
+	/**
+	 * \brief Given Client status, return ASCII string for that error.
+	 *
+	 * \param status Status from Client completion queue element.
+	 * \return Returns status as an ASCII string.
+	 */
+	const char *spdk_req_cpl_get_status_string(const struct spdk_req_status *status);
+
+	/**
+	 * \brief Prints (SPDK_NOTICELOG) the contents of an Client submission queue entry (command).
+	 *
+	 * \param qpair Pointer to the Client queue pair - used to determine admin versus I/O queue.
+	 * \param cmd Pointer to the submission queue command to be formatted.
+	 */
+	void spdk_client_qpair_print_command(struct spdk_client_qpair *qpair,
+										 struct spdk_req_cmd *cmd);
+
+	/**
+	 * \brief Prints (SPDK_NOTICELOG) the contents of an Client completion queue entry.
+	 *
+	 * \param qpair Pointer to the Client queue pair - presently unused.
+	 * \param cpl Pointer to the completion queue element to be formatted.
+	 */
+	void spdk_client_qpair_print_completion(struct spdk_client_qpair *qpair,
+											struct spdk_req_cpl *cpl);
+
+	/**
+	 * \brief Gets the Client qpair ID for the specified qpair.
+	 *
+	 * \param qpair Pointer to the Client queue pair.
+	 * \returns ID for the specified qpair.
+	 */
+	uint16_t spdk_client_qpair_get_id(struct spdk_client_qpair *qpair);
+
+	/**
+	 * \brief Prints (SPDK_NOTICELOG) the contents of an Client submission queue entry (command).
+	 *
+	 * \param qid Queue identifier.
+	 * \param cmd Pointer to the submission queue command to be formatted.
+	 */
+	void spdk_client_print_command(uint16_t qid, struct spdk_req_cmd *cmd);
+
+	/**
+	 * \brief Prints (SPDK_NOTICELOG) the contents of an Client completion queue entry.
+	 *
+	 * \param qid Queue identifier.
+	 * \param cpl Pointer to the completion queue element to be formatted.
+	 */
+	void spdk_client_print_completion(uint16_t qid, struct spdk_req_cpl *cpl);
+
+	struct ibv_context;
+	struct ibv_pd;
+	struct ibv_mr;
+
+	/**
+	 * RDMA Transport Hooks
+	 */
+	struct spdk_client_rdma_hooks
+	{
+		/**
+		 * \brief Get an InfiniBand Verbs protection domain.
+		 *
+		 * \param trid the transport id
+		 * \param verbs Infiniband verbs context
+		 *
+		 * \return pd of the client ctrlr
+		 */
+		struct ibv_pd *(*get_ibv_pd)(const struct spdk_client_transport_id *trid,
+									 struct ibv_context *verbs);
+
+		/**
+		 * \brief Get an InfiniBand Verbs memory region for a buffer.
+		 *
+		 * \param pd The protection domain returned from get_ibv_pd
+		 * \param buf Memory buffer for which an rkey should be returned.
+		 * \param size size of buf
+		 *
+		 * \return Infiniband remote key (rkey) for this buf
+		 */
+		uint64_t (*get_rkey)(struct ibv_pd *pd, void *buf, size_t size);
+
+		/**
+		 * \brief Put back keys got from get_rkey.
+		 *
+		 * \param key The Infiniband remote key (rkey) got from get_rkey
+		 *
+		 */
+		void (*put_rkey)(uint64_t key);
+	};
+
+	/**
+	 * \brief Set the global hooks for the RDMA transport, if necessary.
+	 *
+	 * This call is optional and must be performed prior to probing for
+	 * any devices. By default, the RDMA transport will use the ibverbs
+	 * library to create protection domains and register memory. This
+	 * is a mechanism to subvert that and use an existing registration.
+	 *
+	 * This function may only be called one time per process.
+	 *
+	 * \param hooks for initializing global hooks
+	 */
+	void spdk_client_rdma_init_hooks(struct spdk_client_rdma_hooks *hooks);
+
+	/**
+	 * Get SPDK memory domains used by the given client controller.
+	 *
+	 * The user can call this function with \b domains set to NULL and \b array_size set to 0 to get the
+	 * number of memory domains used by client controller
+	 *
+	 * \param ctrlr Opaque handle to the Client controller.
+	 * \param domains Pointer to an array of memory domains to be filled by this function. The user should allocate big enough
+	 * array to keep all memory domains used by client controller
+	 * \param array_size size of \b domains array
+	 * \return the number of entries in \b domains array or negated errno. If returned value is bigger than \b array_size passed by the user
+	 * then the user should increase the size of \b domains array and call this function again. There is no guarantees that
+	 * the content of \b domains array is valid in that case.
+	 *         -EINVAL if input parameters were invalid
+
+	 */
+	int spdk_client_ctrlr_get_memory_domains(const struct spdk_client_ctrlr *ctrlr,
+											 struct spdk_memory_domain **domains, int array_size);
+
+	/**
+	 * Opaque handle for a transport poll group. Used by the transport function table.
+	 */
+	struct spdk_client_transport_poll_group;
+
+	/**
+	 * Update and populate namespace CUSE devices (Experimental)
+	 *
+	 * \param ctrlr Opaque handle to the Client controller.
+	 *
+	 */
+	void spdk_client_cuse_update_namespaces(struct spdk_client_ctrlr *ctrlr);
+
+	/**
+	 * Signature for callback invoked after completing a register read/write operation.
+	 *
+	 * \param ctx Context passed by the user.
+	 * \param value Value of the register, undefined in case of a failure.
+	 * \param cpl Completion queue entry that contains the status of the command.
+	 */
+	typedef void (*spdk_client_reg_cb)(void *ctx, uint64_t value, const struct spdk_req_cpl *cpl);
+
+	struct client_request;
+
+	struct spdk_client_transport;
+
+	struct spdk_client_transport_ops
+	{
+		char name[SPDK_SRV_TRSTRING_MAX_LEN + 1];
+
+		enum spdk_client_transport_type type;
+
+		struct spdk_client_ctrlr *(*ctrlr_construct)(
+			const struct spdk_client_ctrlr_opts *opts,
+			void *devhandle);
+
+		int (*ctrlr_destruct)(struct spdk_client_ctrlr *ctrlr);
+
+		int (*ctrlr_enable)(struct spdk_client_ctrlr *ctrlr);
+
+		uint32_t (*ctrlr_get_max_xfer_size)(struct spdk_client_ctrlr *ctrlr);
+
+		uint16_t (*ctrlr_get_max_sges)(struct spdk_client_ctrlr *ctrlr);
+
+		int (*ctrlr_reserve_cmb)(struct spdk_client_ctrlr *ctrlr);
+
+		void *(*ctrlr_map_cmb)(struct spdk_client_ctrlr *ctrlr, size_t *size);
+
+		int (*ctrlr_unmap_cmb)(struct spdk_client_ctrlr *ctrlr);
+
+		int (*ctrlr_enable_pmr)(struct spdk_client_ctrlr *ctrlr);
+
+		int (*ctrlr_disable_pmr)(struct spdk_client_ctrlr *ctrlr);
+
+		void *(*ctrlr_map_pmr)(struct spdk_client_ctrlr *ctrlr, size_t *size);
+
+		int (*ctrlr_unmap_pmr)(struct spdk_client_ctrlr *ctrlr);
+
+		struct spdk_client_qpair *(*ctrlr_create_io_qpair)(struct spdk_client_ctrlr *ctrlr, uint16_t qid,
+														   const struct spdk_client_io_qpair_opts *opts);
+
+		int (*ctrlr_delete_io_qpair)(struct spdk_client_ctrlr *ctrlr, struct spdk_client_qpair *qpair);
+
+		int (*ctrlr_connect_qpair)(struct spdk_client_ctrlr *ctrlr, struct spdk_client_qpair *qpair);
+
+		void (*ctrlr_disconnect_qpair)(struct spdk_client_ctrlr *ctrlr, struct spdk_client_qpair *qpair);
+
+		void (*qpair_abort_reqs)(struct spdk_client_qpair *qpair, uint32_t dnr);
+
+		int (*qpair_reset)(struct spdk_client_qpair *qpair);
+
+		int (*qpair_submit_request)(struct spdk_client_qpair *qpair, struct client_request *req);
+
+		int32_t (*qpair_process_completions)(struct spdk_client_qpair *qpair, uint32_t max_completions);
+
+		int (*qpair_iterate_requests)(struct spdk_client_qpair *qpair,
+									  int (*iter_fn)(struct client_request *req, void *arg),
+									  void *arg);
+
+		void (*admin_qpair_abort_aers)(struct spdk_client_qpair *qpair);
+
+		struct spdk_client_transport_poll_group *(*poll_group_create)(void);
+		struct spdk_client_transport_poll_group *(*qpair_get_optimal_poll_group)(
+			struct spdk_client_qpair *qpair);
+
+		int (*poll_group_add)(struct spdk_client_transport_poll_group *tgroup, struct spdk_client_qpair *qpair);
+
+		int (*poll_group_remove)(struct spdk_client_transport_poll_group *tgroup,
+								 struct spdk_client_qpair *qpair);
+
+		int (*poll_group_connect_qpair)(struct spdk_client_qpair *qpair);
+
+		int (*poll_group_disconnect_qpair)(struct spdk_client_qpair *qpair);
+
+		int64_t (*poll_group_process_completions)(struct spdk_client_transport_poll_group *tgroup,
+												  uint32_t completions_per_qpair, spdk_client_disconnected_qpair_cb disconnected_qpair_cb);
+
+		int (*poll_group_destroy)(struct spdk_client_transport_poll_group *tgroup);
+
+		int (*poll_group_get_stats)(struct spdk_client_transport_poll_group *tgroup,
+									struct spdk_client_transport_poll_group_stat **stats);
+
+		void (*poll_group_free_stats)(struct spdk_client_transport_poll_group *tgroup,
+									  struct spdk_client_transport_poll_group_stat *stats);
+
+		int (*ctrlr_get_memory_domains)(const struct spdk_client_ctrlr *ctrlr,
+										struct spdk_memory_domain **domains,
+										int array_size);
+	};
+
+	/**
+	 * Register the operations for a given transport type.
+	 *
+	 * This function should be invoked by referencing the macro
+	 * SPDK_CLIENT_TRANSPORT_REGISTER macro in the transport's .c file.
+	 *
+	 * \param ops The operations associated with an Client-oF transport.
+	 */
+	void spdk_client_transport_register(const struct spdk_client_transport_ops *ops);
+
+	void
+	client_disconnected_qpair_cb(struct spdk_client_qpair *qpair, void *poll_group_ctx);
+
+	int spdk_client_submit_rpc_request(struct spdk_client_qpair *qpair, uint32_t opc, char *raw_data, uint32_t length,
+									   spdk_rpc_request_cb cb_fn, void *cb_arg, bool chek_md5);
+	int spdk_client_submit_rpc_request_iovs(struct spdk_client_qpair *qpair, uint32_t opc, struct iovec *raw_ioves, int raw_iov_cnt, uint32_t length,
+											spdk_rpc_request_cb cb_fn, void *cb_arg, bool chek_md5);
+
+	int spdk_client_empty_free_request(struct spdk_client_qpair *qpair);
+
+	int spdk_client_submit_rpc_request_iovs_directly(struct spdk_client_qpair *qpair, struct iovec *out_ioves, int out_iov_cnt, uint32_t length, spdk_rpc_request_cb cb_fn, void *cb_arg);
+
+	void spdk_client_reclaim_rpc_request(struct rpc_request *req);
+
+/*
+ * Macro used to register new transports.
+ */
+#define SPDK_CLIENT_TRANSPORT_REGISTER(name, transport_ops)                               \
+	static void __attribute__((constructor)) _spdk_client_transport_register_##name(void) \
+	{                                                                                     \
+		spdk_client_transport_register(transport_ops);                                    \
+	}
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif
diff --git a/include/spdk/rdma_common.h b/include/spdk/rdma_common.h
new file mode 100644
index 000000000..077a02286
--- /dev/null
+++ b/include/spdk/rdma_common.h
@@ -0,0 +1,269 @@
+#ifndef SPDK_RDMA_COMMON_H
+#define SPDK_RDMA_COMMON_H
+#include "spdk/bdev.h"
+#include "spdk/memory.h"
+#include "spdk/likely.h"
+
+#define SPDK_SRV_TRSTRING_MAX_LEN 32
+#define SPDK_SERVER_TRADDR_MAX_LEN 256
+
+#define SPDK_SRV_TRADDR_MAX_LEN 256
+#define SPDK_SRV_TRSVCID_MAX_LEN 32
+
+#define SPDK_CLIENT_SGL_SUBTYPE_INVALIDATE_KEY 0xF
+
+#include <openssl/md5.h>
+#include <openssl/evp.h>
+struct spdk_md5ctx
+{
+	EVP_MD_CTX *md5ctx;
+};
+#define SPDK_MD5DIGEST_LEN 16
+
+/**
+ * NVM command set opcodes
+ */
+enum spdk_client_nvm_opcode
+{
+	SPDK_CLIENT_OPC_FLUSH = 0x00,
+	SPDK_CLIENT_OPC_WRITE = 0x01,
+	SPDK_CLIENT_OPC_READ = 0x02,
+	/* 0x03 - reserved */
+	SPDK_CLIENT_OPC_RPC_WRITE = 0x05,
+	SPDK_CLIENT_OPC_RPC_READ = 0x06,
+};
+
+enum spdk_client_submit_data_type
+{
+	SPDK_CLIENT_SUBMIT_CONTING = 0,
+	SPDK_CLIENT_SUBMIT_IOVES = 1,
+	SPDK_CLIENT_SUBMIT_TYPES_TOTAL = 2,
+};
+
+enum spdk_srv_data_transfer
+{
+	/** Opcode does not transfer data */
+	SPDK_SRV_DATA_NONE = 0,
+	/** Opcode transfers data from host to controller (e.g. Write) */
+	SPDK_SRV_DATA_HOST_TO_CONTROLLER = 1,
+	/** Opcode transfers data from controller to host (e.g. Read) */
+	SPDK_SRV_DATA_CONTROLLER_TO_HOST = 2,
+	/** Opcode transfers data both directions */
+	SPDK_SRV_DATA_BIDIRECTIONAL = 3
+};
+
+/**
+ * Srv over Fabrics transport types
+ */
+enum spdk_srv_trtype
+{
+	/** RDMA */
+	SPDK_SRV_TRTYPE_RDMA = 0x1,
+
+	/** TCP */
+	SPDK_SRV_TRTYPE_TCP = 0x2,
+
+	/** Intra-host transport (loopback) */
+	SPDK_SRV_TRTYPE_INTRA_HOST = 0xfe,
+};
+
+enum spdk_srv_transport_type
+{
+	/**
+	 * RDMA Transport (RoCE, iWARP, etc.)
+	 */
+	SPDK_SRV_TRANSPORT_RDMA = SPDK_SRV_TRTYPE_RDMA,
+
+	/**
+	 * TCP Transport
+	 */
+	SPDK_SRV_TRANSPORT_TCP = SPDK_SRV_TRTYPE_TCP,
+
+	/**
+	 * Custom Transport (Not spec defined)
+	 */
+	SPDK_SRV_TRANSPORT_CUSTOM = 4096,
+};
+
+/**
+ * Extract the Data Transfer bits from an Srv opcode.
+ *
+ * This determines whether a command requires a data buffer and
+ * which direction (host to controller or controller to host) it is
+ * transferred.
+ */
+static inline enum spdk_srv_data_transfer spdk_srv_opc_get_data_transfer(uint8_t opc)
+{
+	return (enum spdk_srv_data_transfer)(opc & 3);
+}
+
+struct spdk_srv_rdma_accept_private_data
+{
+	uint16_t recfmt;  /* record format */
+	uint16_t crqsize; /* controller receive queue size */
+	uint8_t reserved[28];
+};
+
+SPDK_STATIC_ASSERT(sizeof(struct spdk_srv_rdma_accept_private_data) == 32, "Incorrect size");
+
+struct spdk_srv_rdma_reject_private_data
+{
+	uint16_t recfmt; /* record format */
+	uint16_t sts;	 /* status */
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_srv_rdma_reject_private_data) == 4, "Incorrect size");
+
+struct spdk_srv_rdma_request_private_data
+{
+	uint16_t recfmt;  /* record format */
+	uint16_t qid;	  /* queue id */
+	uint16_t hrqsize; /* host receive queue size */
+	uint16_t hsqsize; /* host send queue size */
+	uint16_t cntlid;  /* controller id */
+	uint8_t reserved[22];
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_srv_rdma_request_private_data) == 32, "Incorrect size");
+
+enum spdk_srv_adrfam
+{
+	/** IPv4 (AF_INET) */
+	SPDK_SRV_ADRFAM_IPV4 = 0x1,
+
+	/** IPv6 (AF_INET6) */
+	SPDK_SRV_ADRFAM_IPV6 = 0x2,
+
+	/** InfiniBand (AF_IB) */
+	SPDK_SRV_ADRFAM_IB = 0x3,
+
+	/** Fibre Channel address family */
+	SPDK_SRV_ADRFAM_FC = 0x4,
+
+	/** Intra-host transport (loopback) */
+	SPDK_SRV_ADRFAM_INTRA_HOST = 0xfe,
+};
+
+struct __attribute__((packed)) spdk_req_sgl_descriptor
+{
+	uint64_t address;
+	union
+	{
+		struct
+		{
+			uint8_t reserved[7];
+			uint8_t subtype : 4;
+			uint8_t type : 4;
+		} generic;
+
+		struct
+		{
+			uint32_t length;
+			uint8_t reserved[3];
+			uint8_t subtype : 4;
+			uint8_t type : 4;
+		} unkeyed;
+
+		struct
+		{
+			uint64_t length : 24;
+			uint64_t key : 32;
+			uint64_t subtype : 4;
+			uint64_t type : 4;
+		} keyed;
+	};
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_req_sgl_descriptor) == 16, "Incorrect size");
+
+struct spdk_req_cmd
+{
+	/* dword 0 */
+	uint16_t opc : 8;  /* opcode */
+	uint16_t fuse : 2; /* fused operation */
+	uint16_t rsvd1 : 4;
+	uint16_t psdt : 2;
+	uint16_t cid; /* command identifier */
+
+	/* dword 1 */
+	uint32_t rpc_opc; /* rpc opc */
+
+	/* dword 2-3 */
+	uint32_t rsvd2; /* rpc request index */
+	uint32_t rsvd3; /* rpc data length */
+
+	/* dword 4-5 */
+	uint64_t mptr; /* metadata pointer */
+
+	/* dword 6-9: data pointer */
+	union
+	{
+		struct
+		{
+			uint64_t prp1; /* prp entry 1 */
+			uint64_t prp2; /* prp entry 2 */
+		} prp;
+
+		struct spdk_req_sgl_descriptor sgl1;
+	} dptr;
+
+	/* command-specific */
+	union
+	{
+		uint32_t cdw10; // LBA START
+						//		union spdk_req_cmd_cdw10 cdw10_bits;
+	};
+	/* command-specific */
+	union
+	{
+		uint32_t cdw11;
+		//		union spdk_req_cmd_cdw11 cdw11_bits;
+	};
+	/* dword 12-15 */
+	uint32_t cdw12; /* command-specific */ // LBA COUNT
+	uint32_t cdw13;						   /* submit type 0 SPDK_CLIENT_SUBMIT_CONTING or 1 SPDK_CLIENT_SUBMIT_IOVES*/
+	uint32_t cdw14; /* command-specific */ // check md5sum if cdw14 == 1
+	uint32_t cdw15;						   /* command-specific */
+	uint8_t md5sum[16];
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_req_cmd) == 80, "Incorrect size");
+
+struct spdk_req_status
+{
+	uint16_t p : 1;	  /* phase tag */
+	uint16_t sc : 8;  /* status code */
+	uint16_t sct : 3; /* status code type */
+	uint16_t crd : 2; /* command retry delay */
+	uint16_t m : 1;	  /* more */
+	uint16_t dnr : 1; /* do not retry */
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_req_status) == 2, "Incorrect size");
+
+/**
+ * Completion queue entry
+ * cdw0 -> rpc_
+ *
+ *
+ */
+struct spdk_req_cpl
+{
+	/* dword 0 */
+	uint32_t cdw0; /* command-specific used as status code ? */
+
+	/* dword 1 */
+	uint32_t cdw1; /* command-specific required data length*/
+
+	/* dword 2 */
+	uint16_t sqhd; /* submission queue head pointer */
+	uint16_t sqid; /* submission queue identifier */
+
+	/* dword 3 */
+	uint16_t cid; /* command identifier */
+	union
+	{
+		uint16_t status_raw;
+		struct spdk_req_status status;
+	};
+	uint8_t md5sum[16];
+	// uint8_t			data[2048-16];
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_req_cpl) == 32, "Incorrect size");
+
+#endif
\ No newline at end of file
diff --git a/include/spdk/rdma_server.h b/include/spdk/rdma_server.h
new file mode 100644
index 000000000..8ff9478ec
--- /dev/null
+++ b/include/spdk/rdma_server.h
@@ -0,0 +1,799 @@
+#ifndef SPDK_RDMA_SERVER_H
+#define SPDK_RDMA_SERVER_H
+
+#include "spdk/stdinc.h"
+
+#include "spdk/env.h"
+#include "spdk/queue.h"
+#include "spdk/uuid.h"
+#include "spdk/rdma_common.h"
+#ifdef __cplusplus
+extern "C"
+{
+#endif
+
+	struct spdk_srv_tgt;
+	struct spdk_srv_conn;
+	struct spdk_srv_request;
+	struct spdk_srv_request;
+	struct spdk_srv_poll_group;
+	struct spdk_json_write_ctx;
+	struct spdk_json_val;
+	struct spdk_srv_transport;
+	struct spdk_srv_transport_opts;
+	struct spdk_srv_transport_id;
+
+#define SPDK_SRV_TRANSPORT_NAME_FC "FC"
+#define SPDK_SRV_TRANSPORT_NAME_PCIE "PCIE"
+#define SPDK_SRV_TRANSPORT_NAME_RDMA "RDMA"
+#define SPDK_SRV_TRANSPORT_NAME_TCP "TCP"
+#define SPDK_SRV_TRANSPORT_NAME_VFIOUSER "VFIOUSER"
+#define SPDK_SRV_TRANSPORT_NAME_CUSTOM "CUSTOM"
+
+	typedef void (*spdk_srv_state_change_done)(void *cb_arg, int status);
+	typedef void (*spdk_srv_transport_destroy_done_cb)(void *cb_arg);
+	typedef void (*spdk_srv_transport_conn_fini_cb)(void *cb_arg);
+	typedef void (*spdk_srv_poll_group_destroy_done_fn)(void *cb_arg, int status);
+	typedef void(spdk_srv_tgt_destroy_done_fn)(void *ctx, int status);
+	typedef void (*spdk_srv_rpc_service_complete_cb)(void *cb_arg, int status);
+	typedef void (*spdk_srv_rpc_dispatcher_cb)(void *cb_arg, int status, char *data, int length, spdk_srv_rpc_service_complete_cb service_cb, void *service_cb_arg);
+	typedef void (*spdk_srv_rpc_dispatcher)(uint32_t opc, struct iovec *iovs, int iov_cnt, int length, spdk_srv_rpc_dispatcher_cb cb, void *cb_arg);
+	typedef void (*spdk_srv_rpc_dispatcher_iovs_cb)(void *cb_arg, int status, struct iovec *iovs, int iov_cnt, int length, spdk_srv_rpc_service_complete_cb service_cb, void *service_cb_arg);
+	typedef void (*spdk_srv_rpc_dispatcher_iovs)(uint32_t opc, struct iovec *iovs, int iov_cnt, int length, spdk_srv_rpc_dispatcher_iovs_cb cb, void *cb_arg);
+
+#define SRV_TRANSPORT_DEFAULT_ASSOCIATION_TIMEOUT_IN_MS 120000
+#define SPDK_SRV_DEFAULT_ACCEPT_POLL_RATE_US 10000
+
+#define SRV_TGT_NAME_MAX_LENGTH 256
+
+#define SPDK_SRV_MAX_SGL_ENTRIES 16
+/* The maximum number of buffers per request */
+#define SRV_REQ_MAX_BUFFERS (SPDK_SRV_MAX_SGL_ENTRIES * 2 + 1)
+
+#define SRV_DATA_BUFFER_ALIGNMENT VALUE_4KB
+#define SRV_DATA_BUFFER_MASK (SRV_DATA_BUFFER_ALIGNMENT - 1LL)
+
+	enum spdk_srv_conn_state
+	{
+		SPDK_SRV_CONN_UNINITIALIZED = 0,
+		SPDK_SRV_CONN_ACTIVE,
+		SPDK_SRV_CONN_DEACTIVATING,
+		SPDK_SRV_CONN_ERROR,
+	};
+
+	enum spdk_req_sgl_descriptor_type
+	{
+		SPDK_SRV_SGL_TYPE_DATA_BLOCK = 0x0,
+		SPDK_SRV_SGL_TYPE_BIT_BUCKET = 0x1,
+		SPDK_SRV_SGL_TYPE_SEGMENT = 0x2,
+		SPDK_SRV_SGL_TYPE_LAST_SEGMENT = 0x3,
+		SPDK_SRV_SGL_TYPE_KEYED_DATA_BLOCK = 0x4,
+		SPDK_SRV_SGL_TYPE_TRANSPORT_DATA_BLOCK = 0x5,
+		/* 0x6 - 0xE reserved */
+		SPDK_SRV_SGL_TYPE_VENDOR_SPECIFIC = 0xF
+	};
+
+	enum spdk_req_sgl_descriptor_subtype
+	{
+		SPDK_SRV_SGL_SUBTYPE_ADDRESS = 0x0,
+		SPDK_SRV_SGL_SUBTYPE_OFFSET = 0x1,
+		SPDK_SRV_SGL_SUBTYPE_TRANSPORT = 0xa,
+	};
+
+#define SPDK_SRV_SGL_SUBTYPE_INVALIDATE_KEY 0xF
+
+	enum spdk_srv_generic_command_status_code
+	{
+		SPDK_SRV_SC_SUCCESS = 0x00,
+		SPDK_SRV_SC_INVALID_OPCODE = 0x01,
+		SPDK_SRV_SC_INVALID_FIELD = 0x02,
+		SPDK_SRV_SC_COMMAND_ID_CONFLICT = 0x03,
+		SPDK_SRV_SC_DATA_TRANSFER_ERROR = 0x04,
+		SPDK_SRV_SC_ABORTED_POWER_LOSS = 0x05,
+		SPDK_SRV_SC_INTERNAL_DEVICE_ERROR = 0x06,
+		SPDK_SRV_SC_ABORTED_BY_REQUEST = 0x07,
+		SPDK_SRV_SC_ABORTED_SQ_DELETION = 0x08,
+		SPDK_SRV_SC_ABORTED_FAILED_FUSED = 0x09,
+		SPDK_SRV_SC_ABORTED_MISSING_FUSED = 0x0a,
+		SPDK_SRV_SC_INVALID_NAMESPACE_OR_FORMAT = 0x0b,
+		SPDK_SRV_SC_COMMAND_SEQUENCE_ERROR = 0x0c,
+		SPDK_SRV_SC_INVALID_SGL_SEG_DESCRIPTOR = 0x0d,
+		SPDK_SRV_SC_INVALID_NUM_SGL_DESCIRPTORS = 0x0e,
+		SPDK_SRV_SC_DATA_SGL_LENGTH_INVALID = 0x0f,
+		SPDK_SRV_SC_METADATA_SGL_LENGTH_INVALID = 0x10,
+		SPDK_SRV_SC_SGL_DESCRIPTOR_TYPE_INVALID = 0x11,
+		SPDK_SRV_SC_INVALID_CONTROLLER_MEM_BUF = 0x12,
+		SPDK_SRV_SC_INVALID_PRP_OFFSET = 0x13,
+		SPDK_SRV_SC_ATOMIC_WRITE_UNIT_EXCEEDED = 0x14,
+		SPDK_SRV_SC_OPERATION_DENIED = 0x15,
+		SPDK_SRV_SC_INVALID_SGL_OFFSET = 0x16,
+		/* 0x17 - reserved */
+		SPDK_SRV_SC_HOSTID_INCONSISTENT_FORMAT = 0x18,
+		SPDK_SRV_SC_KEEP_ALIVE_EXPIRED = 0x19,
+		SPDK_SRV_SC_KEEP_ALIVE_INVALID = 0x1a,
+		SPDK_SRV_SC_ABORTED_PREEMPT = 0x1b,
+		SPDK_SRV_SC_SANITIZE_FAILED = 0x1c,
+		SPDK_SRV_SC_SANITIZE_IN_PROGRESS = 0x1d,
+		SPDK_SRV_SC_SGL_DATA_BLOCK_GRANULARITY_INVALID = 0x1e,
+		SPDK_SRV_SC_COMMAND_INVALID_IN_CMB = 0x1f,
+		SPDK_SRV_SC_COMMAND_NAMESPACE_IS_PROTECTED = 0x20,
+		SPDK_SRV_SC_COMMAND_INTERRUPTED = 0x21,
+		SPDK_SRV_SC_COMMAND_TRANSIENT_TRANSPORT_ERROR = 0x22,
+
+		SPDK_SRV_SC_LBA_OUT_OF_RANGE = 0x80,
+		SPDK_SRV_SC_CAPACITY_EXCEEDED = 0x81,
+		SPDK_SRV_SC_NAMESPACE_NOT_READY = 0x82,
+		SPDK_SRV_SC_RESERVATION_CONFLICT = 0x83,
+		SPDK_SRV_SC_FORMAT_IN_PROGRESS = 0x84,
+	};
+
+	enum spdk_srv_rdma_transport_error
+	{
+		SPDK_SRV_RDMA_ERROR_INVALID_PRIVATE_DATA_LENGTH = 0x1,
+		SPDK_SRV_RDMA_ERROR_INVALID_RECFMT = 0x2,
+		SPDK_SRV_RDMA_ERROR_INVALID_QID = 0x3,
+		SPDK_SRV_RDMA_ERROR_INVALID_HSQSIZE = 0x4,
+		SPDK_SRV_RDMA_ERROR_INVALID_HRQSIZE = 0x5,
+		SPDK_SRV_RDMA_ERROR_NO_RESOURCES = 0x6,
+		SPDK_SRV_RDMA_ERROR_INVALID_IRD = 0x7,
+		SPDK_SRV_RDMA_ERROR_INVALID_ORD = 0x8,
+	};
+
+	enum spdk_srv_request_exec_status
+	{
+		SPDK_SRV_REQUEST_EXEC_STATUS_COMPLETE,
+		SPDK_SRV_REQUEST_EXEC_STATUS_ASYNCHRONOUS,
+	};
+
+	struct spdk_srv_transport_id
+	{
+		/**
+		 * Srv transport string.
+		 */
+		char trstring[SPDK_SRV_TRSTRING_MAX_LEN + 1];
+
+		/**
+		 * Srv transport type.
+		 */
+		enum spdk_srv_transport_type trtype;
+
+		/**
+		 * Address family of the transport address.
+		 *
+		 * For PCIe, this value is ignored.
+		 */
+		enum spdk_srv_adrfam adrfam;
+
+		/**
+		 * Transport address of the Srv-oF endpoint. For transports which use IP
+		 * addressing (e.g. RDMA), this should be an IP address. For PCIe, this
+		 * can either be a zero length string (the whole bus) or a PCI address
+		 * in the format DDDD:BB:DD.FF or DDDD.BB.DD.FF. For FC the string is
+		 * formatted as: nn-0xWWNN:pn-0xWWPN” where WWNN is the Node_Name of the
+		 * target Srv_Port and WWPN is the N_Port_Name of the target Srv_Port.
+		 */
+		char traddr[SPDK_SRV_TRADDR_MAX_LEN + 1];
+
+		/**
+		 * Transport service id of the Srv-oF endpoint.  For transports which use
+		 * IP addressing (e.g. RDMA), this field should be the port number. For PCIe,
+		 * and FC this is always a zero length string.
+		 */
+		char trsvcid[SPDK_SRV_TRSVCID_MAX_LEN + 1];
+
+		/**
+		 * The Transport connection priority of the Srv-oF endpoint. Currently this is
+		 * only supported by posix based sock implementation on Kernel TCP stack. More
+		 * information of this field can be found from the socket(7) man page.
+		 */
+		int priority;
+	};
+
+	struct spdk_srv_listener
+	{
+		struct spdk_srv_transport_id trid;
+		uint32_t ref;
+
+		TAILQ_ENTRY(spdk_srv_listener)
+		link;
+	};
+
+	struct spdk_srv_conn
+	{
+		enum spdk_srv_conn_state state;
+		spdk_srv_state_change_done state_cb;
+		void *state_cb_arg;
+
+		struct spdk_srv_transport *transport;
+		//	struct spdk_srv_ctrlr			*ctrlr;
+		struct spdk_srv_poll_group *group;
+
+		uint16_t qid;
+		uint16_t sq_head;
+		uint16_t sq_head_max;
+		bool disconnect_started;
+
+		struct spdk_srv_request *first_fused_req;
+
+		TAILQ_HEAD(, spdk_srv_request)
+		outstanding;
+		TAILQ_ENTRY(spdk_srv_conn)
+		link;
+	};
+
+	struct spdk_srv_target_opts
+	{
+		char name[SRV_TGT_NAME_MAX_LENGTH];
+	};
+
+	struct spdk_srv_transport_opts
+	{
+		uint16_t max_queue_depth;
+		uint16_t max_conns_per_tgt;
+		uint32_t in_capsule_data_size;
+		/* used to calculate mdts */
+		uint32_t max_io_size;
+		uint32_t io_unit_size;
+		uint32_t max_aq_depth;
+		uint32_t num_shared_buffers;
+		uint32_t buf_cache_size;
+		bool dif_insert_or_strip;
+
+		uint32_t abort_timeout_sec;
+		/* ms */
+		uint32_t association_timeout;
+
+		const struct spdk_json_val *transport_specific;
+
+		/**
+		 * The size of spdk_srv_transport_opts according to the caller of this library is used for ABI
+		 * compatibility. The library uses this field to know how many fields in this
+		 * structure are valid. And the library will populate any remaining fields with default values.
+		 * New added fields should be put at the end of the struct.
+		 */
+		size_t opts_size;
+		uint32_t acceptor_poll_rate;
+		/* Use zero-copy operations if the underlying bdev supports them */
+		bool zcopy;
+	};
+
+	struct spdk_srv_listen_opts
+	{
+		/**
+		 * The size of spdk_srv_listen_opts according to the caller of this library is used for ABI
+		 * compatibility. The library uses this field to know how many fields in this
+		 * structure are valid. And the library will populate any remaining fields with default values.
+		 * New added fields should be put at the end of the struct.
+		 */
+		size_t opts_size;
+
+		const struct spdk_json_val *transport_specific;
+	};
+
+	struct spdk_srv_transport_ops
+	{
+		/**
+		 * Transport name
+		 */
+		char name[SPDK_SRV_TRSTRING_MAX_LEN];
+
+		/**
+		 * Transport type
+		 */
+		enum spdk_srv_transport_type type;
+
+		/**
+		 * Initialize transport options to default value
+		 */
+		void (*opts_init)(struct spdk_srv_transport_opts *opts);
+
+		/**
+		 * Create a transport for the given transport opts
+		 */
+		struct spdk_srv_transport *(*create)(struct spdk_srv_transport_opts *opts);
+
+		/**
+		 * Dump transport-specific opts into JSON
+		 */
+		void (*dump_opts)(struct spdk_srv_transport *transport,
+						  struct spdk_json_write_ctx *w);
+
+		/**
+		 * Destroy the transport
+		 */
+		int (*destroy)(struct spdk_srv_transport *transport,
+					   spdk_srv_transport_destroy_done_cb cb_fn, void *cb_arg);
+
+		/**
+		 * Instruct the transport to accept new connections at the address
+		 * provided. This may be called multiple times.
+		 */
+		int (*listen)(struct spdk_srv_transport *transport, const struct spdk_srv_transport_id *trid,
+					  struct spdk_srv_listen_opts *opts);
+
+		/**
+		 * Dump transport-specific listen opts into JSON
+		 */
+		void (*listen_dump_opts)(struct spdk_srv_transport *transport,
+								 const struct spdk_srv_transport_id *trid, struct spdk_json_write_ctx *w);
+
+		/**
+		 * Stop accepting new connections at the given address.
+		 */
+		void (*stop_listen)(struct spdk_srv_transport *transport,
+							const struct spdk_srv_transport_id *trid);
+
+		/**
+		 * Create a new poll group
+		 */
+		struct spdk_srv_transport_poll_group *(*poll_group_create)(struct spdk_srv_transport *transport);
+
+		/**
+		 * Get the polling group of the queue pair optimal for the specific transport
+		 */
+		struct spdk_srv_transport_poll_group *(*get_optimal_poll_group)(struct spdk_srv_conn *conn);
+
+		/**
+		 * Destroy a poll group
+		 */
+		void (*poll_group_destroy)(struct spdk_srv_transport_poll_group *group);
+
+		/**
+		 * Add a qpair to a poll group
+		 */
+		int (*poll_group_add)(struct spdk_srv_transport_poll_group *group,
+							  struct spdk_srv_conn *conn);
+
+		/**
+		 * Remove a qpair from a poll group
+		 */
+		int (*poll_group_remove)(struct spdk_srv_transport_poll_group *group,
+								 struct spdk_srv_conn *conn);
+
+		/**
+		 * Poll the group to process I/O
+		 */
+		int (*poll_group_poll)(struct spdk_srv_transport_poll_group *group);
+
+		/*
+		 * Free the request without sending a response
+		 * to the originator. Release memory tied to this request.
+		 */
+		int (*req_free)(struct spdk_srv_request *req);
+
+		/*
+		 * Signal request completion, which sends a response
+		 * to the originator.
+		 */
+		int (*req_complete)(struct spdk_srv_request *req);
+
+		/*
+		 * Deinitialize a connection.
+		 */
+		void (*conn_fini)(struct spdk_srv_conn *conn,
+						  spdk_srv_transport_conn_fini_cb cb_fn,
+						  void *cb_args);
+
+		/*
+		 * Get the peer transport ID for the queue pair.
+		 */
+		int (*conn_get_peer_trid)(struct spdk_srv_conn *conn,
+								  struct spdk_srv_transport_id *trid);
+
+		/*
+		 * Get the local transport ID for the queue pair.
+		 */
+		int (*conn_get_local_trid)(struct spdk_srv_conn *conn,
+								   struct spdk_srv_transport_id *trid);
+
+		/*
+		 * Get the listener transport ID that accepted this qpair originally.
+		 */
+		int (*conn_get_listen_trid)(struct spdk_srv_conn *conn,
+									struct spdk_srv_transport_id *trid);
+
+		/*
+		 * Abort the request which the abort request specifies.
+		 * This function can complete synchronously or asynchronously, but
+		 * is expected to call spdk_srv_request_complete() in the end
+		 * for both cases.
+		 */
+		void (*conn_abort_request)(struct spdk_srv_conn *conn,
+								   struct spdk_srv_request *req);
+
+		/*
+		 * Dump transport poll group statistics into JSON.
+		 */
+		void (*poll_group_dump_stat)(struct spdk_srv_transport_poll_group *group,
+									 struct spdk_json_write_ctx *w);
+	};
+
+	struct spdk_srv_request
+	{
+		struct spdk_srv_conn *conn;
+		uint32_t length;
+		uint8_t xfer; /* type enum spdk_srv_data_transfer */
+		bool data_from_pool;
+		bool dif_enabled;
+		void *data;
+		struct spdk_req_cmd *cmd;
+		struct spdk_req_cpl *rsp;
+		STAILQ_ENTRY(spdk_srv_request)
+		buf_link;
+		uint64_t timeout_tsc;
+
+		uint32_t iovcnt;
+		struct iovec iov[SRV_REQ_MAX_BUFFERS];
+		void *buffers[SRV_REQ_MAX_BUFFERS];
+
+		struct spdk_bdev_io_wait_entry bdev_io_wait;
+		struct spdk_srv_request *req_to_abort;
+		struct spdk_poller *poller;
+		struct spdk_bdev_io *zcopy_bdev_io; /* Contains the bdev_io when using ZCOPY */
+
+		TAILQ_ENTRY(spdk_srv_request)
+		link;
+	};
+
+	struct spdk_srv_transport_pg_cache_buf
+	{
+		STAILQ_ENTRY(spdk_srv_transport_pg_cache_buf)
+		link;
+	};
+
+	struct spdk_srv_transport_poll_group
+	{
+		struct spdk_srv_transport *transport;
+		/* Requests that are waiting to obtain a data buffer */
+		STAILQ_HEAD(, spdk_srv_request)
+		pending_buf_queue;
+		STAILQ_HEAD(, spdk_srv_transport_pg_cache_buf)
+		buf_cache;
+		uint32_t buf_cache_count;
+		uint32_t buf_cache_size;
+		struct spdk_srv_poll_group *group;
+		TAILQ_ENTRY(spdk_srv_transport_poll_group)
+		link;
+	};
+
+	struct spdk_srv_poll_group_stat
+	{
+		/* cumulative io qpair count */
+		uint32_t conns;
+		/* current io qpair count */
+		uint32_t current_conns;
+		uint64_t pending_bdev_io;
+	};
+
+	struct spdk_srv_poll_group
+	{
+		struct spdk_thread *thread;
+		struct spdk_poller *poller;
+		uint64_t poll_cnt;
+		uint64_t last_tick;
+		TAILQ_HEAD(, spdk_srv_transport_poll_group)
+		tgroups;
+
+		/* All of the queue pairs that belong to this poll group */
+		TAILQ_HEAD(, spdk_srv_conn)
+		conns;
+
+		/* Statistics */
+		struct spdk_srv_poll_group_stat stat;
+
+		spdk_srv_poll_group_destroy_done_fn destroy_cb_fn;
+		void *destroy_cb_arg;
+
+		TAILQ_ENTRY(spdk_srv_poll_group)
+		link;
+	};
+
+	static inline enum spdk_srv_data_transfer
+	spdk_srv_req_get_xfer(struct spdk_srv_request *req)
+	{
+		enum spdk_srv_data_transfer xfer;
+		struct spdk_req_cmd *cmd = req->cmd;
+		struct spdk_req_sgl_descriptor *sgl = &cmd->dptr.sgl1;
+
+		xfer = spdk_srv_opc_get_data_transfer(cmd->opc);
+
+		if (xfer == SPDK_SRV_DATA_NONE)
+		{
+			return xfer;
+		}
+
+		/* Even for commands that may transfer data, they could have specified 0 length.
+		 * We want those to show up with xfer SPDK_SRV_DATA_NONE.
+		 */
+		switch (sgl->generic.type)
+		{
+		case SPDK_SRV_SGL_TYPE_DATA_BLOCK:
+		case SPDK_SRV_SGL_TYPE_BIT_BUCKET:
+		case SPDK_SRV_SGL_TYPE_SEGMENT:
+		case SPDK_SRV_SGL_TYPE_LAST_SEGMENT:
+		case SPDK_SRV_SGL_TYPE_TRANSPORT_DATA_BLOCK:
+			if (sgl->unkeyed.length == 0)
+			{
+				xfer = SPDK_SRV_DATA_NONE;
+			}
+			break;
+		case SPDK_SRV_SGL_TYPE_KEYED_DATA_BLOCK:
+			if (sgl->keyed.length == 0)
+			{
+				xfer = SPDK_SRV_DATA_NONE;
+			}
+			break;
+		}
+
+		return xfer;
+	}
+
+	struct spdk_srv_transport
+	{
+		struct spdk_srv_tgt *tgt;
+		const struct spdk_srv_transport_ops *ops;
+		struct spdk_srv_transport_opts opts;
+
+		/* A mempool for transport related data transfers */
+		struct spdk_mempool *data_buf_pool;
+
+		TAILQ_HEAD(, spdk_srv_listener)
+		listeners;
+		TAILQ_ENTRY(spdk_srv_transport)
+		link;
+	};
+
+	typedef enum spdk_srv_transport_type spdk_srv_transport_type_t;
+
+	struct spdk_srv_tgt
+	{
+		char name[SRV_TGT_NAME_MAX_LENGTH];
+
+		pthread_mutex_t mutex;
+
+		uint64_t discovery_genctr;
+
+		TAILQ_HEAD(, spdk_srv_transport)
+		transports;
+		TAILQ_HEAD(, spdk_srv_poll_group)
+		poll_groups;
+
+		/* Used for round-robin assignment of connections to poll groups */
+		struct spdk_srv_poll_group *next_poll_group;
+
+		spdk_srv_tgt_destroy_done_fn *destroy_cb_fn;
+		void *destroy_cb_arg;
+
+		uint16_t crdt[3];
+
+		TAILQ_ENTRY(spdk_srv_tgt)
+		link;
+	};
+
+	typedef void (*spdk_srv_poll_group_mod_done)(void *cb_arg, int status);
+
+	/**
+	 * Initialize transport options
+	 *
+	 * \param transport_name The transport type to create
+	 * \param opts The transport options (e.g. max_io_size)
+	 * \param opts_size Must be set to sizeof(struct spdk_srv_transport_opts).
+	 *
+	 * \return bool. true if successful, false if transport type
+	 *	   not found.
+	 */
+	bool
+	spdk_srv_transport_opts_init(const char *transport_name,
+								 struct spdk_srv_transport_opts *opts, size_t opts_size);
+
+	/**
+	 * Create a protocol transport
+	 *
+	 * \param transport_name The transport type to create
+	 * \param opts The transport options (e.g. max_io_size). It should not be NULL, and opts_size
+	 *        pointed in this structure should not be zero value.
+	 *
+	 * \return new transport or NULL if create fails
+	 */
+	struct spdk_srv_transport *spdk_srv_transport_create(const char *transport_name,
+														 struct spdk_srv_transport_opts *opts);
+
+	typedef void (*spdk_srv_transport_destroy_done_cb)(void *cb_arg);
+
+	/**
+	 * Destroy a protocol transport
+	 *
+	 * \param transport The transport to destroy
+	 * \param cb_fn A callback that will be called once the transport is destroyed
+	 * \param cb_arg A context argument passed to cb_fn.
+	 *
+	 * \return 0 on success, -1 on failure.
+	 */
+	int spdk_srv_transport_destroy(struct spdk_srv_transport *transport,
+								   spdk_srv_transport_destroy_done_cb cb_fn, void *cb_arg);
+
+	/**
+	 * Get an existing transport from the target
+	 *
+	 * \param tgt The NVMe-oF target
+	 * \param transport_name The name of the transport type to get.
+	 *
+	 * \return the transport or NULL if not found
+	 */
+	struct spdk_srv_transport *spdk_srv_tgt_get_transport(struct spdk_srv_tgt *tgt,
+														  const char *transport_name);
+
+	/**
+	 * Get the first transport registered with the given target
+	 *
+	 * \param tgt The NVMe-oF target
+	 *
+	 * \return The first transport registered on the target
+	 */
+	struct spdk_srv_transport *spdk_srv_transport_get_first(struct spdk_srv_tgt *tgt);
+
+	/**
+	 * Get the next transport in a target's list.
+	 *
+	 * \param transport A handle to a transport object
+	 *
+	 * \return The next transport associated with the NVMe-oF target
+	 */
+	struct spdk_srv_transport *spdk_srv_transport_get_next(struct spdk_srv_transport *transport);
+
+	/**
+	 * Get the opts for a given transport.
+	 *
+	 * \param transport The transport to query
+	 *
+	 * \return The opts associated with the given transport
+	 */
+	const struct spdk_srv_transport_opts *spdk_srv_get_transport_opts(struct spdk_srv_transport
+																		  *transport);
+
+	/**
+	 * Get the transport type for a given transport.
+	 *
+	 * \param transport The transport to query
+	 *
+	 * \return the transport type for the given transport
+	 */
+	spdk_srv_transport_type_t spdk_srv_get_transport_type(struct spdk_srv_transport *transport);
+
+	/**
+	 * Get the transport name for a given transport.
+	 *
+	 * \param transport The transport to query
+	 *
+	 * \return the transport name for the given transport
+	 */
+	const char *spdk_srv_get_transport_name(struct spdk_srv_transport *transport);
+
+	/**
+	 * Function to be called once transport add is complete
+	 *
+	 * \param cb_arg Callback argument passed to this function.
+	 * \param status 0 if it completed successfully, or negative errno if it failed.
+	 */
+	typedef void (*spdk_srv_tgt_add_transport_done_fn)(void *cb_arg, int status);
+
+	/**
+	 * Add a transport to a target
+	 *
+	 * \param tgt The NVMe-oF target
+	 * \param transport The transport to add
+	 * \param cb_fn A callback that will be called once the transport is created
+	 * \param cb_arg A context argument passed to cb_fn.
+	 */
+	void spdk_srv_tgt_add_transport(struct spdk_srv_tgt *tgt,
+									struct spdk_srv_transport *transport,
+									spdk_srv_tgt_add_transport_done_fn cb_fn,
+									void *cb_arg);
+
+	/**
+	 * Add listener to transport and begin accepting new connections.
+	 *
+	 * \param transport The transport to add listener to.
+	 * \param trid The address to listen at.
+	 * \param opts Listener options.
+	 *
+	 * \return int. 0 if it completed successfully, or negative errno if it failed.
+	 */
+	int
+	spdk_srv_transport_listen(struct spdk_srv_transport *transport,
+							  const struct spdk_srv_transport_id *trid, struct spdk_srv_listen_opts *opts);
+
+	/**
+	 * Remove listener from transport and stop accepting new connections.
+	 *
+	 * \param transport The transport to remove listener from
+	 * \param trid Address to stop listen at
+	 *
+	 * \return int. 0 if it completed successfully, or negative errno if it failed.
+	 */
+	int
+	spdk_srv_transport_stop_listen(struct spdk_srv_transport *transport,
+								   const struct spdk_srv_transport_id *trid);
+
+	/**
+	 * Dump poll group statistics into JSON.
+	 *
+	 * \param group The group which statistics should be dumped.
+	 * \param w The JSON write context to which statistics should be dumped.
+	 */
+	void spdk_srv_poll_group_dump_stat(struct spdk_srv_poll_group *group,
+									   struct spdk_json_write_ctx *w);
+
+	struct spdk_srv_tgt *
+	spdk_srv_tgt_create(struct spdk_srv_target_opts *opts);
+
+	void
+	spdk_srv_tgt_destroy(struct spdk_srv_tgt *tgt,
+						 spdk_srv_tgt_destroy_done_fn cb_fn,
+						 void *cb_arg);
+
+	struct spdk_srv_tgt *
+	spdk_srv_get_tgt(const char *name);
+
+	int
+	spdk_srv_tgt_stop_listen(struct spdk_srv_tgt *tgt,
+							 struct spdk_srv_transport_id *trid);
+
+	void
+	spdk_srv_tgt_new_conn(struct spdk_srv_tgt *tgt, struct spdk_srv_conn *conn);
+
+	struct spdk_srv_poll_group *
+	spdk_srv_poll_group_create(struct spdk_srv_tgt *tgt);
+
+	void
+	spdk_srv_poll_group_destroy(struct spdk_srv_poll_group *group,
+								spdk_srv_poll_group_destroy_done_fn cb_fn,
+								void *cb_arg);
+
+	void
+	spdk_srv_poll_group_destroy(struct spdk_srv_poll_group *group,
+								spdk_srv_poll_group_destroy_done_fn cb_fn,
+								void *cb_arg);
+
+	void
+	spdk_srv_poll_group_remove(struct spdk_srv_conn *conn);
+
+	void spdk_srv_transport_register(const struct spdk_srv_transport_ops *ops);
+	int spdk_srv_request_complete(struct spdk_srv_request *req);
+	int spdk_srv_request_get_buffers(struct spdk_srv_request *req,
+									 struct spdk_srv_transport_poll_group *group,
+									 struct spdk_srv_transport *transport,
+									 uint32_t length);
+	void
+	spdk_srv_request_free_buffers(struct spdk_srv_request *req,
+								  struct spdk_srv_transport_poll_group *group,
+								  struct spdk_srv_transport *transport);
+
+	int
+	spdk_srv_transport_id_compare(const struct spdk_srv_transport_id *trid1,
+								  const struct spdk_srv_transport_id *trid2);
+
+	void
+	spdk_srv_request_exec(struct spdk_srv_request *req);
+
+	void
+	spdk_srv_trid_populate_transport(struct spdk_srv_transport_id *trid,
+									 enum spdk_srv_transport_type trtype);
+
+	void spdk_srv_rpc_register_dispatcher(void *dispatcher, int submit_type);
+
+	typedef void (*srv_conn_disconnect_cb)(void *ctx);
+	int
+	spdk_srv_conn_disconnect(struct spdk_srv_conn *conn, srv_conn_disconnect_cb cb_fn, void *ctx);
+
+#define SPDK_SRV_TRANSPORT_REGISTER(name, transport_ops)                               \
+	static void __attribute__((constructor)) _spdk_srv_transport_register_##name(void) \
+	{                                                                                  \
+		spdk_srv_transport_register(transport_ops);                                    \
+	}
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif
\ No newline at end of file
diff --git a/include/spdk_internal/rdma.h b/include/spdk_internal/rdma.h
index 654e49969..5b8331fb6 100644
--- a/include/spdk_internal/rdma.h
+++ b/include/spdk_internal/rdma.h
@@ -42,6 +42,10 @@
 /* Contains hooks definition */
 #include "spdk/nvme.h"
 
+#ifdef __cplusplus
+extern "C" {
+#endif
+
 /* rxe driver vendor_id has been changed from 0 to 0XFFFFFF in 0184afd15a141d7ce24c32c0d86a1e3ba6bc0eb3 */
 #define SPDK_RDMA_RXE_VENDOR_ID_OLD 0
 #define SPDK_RDMA_RXE_VENDOR_ID_NEW 0XFFFFFF
@@ -299,4 +303,8 @@ static inline uint32_t spdk_rdma_memory_translation_get_rkey(struct spdk_rdma_me
 	       translation->mr_or_key.mr->rkey : (uint32_t)translation->mr_or_key.key;
 }
 
+#ifdef __cplusplus
+}
+#endif
+
 #endif /* SPDK_RDMA_H */
diff --git a/include/spdk_internal/rdma_client.h b/include/spdk_internal/rdma_client.h
new file mode 100644
index 000000000..819be6dcc
--- /dev/null
+++ b/include/spdk_internal/rdma_client.h
@@ -0,0 +1,1952 @@
+/*-
+ *   BSD LICENSE
+ *
+ *   Copyright (c) Intel Corporation. All rights reserved.
+ *   Copyright (c) 2020, 2021 Mellanox Technologies LTD. All rights reserved.
+ *   Copyright (c) 2021 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ *
+ *   Redistribution and use in source and binary forms, with or without
+ *   modification, are permitted provided that the following conditions
+ *   are met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in
+ *       the documentation and/or other materials provided with the
+ *       distribution.
+ *     * Neither the name of Intel Corporation nor the names of its
+ *       contributors may be used to endorse or promote products derived
+ *       from this software without specific prior written permission.
+ *
+ *   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ *   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ *   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ *   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ *   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ *   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ *   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ *   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ *   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ *   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ *   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef __CLIENT_INTERNAL_H__
+#define __CLIENT_INTERNAL_H__
+
+#ifdef __cplusplus
+extern "C"
+{
+#endif
+
+#include "spdk/config.h"
+#include "spdk/likely.h"
+#include "spdk/stdinc.h"
+
+#if defined(__i386__) || defined(__x86_64__)
+#include <x86intrin.h>
+#endif
+
+#include "spdk/queue.h"
+#include "spdk/barrier.h"
+#include "spdk/bit_array.h"
+#include "spdk/mmio.h"
+#include "spdk/pci_ids.h"
+#include "spdk/util.h"
+#include "spdk/memory.h"
+#include "spdk/tree.h"
+#include "spdk/uuid.h"
+
+#include "spdk/log.h"
+#include "spdk/rdma_client.h"
+
+extern pid_t g_spdk_client_pid;
+
+#define SPDK_CLIENT_MAX_IO_QUEUES (65535)
+
+#define SPDK_CLIENT_IO_QUEUE_MIN_ENTRIES 2
+#define SPDK_CLIENT_IO_QUEUE_MAX_ENTRIES 65536
+
+enum spdk_client_sgl_descriptor_type
+{
+	SPDK_CLIENT_SGL_TYPE_DATA_BLOCK = 0x0,
+	SPDK_CLIENT_SGL_TYPE_BIT_BUCKET = 0x1,
+	SPDK_CLIENT_SGL_TYPE_SEGMENT = 0x2,
+	SPDK_CLIENT_SGL_TYPE_LAST_SEGMENT = 0x3,
+	SPDK_CLIENT_SGL_TYPE_KEYED_DATA_BLOCK = 0x4,
+	SPDK_CLIENT_SGL_TYPE_TRANSPORT_DATA_BLOCK = 0x5,
+	/* 0x6 - 0xE reserved */
+	SPDK_CLIENT_SGL_TYPE_VENDOR_SPECIFIC = 0xF
+};
+
+enum spdk_client_sgl_descriptor_subtype
+{
+	SPDK_CLIENT_SGL_SUBTYPE_ADDRESS = 0x0,
+	SPDK_CLIENT_SGL_SUBTYPE_OFFSET = 0x1,
+	SPDK_CLIENT_SGL_SUBTYPE_TRANSPORT = 0xa,
+};
+
+enum spdk_client_psdt_value
+{
+	SPDK_CLIENT_PSDT_PRP = 0x0,
+	SPDK_CLIENT_PSDT_SGL_MPTR_CONTIG = 0x1,
+	SPDK_CLIENT_PSDT_SGL_MPTR_SGL = 0x2,
+	SPDK_CLIENT_PSDT_RESERVED = 0x3
+};
+
+enum spdk_client_data_transfer
+{
+	/** Opcode does not transfer data */
+	SPDK_CLIENT_DATA_NONE = 0,
+	/** Opcode transfers data from host to controller (e.g. Write) */
+	SPDK_CLIENT_DATA_HOST_TO_CONTROLLER = 1,
+	/** Opcode transfers data from controller to host (e.g. Read) */
+	SPDK_CLIENT_DATA_CONTROLLER_TO_HOST = 2,
+	/** Opcode transfers data both directions */
+	SPDK_CLIENT_DATA_BIDIRECTIONAL = 3
+};
+
+/*
+ * Some Intel devices support vendor-unique read latency log page even
+ * though the log page directory says otherwise.
+ */
+#define CLIENT_INTEL_QUIRK_READ_LATENCY 0x1
+
+/*
+ * Some Intel devices support vendor-unique write latency log page even
+ * though the log page directory says otherwise.
+ */
+#define CLIENT_INTEL_QUIRK_WRITE_LATENCY 0x2
+
+/*
+ * The controller needs a delay before starts checking the device
+ * readiness, which is done by reading the CLIENT_CSTS_RDY bit.
+ */
+#define CLIENT_QUIRK_DELAY_BEFORE_CHK_RDY 0x4
+
+/*
+ * The controller performs best when I/O is split on particular
+ * LBA boundaries.
+ */
+#define CLIENT_INTEL_QUIRK_STRIPING 0x8
+
+/*
+ * The controller needs a delay after allocating an I/O queue pair
+ * before it is ready to accept I/O commands.
+ */
+#define CLIENT_QUIRK_DELAY_AFTER_QUEUE_ALLOC 0x10
+
+/*
+ * Earlier Client devices do not indicate whether unmapped blocks
+ * will read all zeroes or not. This define indicates that the
+ * device does in fact read all zeroes after an unmap event
+ */
+#define CLIENT_QUIRK_READ_ZERO_AFTER_DEALLOCATE 0x20
+
+/*
+ * The controller doesn't handle Identify value others than 0 or 1 correctly.
+ */
+#define CLIENT_QUIRK_IDENTIFY_CNS 0x40
+
+/*
+ * The controller supports Open Channel command set if matching additional
+ * condition, like the first byte (value 0x1) in the vendor specific
+ * bits of the namespace identify structure is set.
+ */
+#define CLIENT_QUIRK_OCSSD 0x80
+
+/*
+ * The controller has an Intel vendor ID but does not support Intel vendor-specific
+ * log pages.  This is primarily for QEMU emulated SSDs which report an Intel vendor
+ * ID but do not support these log pages.
+ */
+#define CLIENT_INTEL_QUIRK_NO_LOG_PAGES 0x100
+
+/*
+ * The controller does not set SHST_COMPLETE in a reasonable amount of time.  This
+ * is primarily seen in virtual VMWare Client SSDs.  This quirk merely adds an additional
+ * error message that on VMWare Client SSDs, the shutdown timeout may be expected.
+ */
+#define CLIENT_QUIRK_SHST_COMPLETE 0x200
+
+/*
+ * The controller requires an extra delay before starting the initialization process
+ * during attach.
+ */
+#define CLIENT_QUIRK_DELAY_BEFORE_INIT 0x400
+
+/*
+ * Some SSDs exhibit poor performance with the default SPDK Client IO queue size.
+ * This quirk will increase the default to 1024 which matches other operating
+ * systems, at the cost of some extra memory usage.  Users can still override
+ * the increased default by changing the spdk_client_io_qpair_opts when allocating
+ * a new queue pair.
+ */
+#define CLIENT_QUIRK_MINIMUM_IO_QUEUE_SIZE 0x800
+
+/**
+ * The maximum access width to PCI memory space is 8 Bytes, don't use AVX2 or
+ * SSE instructions to optimize the memory access(memcpy or memset) larger than
+ * 8 Bytes.
+ */
+#define CLIENT_QUIRK_MAXIMUM_PCI_ACCESS_WIDTH 0x1000
+
+/**
+ * The SSD does not support OPAL even through it sets the security bit in OACS.
+ */
+#define CLIENT_QUIRK_OACS_SECURITY 0x2000
+
+/**
+ * Intel P55XX SSDs can't support Dataset Management command with SGL format,
+ * so use PRP with DSM command.
+ */
+#define CLIENT_QUIRK_NO_SGL_FOR_DSM 0x4000
+
+/**
+ * Maximum Data Transfer Size(MDTS) excludes interleaved metadata.
+ */
+#define CLIENT_QUIRK_MDTS_EXCLUDE_MD 0x8000
+
+#define CLIENT_MAX_ASYNC_EVENTS (8)
+
+#define CLIENT_MAX_ADMIN_TIMEOUT_IN_SECS (30)
+
+/* Maximum log page size to fetch for AERs. */
+#define CLIENT_MAX_AER_LOG_SIZE (4096)
+
+/*
+ * CLIENT_MAX_IO_QUEUES in client_spec.h defines the 64K spec-limit, but this
+ *  define specifies the maximum number of queues this driver will actually
+ *  try to configure, if available.
+ */
+#define DEFAULT_MAX_IO_QUEUES (1024)
+#define DEFAULT_ADMIN_QUEUE_SIZE (32)
+#define DEFAULT_IO_QUEUE_SIZE (128)
+#define DEFAULT_IO_QUEUE_SIZE_FOR_QUIRK (1024) /* Matches Linux kernel driver */
+
+#define DEFAULT_IO_QUEUE_REQUESTS (4096)
+
+#define DEFAULT_SECTOR_SIZE (512)
+
+#define DEFAULT_SECTORS_PER_MAX_IO (256)
+
+#define DEFAULT_SECTORS_PER_STRIPE (0)
+
+#define DEFAULT_EXTENDED_LBA_SIZE (0)
+
+#define DEFAULT_MD_SIZE (0)
+
+#define SPDK_CLIENT_DEFAULT_RETRY_COUNT (4)
+
+#define SPDK_CLIENT_TRANSPORT_ACK_TIMEOUT_DISABLED (0)
+#define SPDK_CLIENT_DEFAULT_TRANSPORT_ACK_TIMEOUT SPDK_CLIENT_TRANSPORT_ACK_TIMEOUT_DISABLED
+
+#define MIN_KEEP_ALIVE_TIMEOUT_IN_MS (10000)
+
+/* We want to fit submission and completion rings each in a single 2MB
+ * hugepage to ensure physical address contiguity.
+ */
+#define MAX_IO_QUEUE_ENTRIES (VALUE_2MB / spdk_max(                        \
+											  sizeof(struct spdk_req_cmd), \
+											  sizeof(struct spdk_req_cpl)))
+
+/* Default timeout for fabrics connect commands. */
+#ifdef DEBUG
+#define CLIENT_FABRIC_CONNECT_COMMAND_TIMEOUT 0
+#else
+/* 500 millisecond timeout. */
+#define CLIENT_FABRIC_CONNECT_COMMAND_TIMEOUT 500000
+#endif
+
+/* This value indicates that a read from a PCIe register is invalid. This can happen when a device is no longer present */
+#define SPDK_CLIENT_INVALID_REGISTER_VALUE 0xFFFFFFFFu
+
+enum client_payload_type
+{
+	CLIENT_PAYLOAD_TYPE_INVALID = 0,
+
+	/** client_request::u.payload.contig_buffer is valid for this request */
+	CLIENT_PAYLOAD_TYPE_CONTIG,
+
+	/** client_request::u.sgl is valid for this request */
+	CLIENT_PAYLOAD_TYPE_SGL,
+};
+
+/** Boot partition write states */
+enum client_bp_write_state
+{
+	SPDK_CLIENT_BP_WS_DOWNLOADING = 0x0,
+	SPDK_CLIENT_BP_WS_DOWNLOADED = 0x1,
+	SPDK_CLIENT_BP_WS_REPLACE = 0x2,
+	SPDK_CLIENT_BP_WS_ACTIVATE = 0x3,
+};
+
+/**
+ * Descriptor for a request data payload.
+ */
+struct client_payload
+{
+	/**
+	 * Functions for retrieving physical addresses for scattered payloads.
+	 */
+	spdk_client_req_reset_sgl_cb reset_sgl_fn;
+	spdk_client_req_next_sge_cb next_sge_fn;
+
+	/**
+	 * Extended IO options passed by the user
+	 */
+	struct spdk_client_ns_cmd_ext_io_opts *opts;
+	/**
+	 * If reset_sgl_fn == NULL, this is a contig payload, and contig_or_cb_arg contains the
+	 * virtual memory address of a single virtually contiguous buffer.
+	 *
+	 * If reset_sgl_fn != NULL, this is a SGL payload, and contig_or_cb_arg contains the
+	 * cb_arg that will be passed to the SGL callback functions.
+	 */
+	void *contig_or_cb_arg;
+
+	/** Virtual memory address of a single virtually contiguous metadata buffer */
+	void *md;
+
+	// used for rpc write/read
+	uint32_t rpc_request_id;
+	uint32_t data_length;
+	uint32_t rpc_opc;
+	uint32_t submit_type;
+	uint8_t *md5sum;
+};
+
+#define CLIENT_PAYLOAD_CONTIG(contig_, md_) \
+	(struct client_payload)                 \
+	{                                       \
+		.reset_sgl_fn = NULL,               \
+		.next_sge_fn = NULL,                \
+		.contig_or_cb_arg = (contig_),      \
+		.md = (md_),                        \
+	}
+
+#define CLIENT_PAYLOAD_SGL(reset_sgl_fn_, next_sge_fn_, cb_arg_, md_, rpc_request_id_, data_length_, rpc_opc_, submit_type_, md5sum_) \
+	(struct client_payload)                                                                                                           \
+	{                                                                                                                                 \
+		.reset_sgl_fn = (reset_sgl_fn_),                                                                                              \
+		.next_sge_fn = (next_sge_fn_),                                                                                                \
+		.contig_or_cb_arg = (cb_arg_),                                                                                                \
+		.md = (md_),                                                                                                                  \
+		.rpc_request_id = (rpc_request_id_),                                                                                          \
+		.data_length = (data_length_),                                                                                                \
+		.rpc_opc = (rpc_opc_),                                                                                                        \
+		.submit_type = (submit_type_),                                                                                                \
+		.md5sum = (md5sum_),                                                                                                          \
+	}
+
+static inline enum client_payload_type
+client_payload_type(const struct client_payload *payload)
+{
+	return payload->reset_sgl_fn ? CLIENT_PAYLOAD_TYPE_SGL : CLIENT_PAYLOAD_TYPE_CONTIG;
+}
+
+struct client_error_cmd
+{
+	bool do_not_submit;
+	uint64_t timeout_tsc;
+	uint32_t err_count;
+	uint8_t opc;
+	struct spdk_req_status status;
+	TAILQ_ENTRY(client_error_cmd)
+	link;
+};
+
+struct client_request
+{
+	struct spdk_req_cmd cmd;
+
+	uint8_t retries;
+
+	uint8_t timed_out : 1;
+
+	/**
+	 * True if the request is in the queued_req list.
+	 */
+	uint8_t queued : 1;
+	uint8_t reserved : 6;
+
+	/**
+	 * Number of children requests still outstanding for this
+	 *  request which was split into multiple child requests.
+	 */
+	uint16_t num_children;
+
+	/**
+	 * Offset in bytes from the beginning of payload for this request.
+	 * This is used for I/O commands that are split into multiple requests.
+	 */
+	uint32_t payload_offset;
+	uint32_t md_offset;
+
+	uint32_t payload_size;
+
+	/**
+	 * Timeout ticks for error injection requests, can be extended in future
+	 * to support per-request timeout feature.
+	 */
+	uint64_t timeout_tsc;
+
+	/**
+	 * Data payload for this request's command.
+	 */
+	struct client_payload payload;
+
+	spdk_req_cmd_cb cb_fn;
+	void *cb_arg;
+	STAILQ_ENTRY(client_request)
+	stailq;
+
+	struct spdk_client_qpair *qpair;
+
+	/*
+	 * The value of spdk_get_ticks() when the request was submitted to the hardware.
+	 * Only set if ctrlr->timeout_enabled is true.
+	 */
+	uint64_t submit_tick;
+
+	/**
+	 * The active admin request can be moved to a per process pending
+	 *  list based on the saved pid to tell which process it belongs
+	 *  to. The cpl saves the original completion information which
+	 *  is used in the completion callback.
+	 * NOTE: these below two fields are only used for admin request.
+	 */
+	pid_t pid;
+	struct spdk_req_cpl cpl;
+
+	uint32_t md_size;
+
+	/**
+	 * The following members should not be reordered with members
+	 *  above.  These members are only needed when splitting
+	 *  requests which is done rarely, and the driver is careful
+	 *  to not touch the following fields until a split operation is
+	 *  needed, to avoid touching an extra cacheline.
+	 */
+
+	/**
+	 * Points to the outstanding child requests for a parent request.
+	 *  Only valid if a request was split into multiple children
+	 *  requests, and is not initialized for non-split requests.
+	 */
+	TAILQ_HEAD(, client_request)
+	children;
+
+	/**
+	 * Linked-list pointers for a child request in its parent's list.
+	 */
+	TAILQ_ENTRY(client_request)
+	child_tailq;
+
+	/**
+	 * Points to a parent request if part of a split request,
+	 *   NULL otherwise.
+	 */
+	struct client_request *parent;
+
+	/**
+	 * Completion status for a parent request.  Initialized to all 0's
+	 *  (SUCCESS) before child requests are submitted.  If a child
+	 *  request completes with error, the error status is copied here,
+	 *  to ensure that the parent request is also completed with error
+	 *  status once all child requests are completed.
+	 */
+	struct spdk_req_cpl parent_status;
+
+	/**
+	 * The user_cb_fn and user_cb_arg fields are used for holding the original
+	 * callback data when using client_allocate_request_user_copy.
+	 */
+	spdk_req_cmd_cb user_cb_fn;
+	void *user_cb_arg;
+	void *user_buffer;
+};
+
+struct client_completion_poll_status
+{
+	struct spdk_req_cpl cpl;
+	uint64_t timeout_tsc;
+	/**
+	 * DMA buffer retained throughout the duration of the command.  It'll be released
+	 * automatically if the command times out, otherwise the user is responsible for freeing it.
+	 */
+	void *dma_data;
+	bool done;
+	/* This flag indicates that the request has been timed out and the memory
+	   must be freed in a completion callback */
+	bool timed_out;
+};
+
+struct client_async_event_request
+{
+	struct spdk_client_ctrlr *ctrlr;
+	struct client_request *req;
+	struct spdk_req_cpl cpl;
+};
+
+enum client_qpair_state
+{
+	CLIENT_QPAIR_DISCONNECTED,
+	CLIENT_QPAIR_DISCONNECTING,
+	CLIENT_QPAIR_CONNECTING,
+	CLIENT_QPAIR_CONNECTED,
+	CLIENT_QPAIR_ENABLING,
+	CLIENT_QPAIR_ENABLED,
+	CLIENT_QPAIR_DESTROYING,
+};
+
+struct spdk_client_qpair
+{
+	struct spdk_client_ctrlr *ctrlr;
+
+	spdk_connected_cb cb;
+	void *cb_args;
+	uint16_t id;
+
+	uint8_t qprio;
+
+	uint8_t state : 3;
+
+	uint8_t async : 1;
+
+	uint8_t is_new_qpair : 1;
+
+	/*
+	 * Members for handling IO qpair deletion inside of a completion context.
+	 * These are specifically defined as single bits, so that they do not
+	 *  push this data structure out to another cacheline.
+	 */
+	uint8_t in_completion_context : 1;
+	uint8_t delete_after_completion_context : 1;
+
+	/*
+	 * Set when no deletion notification is needed. For example, the process
+	 * which allocated this qpair exited unexpectedly.
+	 */
+	uint8_t no_deletion_notification_needed : 1;
+
+	uint8_t last_fuse : 2;
+
+	uint8_t transport_failure_reason : 2;
+	uint8_t last_transport_failure_reason : 2;
+
+	enum spdk_client_transport_type trtype;
+
+	/* request object used only for this qpair's FABRICS/CONNECT command (if needed) */
+	struct client_request *reserved_req;
+
+	STAILQ_HEAD(, rpc_request)
+	free_rpc_req;
+
+	STAILQ_HEAD(, client_request)
+	free_req;
+	STAILQ_HEAD(, client_request)
+	queued_req;
+
+	/* List entry for spdk_client_transport_poll_group::qpairs */
+	STAILQ_ENTRY(spdk_client_qpair)
+	poll_group_stailq;
+
+	/** Commands opcode in this list will return error */
+	TAILQ_HEAD(, client_error_cmd)
+	err_cmd_head;
+	/** Requests in this list will return error */
+	STAILQ_HEAD(, client_request)
+	err_req_head;
+
+	struct spdk_client_ctrlr_process *active_proc;
+
+	struct spdk_client_transport_poll_group *poll_group;
+
+	void *poll_group_tailq_head;
+
+	const struct spdk_client_transport *transport;
+
+	/* Entries below here are not touched in the main I/O path. */
+
+	struct client_completion_poll_status *poll_status;
+
+	/* List entry for spdk_client_ctrlr::active_io_qpairs */
+	TAILQ_ENTRY(spdk_client_qpair)
+	tailq;
+
+	/* List entry for spdk_client_ctrlr_process::allocated_io_qpairs */
+	TAILQ_ENTRY(spdk_client_qpair)
+	per_process_tailq;
+
+	STAILQ_HEAD(, client_request)
+	aborting_queued_req;
+
+	void *req_buf;
+	void *rpc_req_buf;
+	struct spdk_client_transport_id *trid;
+};
+
+struct spdk_client_poll_group
+{
+	void *ctx;
+	struct spdk_client_accel_fn_table accel_fn_table;
+	STAILQ_HEAD(, spdk_client_transport_poll_group)
+	tgroups;
+};
+
+struct spdk_client_transport_poll_group
+{
+	struct spdk_client_poll_group *group;
+	const struct spdk_client_transport *transport;
+	STAILQ_HEAD(, spdk_client_qpair)
+	connected_qpairs;
+	STAILQ_HEAD(, spdk_client_qpair)
+	disconnected_qpairs;
+	STAILQ_ENTRY(spdk_client_transport_poll_group)
+	link;
+	bool in_completion_context;
+	uint64_t num_qpairs_to_delete;
+};
+
+struct spdk_client_ns
+{
+	struct spdk_client_ctrlr *ctrlr;
+	uint32_t sector_size;
+
+	/*
+	 * Size of data transferred as part of each block,
+	 * including metadata if FLBAS indicates the metadata is transferred
+	 * as part of the data buffer at the end of each LBA.
+	 */
+	uint32_t extended_lba_size;
+
+	uint32_t md_size;
+	uint32_t pi_type;
+	uint32_t sectors_per_max_io;
+	uint32_t sectors_per_max_io_no_md;
+	uint32_t sectors_per_stripe;
+	uint32_t id;
+	uint16_t flags;
+	bool active;
+
+	RB_ENTRY(spdk_client_ns)
+	node;
+};
+
+/**
+ * State of struct spdk_client_ctrlr (in particular, during initialization).
+ */
+enum client_ctrlr_state
+{
+	/**
+	 * Wait before initializing the controller.
+	 */
+	CLIENT_CTRLR_STATE_INIT_DELAY,
+
+	/**
+	 * Connect the admin queue.
+	 */
+	CLIENT_CTRLR_STATE_CONNECT_ADMINQ,
+
+	/**
+	 * Controller has not started initialized yet.
+	 */
+	CLIENT_CTRLR_STATE_INIT = CLIENT_CTRLR_STATE_CONNECT_ADMINQ,
+
+	/**
+	 * Waiting for admin queue to connect.
+	 */
+	CLIENT_CTRLR_STATE_WAIT_FOR_CONNECT_ADMINQ,
+
+	/**
+	 * Read Version (VS) register.
+	 */
+	CLIENT_CTRLR_STATE_READ_VS,
+
+	/**
+	 * Waiting for Version (VS) register to be read.
+	 */
+	CLIENT_CTRLR_STATE_READ_VS_WAIT_FOR_VS,
+
+	/**
+	 * Read Capabilities (CAP) register.
+	 */
+	CLIENT_CTRLR_STATE_READ_CAP,
+
+	/**
+	 * Waiting for Capabilities (CAP) register to be read.
+	 */
+	CLIENT_CTRLR_STATE_READ_CAP_WAIT_FOR_CAP,
+
+	/**
+	 * Check EN to prepare for controller initialization.
+	 */
+	CLIENT_CTRLR_STATE_CHECK_EN,
+
+	/**
+	 * Waiting for CC to be read as part of EN check.
+	 */
+	CLIENT_CTRLR_STATE_CHECK_EN_WAIT_FOR_CC,
+
+	/**
+	 * Waiting for CSTS.RDY to transition from 0 to 1 so that CC.EN may be set to 0.
+	 */
+	CLIENT_CTRLR_STATE_DISABLE_WAIT_FOR_READY_1,
+
+	/**
+	 * Waiting for CSTS register to be read as part of waiting for CSTS.RDY = 1.
+	 */
+	CLIENT_CTRLR_STATE_DISABLE_WAIT_FOR_READY_1_WAIT_FOR_CSTS,
+
+	/**
+	 * Disabling the controller by setting CC.EN to 0.
+	 */
+	CLIENT_CTRLR_STATE_SET_EN_0,
+
+	/**
+	 * Waiting for the CC register to be read as part of disabling the controller.
+	 */
+	CLIENT_CTRLR_STATE_SET_EN_0_WAIT_FOR_CC,
+
+	/**
+	 * Waiting for CSTS.RDY to transition from 1 to 0 so that CC.EN may be set to 1.
+	 */
+	CLIENT_CTRLR_STATE_DISABLE_WAIT_FOR_READY_0,
+
+	/**
+	 * Waiting for CSTS register to be read as part of waiting for CSTS.RDY = 0.
+	 */
+	CLIENT_CTRLR_STATE_DISABLE_WAIT_FOR_READY_0_WAIT_FOR_CSTS,
+
+	/**
+	 * Enable the controller by writing CC.EN to 1
+	 */
+	CLIENT_CTRLR_STATE_ENABLE,
+
+	/**
+	 * Waiting for CC register to be written as part of enabling the controller.
+	 */
+	CLIENT_CTRLR_STATE_ENABLE_WAIT_FOR_CC,
+
+	/**
+	 * Waiting for CSTS.RDY to transition from 0 to 1 after enabling the controller.
+	 */
+	CLIENT_CTRLR_STATE_ENABLE_WAIT_FOR_READY_1,
+
+	/**
+	 * Waiting for CSTS register to be read as part of waiting for CSTS.RDY = 1.
+	 */
+	CLIENT_CTRLR_STATE_ENABLE_WAIT_FOR_READY_1_WAIT_FOR_CSTS,
+
+	/**
+	 * Reset the Admin queue of the controller.
+	 */
+	CLIENT_CTRLR_STATE_RESET_ADMIN_QUEUE,
+
+	/**
+	 * Identify Controller command will be sent to then controller.
+	 */
+	CLIENT_CTRLR_STATE_IDENTIFY,
+
+	/**
+	 * Waiting for Identify Controller command be completed.
+	 */
+	CLIENT_CTRLR_STATE_WAIT_FOR_IDENTIFY,
+
+	/**
+	 * Configure AER of the controller.
+	 */
+	CLIENT_CTRLR_STATE_CONFIGURE_AER,
+
+	/**
+	 * Waiting for the Configure AER to be completed.
+	 */
+	CLIENT_CTRLR_STATE_WAIT_FOR_CONFIGURE_AER,
+
+	/**
+	 * Set Keep Alive Timeout of the controller.
+	 */
+	CLIENT_CTRLR_STATE_SET_KEEP_ALIVE_TIMEOUT,
+
+	/**
+	 * Waiting for Set Keep Alive Timeout to be completed.
+	 */
+	CLIENT_CTRLR_STATE_WAIT_FOR_KEEP_ALIVE_TIMEOUT,
+
+	/**
+	 * Get Identify I/O Command Set Specific Controller data structure.
+	 */
+	CLIENT_CTRLR_STATE_IDENTIFY_IOCS_SPECIFIC,
+
+	/**
+	 * Waiting for Identify I/O Command Set Specific Controller command to be completed.
+	 */
+	CLIENT_CTRLR_STATE_WAIT_FOR_IDENTIFY_IOCS_SPECIFIC,
+
+	/**
+	 * Get Commands Supported and Effects log page for the Zoned Namespace Command Set.
+	 */
+	CLIENT_CTRLR_STATE_GET_ZNS_CMD_EFFECTS_LOG,
+
+	/**
+	 * Waiting for the Get Log Page command to be completed.
+	 */
+	CLIENT_CTRLR_STATE_WAIT_FOR_GET_ZNS_CMD_EFFECTS_LOG,
+
+	/**
+	 * Set Number of Queues of the controller.
+	 */
+	CLIENT_CTRLR_STATE_SET_NUM_QUEUES,
+
+	/**
+	 * Waiting for Set Num of Queues command to be completed.
+	 */
+	CLIENT_CTRLR_STATE_WAIT_FOR_SET_NUM_QUEUES,
+
+	/**
+	 * Get active Namespace list of the controller.
+	 */
+	CLIENT_CTRLR_STATE_IDENTIFY_ACTIVE_NS,
+
+	/**
+	 * Waiting for the Identify Active Namespace commands to be completed.
+	 */
+	CLIENT_CTRLR_STATE_WAIT_FOR_IDENTIFY_ACTIVE_NS,
+
+	/**
+	 * Get Identify Namespace Data structure for each NS.
+	 */
+	CLIENT_CTRLR_STATE_IDENTIFY_NS,
+
+	/**
+	 * Waiting for the Identify Namespace commands to be completed.
+	 */
+	CLIENT_CTRLR_STATE_WAIT_FOR_IDENTIFY_NS,
+
+	/**
+	 * Get Identify Namespace Identification Descriptors.
+	 */
+	CLIENT_CTRLR_STATE_IDENTIFY_ID_DESCS,
+
+	/**
+	 * Get Identify I/O Command Set Specific Namespace data structure for each NS.
+	 */
+	CLIENT_CTRLR_STATE_IDENTIFY_NS_IOCS_SPECIFIC,
+
+	/**
+	 * Waiting for the Identify I/O Command Set Specific Namespace commands to be completed.
+	 */
+	CLIENT_CTRLR_STATE_WAIT_FOR_IDENTIFY_NS_IOCS_SPECIFIC,
+
+	/**
+	 * Waiting for the Identify Namespace Identification
+	 * Descriptors to be completed.
+	 */
+	CLIENT_CTRLR_STATE_WAIT_FOR_IDENTIFY_ID_DESCS,
+
+	/**
+	 * Set supported log pages of the controller.
+	 */
+	CLIENT_CTRLR_STATE_SET_SUPPORTED_LOG_PAGES,
+
+	/**
+	 * Set supported log pages of INTEL controller.
+	 */
+	CLIENT_CTRLR_STATE_SET_SUPPORTED_INTEL_LOG_PAGES,
+
+	/**
+	 * Waiting for supported log pages of INTEL controller.
+	 */
+	CLIENT_CTRLR_STATE_WAIT_FOR_SUPPORTED_INTEL_LOG_PAGES,
+
+	/**
+	 * Set supported features of the controller.
+	 */
+	CLIENT_CTRLR_STATE_SET_SUPPORTED_FEATURES,
+
+	/**
+	 * Set Doorbell Buffer Config of the controller.
+	 */
+	CLIENT_CTRLR_STATE_SET_DB_BUF_CFG,
+
+	/**
+	 * Waiting for Doorbell Buffer Config to be completed.
+	 */
+	CLIENT_CTRLR_STATE_WAIT_FOR_DB_BUF_CFG,
+
+	/**
+	 * Set Host ID of the controller.
+	 */
+	CLIENT_CTRLR_STATE_SET_HOST_ID,
+
+	/**
+	 * Waiting for Set Host ID to be completed.
+	 */
+	CLIENT_CTRLR_STATE_WAIT_FOR_HOST_ID,
+
+	/**
+	 * Controller initialization has completed and the controller is ready.
+	 */
+	CLIENT_CTRLR_STATE_READY,
+
+	/**
+	 * Controller initialization has an error.
+	 */
+	CLIENT_CTRLR_STATE_ERROR
+};
+
+#define spdk_req_cpl_is_error(cpl)                 \
+	((cpl)->status.sc != SPDK_CLIENT_SC_SUCCESS || \
+	 (cpl)->status.sct != SPDK_CLIENT_SCT_GENERIC)
+
+#define spdk_req_cpl_is_success(cpl) (!spdk_req_cpl_is_error(cpl))
+
+#define spdk_req_cpl_is_pi_error(cpl)                                   \
+	((cpl)->status.sct == SPDK_CLIENT_SCT_MEDIA_ERROR &&                \
+	 ((cpl)->status.sc == SPDK_CLIENT_SC_GUARD_CHECK_ERROR ||           \
+	  (cpl)->status.sc == SPDK_CLIENT_SC_APPLICATION_TAG_CHECK_ERROR || \
+	  (cpl)->status.sc == SPDK_CLIENT_SC_REFERENCE_TAG_CHECK_ERROR))
+
+#define spdk_req_cpl_is_abort_success(cpl) \
+	(spdk_req_cpl_is_success(cpl) && !((cpl)->cdw0 & 1U))
+
+#define spdk_req_cpl_is_path_error(cpl) \
+	((cpl)->status.sct == SPDK_CLIENT_SCT_PATH)
+
+#define spdk_req_cpl_is_ana_error(cpl)                                        \
+	((cpl)->status.sct == SPDK_CLIENT_SCT_PATH &&                             \
+	 ((cpl)->status.sc == SPDK_CLIENT_SC_ASYMMETRIC_ACCESS_PERSISTENT_LOSS || \
+	  (cpl)->status.sc == SPDK_CLIENT_SC_ASYMMETRIC_ACCESS_INACCESSIBLE ||    \
+	  (cpl)->status.sc == SPDK_CLIENT_SC_ASYMMETRIC_ACCESS_TRANSITION))
+
+#define spdk_req_cpl_is_aborted_sq_deletion(cpl)     \
+	((cpl)->status.sct == SPDK_CLIENT_SCT_GENERIC && \
+	 (cpl)->status.sc == SPDK_CLIENT_SC_ABORTED_SQ_DELETION)
+
+#define spdk_req_cpl_is_aborted_by_request(cpl)      \
+	((cpl)->status.sct == SPDK_CLIENT_SCT_GENERIC && \
+	 (cpl)->status.sc == SPDK_CLIENT_SC_ABORTED_BY_REQUEST)
+
+#define CLIENT_TIMEOUT_INFINITE 0
+#define CLIENT_TIMEOUT_KEEP_EXISTING UINT64_MAX
+
+struct spdk_client_ctrlr_aer_completion_list
+{
+	struct spdk_req_cpl cpl;
+	STAILQ_ENTRY(spdk_client_ctrlr_aer_completion_list)
+	link;
+};
+
+/*
+ * Used to track properties for all processes accessing the controller.
+ */
+struct spdk_client_ctrlr_process
+{
+	/** Whether it is the primary process  */
+	bool is_primary;
+
+	/** Process ID */
+	pid_t pid;
+
+	/** Active admin requests to be completed */
+	STAILQ_HEAD(, client_request)
+	active_reqs;
+
+	TAILQ_ENTRY(spdk_client_ctrlr_process)
+	tailq;
+
+	/** Per process PCI device handle */
+	struct spdk_pci_device *devhandle;
+
+	/** Reference to track the number of attachment to this controller. */
+	int ref;
+
+	/** Allocated IO qpairs */
+	TAILQ_HEAD(, spdk_client_qpair)
+	allocated_io_qpairs;
+
+	spdk_client_aer_cb aer_cb_fn;
+	void *aer_cb_arg;
+
+	/**
+	 * A function pointer to timeout callback function
+	 */
+	spdk_client_timeout_cb timeout_cb_fn;
+	void *timeout_cb_arg;
+	/** separate timeout values for io vs. admin reqs */
+	uint64_t timeout_io_ticks;
+	uint64_t timeout_admin_ticks;
+
+	/** List to publish AENs to all procs in multiprocess setup */
+	STAILQ_HEAD(, spdk_client_ctrlr_aer_completion_list)
+	async_events;
+};
+
+struct client_register_completion
+{
+	struct spdk_req_cpl cpl;
+	uint64_t value;
+	spdk_client_reg_cb cb_fn;
+	void *cb_ctx;
+	STAILQ_ENTRY(client_register_completion)
+	stailq;
+};
+
+union spdk_client_cc_register
+{
+	uint32_t raw;
+	struct
+	{
+		/** enable */
+		uint32_t en : 1;
+
+		uint32_t reserved1 : 3;
+
+		/** i/o command set selected */
+		uint32_t css : 3;
+
+		/** memory page size */
+		uint32_t mps : 4;
+
+		/** arbitration mechanism selected */
+		uint32_t ams : 3;
+
+		/** shutdown notification */
+		uint32_t shn : 2;
+
+		/** i/o submission queue entry size */
+		uint32_t iosqes : 4;
+
+		/** i/o completion queue entry size */
+		uint32_t iocqes : 4;
+
+		uint32_t reserved2 : 8;
+	} bits;
+};
+SPDK_STATIC_ASSERT(sizeof(union spdk_client_cc_register) == 4, "Incorrect size");
+
+union spdk_client_csts_register
+{
+	uint32_t raw;
+	struct
+	{
+		/** ready */
+		uint32_t rdy : 1;
+
+		/** controller fatal status */
+		uint32_t cfs : 1;
+
+		/** shutdown status */
+		uint32_t shst : 2;
+
+		/** NVM subsystem reset occurred */
+		uint32_t nssro : 1;
+
+		/** Processing paused */
+		uint32_t pp : 1;
+
+		uint32_t reserved1 : 26;
+	} bits;
+};
+SPDK_STATIC_ASSERT(sizeof(union spdk_client_csts_register) == 4, "Incorrect size");
+
+enum spdk_client_shst_value
+{
+	SPDK_CLIENT_SHST_NORMAL = 0x0,
+	SPDK_CLIENT_SHST_OCCURRING = 0x1,
+	SPDK_CLIENT_SHST_COMPLETE = 0x2,
+};
+
+/**
+ * Status code types
+ */
+enum spdk_client_status_code_type
+{
+	SPDK_CLIENT_SCT_GENERIC = 0x0,
+	SPDK_CLIENT_SCT_COMMAND_SPECIFIC = 0x1,
+	SPDK_CLIENT_SCT_MEDIA_ERROR = 0x2,
+	SPDK_CLIENT_SCT_PATH = 0x3,
+	/* 0x4-0x6 - reserved */
+	SPDK_CLIENT_SCT_VENDOR_SPECIFIC = 0x7,
+};
+
+/**
+ * Generic command status codes
+ */
+enum spdk_client_generic_command_status_code
+{
+	SPDK_CLIENT_SC_SUCCESS = 0x00,
+	SPDK_CLIENT_SC_INVALID_OPCODE = 0x01,
+	SPDK_CLIENT_SC_INVALID_FIELD = 0x02,
+	SPDK_CLIENT_SC_COMMAND_ID_CONFLICT = 0x03,
+	SPDK_CLIENT_SC_DATA_TRANSFER_ERROR = 0x04,
+	SPDK_CLIENT_SC_ABORTED_POWER_LOSS = 0x05,
+	SPDK_CLIENT_SC_INTERNAL_DEVICE_ERROR = 0x06,
+	SPDK_CLIENT_SC_ABORTED_BY_REQUEST = 0x07,
+	SPDK_CLIENT_SC_ABORTED_SQ_DELETION = 0x08,
+	SPDK_CLIENT_SC_ABORTED_FAILED_FUSED = 0x09,
+	SPDK_CLIENT_SC_ABORTED_MISSING_FUSED = 0x0a,
+	SPDK_CLIENT_SC_INVALID_NAMESPACE_OR_FORMAT = 0x0b,
+	SPDK_CLIENT_SC_COMMAND_SEQUENCE_ERROR = 0x0c,
+	SPDK_CLIENT_SC_INVALID_SGL_SEG_DESCRIPTOR = 0x0d,
+	SPDK_CLIENT_SC_INVALID_NUM_SGL_DESCIRPTORS = 0x0e,
+	SPDK_CLIENT_SC_DATA_SGL_LENGTH_INVALID = 0x0f,
+	SPDK_CLIENT_SC_METADATA_SGL_LENGTH_INVALID = 0x10,
+	SPDK_CLIENT_SC_SGL_DESCRIPTOR_TYPE_INVALID = 0x11,
+	SPDK_CLIENT_SC_INVALID_CONTROLLER_MEM_BUF = 0x12,
+	SPDK_CLIENT_SC_INVALID_PRP_OFFSET = 0x13,
+	SPDK_CLIENT_SC_ATOMIC_WRITE_UNIT_EXCEEDED = 0x14,
+	SPDK_CLIENT_SC_OPERATION_DENIED = 0x15,
+	SPDK_CLIENT_SC_INVALID_SGL_OFFSET = 0x16,
+	/* 0x17 - reserved */
+	SPDK_CLIENT_SC_HOSTID_INCONSISTENT_FORMAT = 0x18,
+	SPDK_CLIENT_SC_KEEP_ALIVE_EXPIRED = 0x19,
+	SPDK_CLIENT_SC_KEEP_ALIVE_INVALID = 0x1a,
+	SPDK_CLIENT_SC_ABORTED_PREEMPT = 0x1b,
+	SPDK_CLIENT_SC_SANITIZE_FAILED = 0x1c,
+	SPDK_CLIENT_SC_SANITIZE_IN_PROGRESS = 0x1d,
+	SPDK_CLIENT_SC_SGL_DATA_BLOCK_GRANULARITY_INVALID = 0x1e,
+	SPDK_CLIENT_SC_COMMAND_INVALID_IN_CMB = 0x1f,
+	SPDK_CLIENT_SC_COMMAND_NAMESPACE_IS_PROTECTED = 0x20,
+	SPDK_CLIENT_SC_COMMAND_INTERRUPTED = 0x21,
+	SPDK_CLIENT_SC_COMMAND_TRANSIENT_TRANSPORT_ERROR = 0x22,
+
+	SPDK_CLIENT_SC_LBA_OUT_OF_RANGE = 0x80,
+	SPDK_CLIENT_SC_CAPACITY_EXCEEDED = 0x81,
+	SPDK_CLIENT_SC_NAMESPACE_NOT_READY = 0x82,
+	SPDK_CLIENT_SC_RESERVATION_CONFLICT = 0x83,
+	SPDK_CLIENT_SC_FORMAT_IN_PROGRESS = 0x84,
+};
+
+/**
+ * Command specific status codes
+ */
+enum spdk_client_command_specific_status_code
+{
+	SPDK_CLIENT_SC_COMPLETION_QUEUE_INVALID = 0x00,
+	SPDK_CLIENT_SC_INVALID_QUEUE_IDENTIFIER = 0x01,
+	SPDK_CLIENT_SC_INVALID_QUEUE_SIZE = 0x02,
+	SPDK_CLIENT_SC_ABORT_COMMAND_LIMIT_EXCEEDED = 0x03,
+	/* 0x04 - reserved */
+	SPDK_CLIENT_SC_ASYNC_EVENT_REQUEST_LIMIT_EXCEEDED = 0x05,
+	SPDK_CLIENT_SC_INVALID_FIRMWARE_SLOT = 0x06,
+	SPDK_CLIENT_SC_INVALID_FIRMWARE_IMAGE = 0x07,
+	SPDK_CLIENT_SC_INVALID_INTERRUPT_VECTOR = 0x08,
+	SPDK_CLIENT_SC_INVALID_LOG_PAGE = 0x09,
+	SPDK_CLIENT_SC_INVALID_FORMAT = 0x0a,
+	SPDK_CLIENT_SC_FIRMWARE_REQ_CONVENTIONAL_RESET = 0x0b,
+	SPDK_CLIENT_SC_INVALID_QUEUE_DELETION = 0x0c,
+	SPDK_CLIENT_SC_FEATURE_ID_NOT_SAVEABLE = 0x0d,
+	SPDK_CLIENT_SC_FEATURE_NOT_CHANGEABLE = 0x0e,
+	SPDK_CLIENT_SC_FEATURE_NOT_NAMESPACE_SPECIFIC = 0x0f,
+	SPDK_CLIENT_SC_FIRMWARE_REQ_NVM_RESET = 0x10,
+	SPDK_CLIENT_SC_FIRMWARE_REQ_RESET = 0x11,
+	SPDK_CLIENT_SC_FIRMWARE_REQ_MAX_TIME_VIOLATION = 0x12,
+	SPDK_CLIENT_SC_FIRMWARE_ACTIVATION_PROHIBITED = 0x13,
+	SPDK_CLIENT_SC_OVERLAPPING_RANGE = 0x14,
+	SPDK_CLIENT_SC_NAMESPACE_INSUFFICIENT_CAPACITY = 0x15,
+	SPDK_CLIENT_SC_NAMESPACE_ID_UNAVAILABLE = 0x16,
+	/* 0x17 - reserved */
+	SPDK_CLIENT_SC_NAMESPACE_ALREADY_ATTACHED = 0x18,
+	SPDK_CLIENT_SC_NAMESPACE_IS_PRIVATE = 0x19,
+	SPDK_CLIENT_SC_NAMESPACE_NOT_ATTACHED = 0x1a,
+	SPDK_CLIENT_SC_THINPROVISIONING_NOT_SUPPORTED = 0x1b,
+	SPDK_CLIENT_SC_CONTROLLER_LIST_INVALID = 0x1c,
+	SPDK_CLIENT_SC_DEVICE_SELF_TEST_IN_PROGRESS = 0x1d,
+	SPDK_CLIENT_SC_BOOT_PARTITION_WRITE_PROHIBITED = 0x1e,
+	SPDK_CLIENT_SC_INVALID_CTRLR_ID = 0x1f,
+	SPDK_CLIENT_SC_INVALID_SECONDARY_CTRLR_STATE = 0x20,
+	SPDK_CLIENT_SC_INVALID_NUM_CTRLR_RESOURCES = 0x21,
+	SPDK_CLIENT_SC_INVALID_RESOURCE_ID = 0x22,
+
+	SPDK_CLIENT_SC_IOCS_NOT_SUPPORTED = 0x29,
+	SPDK_CLIENT_SC_IOCS_NOT_ENABLED = 0x2a,
+	SPDK_CLIENT_SC_IOCS_COMBINATION_REJECTED = 0x2b,
+	SPDK_CLIENT_SC_INVALID_IOCS = 0x2c,
+
+	SPDK_CLIENT_SC_STREAM_RESOURCE_ALLOCATION_FAILED = 0x7f,
+	SPDK_CLIENT_SC_CONFLICTING_ATTRIBUTES = 0x80,
+	SPDK_CLIENT_SC_INVALID_PROTECTION_INFO = 0x81,
+	SPDK_CLIENT_SC_ATTEMPTED_WRITE_TO_RO_RANGE = 0x82,
+	SPDK_CLIENT_SC_CMD_SIZE_LIMIT_SIZE_EXCEEDED = 0x83,
+};
+
+/**
+ * Media error status codes
+ */
+enum spdk_client_media_error_status_code
+{
+	SPDK_CLIENT_SC_WRITE_FAULTS = 0x80,
+	SPDK_CLIENT_SC_UNRECOVERED_READ_ERROR = 0x81,
+	SPDK_CLIENT_SC_GUARD_CHECK_ERROR = 0x82,
+	SPDK_CLIENT_SC_APPLICATION_TAG_CHECK_ERROR = 0x83,
+	SPDK_CLIENT_SC_REFERENCE_TAG_CHECK_ERROR = 0x84,
+	SPDK_CLIENT_SC_COMPARE_FAILURE = 0x85,
+	SPDK_CLIENT_SC_ACCESS_DENIED = 0x86,
+	SPDK_CLIENT_SC_DEALLOCATED_OR_UNWRITTEN_BLOCK = 0x87,
+};
+
+/**
+ * Path related status codes
+ */
+enum spdk_client_path_status_code
+{
+	SPDK_CLIENT_SC_INTERNAL_PATH_ERROR = 0x00,
+	SPDK_CLIENT_SC_ASYMMETRIC_ACCESS_PERSISTENT_LOSS = 0x01,
+	SPDK_CLIENT_SC_ASYMMETRIC_ACCESS_INACCESSIBLE = 0x02,
+	SPDK_CLIENT_SC_ASYMMETRIC_ACCESS_TRANSITION = 0x03,
+
+	SPDK_CLIENT_SC_CONTROLLER_PATH_ERROR = 0x60,
+
+	SPDK_CLIENT_SC_HOST_PATH_ERROR = 0x70,
+	SPDK_CLIENT_SC_ABORTED_BY_HOST = 0x71,
+};
+
+#define SPDK_CLIENT_MAX_OPC 0xff
+
+/**
+ * Admin opcodes
+ */
+enum spdk_client_admin_opcode
+{
+	SPDK_CLIENT_OPC_DELETE_IO_SQ = 0x00,
+	SPDK_CLIENT_OPC_CREATE_IO_SQ = 0x01,
+	SPDK_CLIENT_OPC_GET_LOG_PAGE = 0x02,
+	/* 0x03 - reserved */
+	SPDK_CLIENT_OPC_DELETE_IO_CQ = 0x04,
+	SPDK_CLIENT_OPC_CREATE_IO_CQ = 0x05,
+	SPDK_CLIENT_OPC_IDENTIFY = 0x06,
+	/* 0x07 - reserved */
+	SPDK_CLIENT_OPC_ABORT = 0x08,
+	SPDK_CLIENT_OPC_SET_FEATURES = 0x09,
+	SPDK_CLIENT_OPC_GET_FEATURES = 0x0a,
+	/* 0x0b - reserved */
+	SPDK_CLIENT_OPC_ASYNC_EVENT_REQUEST = 0x0c,
+	SPDK_CLIENT_OPC_NS_MANAGEMENT = 0x0d,
+	/* 0x0e-0x0f - reserved */
+	SPDK_CLIENT_OPC_FIRMWARE_COMMIT = 0x10,
+	SPDK_CLIENT_OPC_FIRMWARE_IMAGE_DOWNLOAD = 0x11,
+
+	SPDK_CLIENT_OPC_DEVICE_SELF_TEST = 0x14,
+	SPDK_CLIENT_OPC_NS_ATTACHMENT = 0x15,
+
+	SPDK_CLIENT_OPC_KEEP_ALIVE = 0x18,
+	SPDK_CLIENT_OPC_DIRECTIVE_SEND = 0x19,
+	SPDK_CLIENT_OPC_DIRECTIVE_RECEIVE = 0x1a,
+
+	SPDK_CLIENT_OPC_VIRTUALIZATION_MANAGEMENT = 0x1c,
+	SPDK_CLIENT_OPC_CLIENT_MI_SEND = 0x1d,
+	SPDK_CLIENT_OPC_CLIENT_MI_RECEIVE = 0x1e,
+
+	SPDK_CLIENT_OPC_DOORBELL_BUFFER_CONFIG = 0x7c,
+
+	SPDK_CLIENT_OPC_FORMAT_NVM = 0x80,
+	SPDK_CLIENT_OPC_SECURITY_SEND = 0x81,
+	SPDK_CLIENT_OPC_SECURITY_RECEIVE = 0x82,
+
+	SPDK_CLIENT_OPC_SANITIZE = 0x84,
+
+	SPDK_CLIENT_OPC_GET_LBA_STATUS = 0x86,
+};
+
+struct spdk_client_ns_list
+{
+	uint32_t ns_list[1024];
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_client_ns_list) == 4096, "Incorrect size");
+
+/*
+ * One of these per allocated PCI device.
+ */
+struct spdk_client_ctrlr
+{
+	/* Hot data (accessed in I/O path) starts here. */
+
+	/* Tree of namespaces */
+	RB_HEAD(client_ns_tree, spdk_client_ns)
+	ns;
+
+	/* The number of active namespaces */
+	uint32_t active_ns_count;
+
+	bool is_removed;
+
+	bool is_resetting;
+
+	bool is_failed;
+
+	bool is_destructed;
+
+	bool timeout_enabled;
+
+	/* The application is preparing to reset the controller.  Transports
+	 * can use this to skip unnecessary parts of the qpair deletion process
+	 * for example, like the DELETE_SQ/CQ commands.
+	 */
+	bool prepare_for_reset;
+
+	uint16_t max_sges;
+
+	uint16_t cntlid;
+
+	/** Controller support flags */
+	uint64_t flags;
+
+	/** CLIENToF in-capsule data size in bytes */
+	uint32_t ioccsz_bytes;
+
+	/** CLIENToF in-capsule data offset in 16 byte units */
+	uint16_t icdoff;
+
+	char trstring[SPDK_SRV_TRSTRING_MAX_LEN + 1];
+	enum spdk_client_transport_type trtype;
+
+	int state;
+	uint64_t state_timeout_tsc;
+
+	uint64_t next_keep_alive_tick;
+	uint64_t keep_alive_interval_ticks;
+
+	TAILQ_ENTRY(spdk_client_ctrlr)
+	tailq;
+
+	/** maximum i/o size in bytes */
+	uint32_t max_xfer_size;
+
+	/** minimum page size supported by this controller in bytes */
+	uint32_t min_page_size;
+
+	/** selected memory page size for this controller in bytes */
+	uint32_t page_size;
+
+	/** guards access to the controller itself, including admin queues */
+	pthread_mutex_t ctrlr_lock;
+
+	struct spdk_client_qpair *adminq;
+
+	struct spdk_bit_array *free_io_qids;
+	TAILQ_HEAD(, spdk_client_qpair)
+	active_io_qpairs;
+	STAILQ_HEAD(, rpc_request)
+	pending_rpc_requests;
+
+	struct spdk_client_ctrlr_opts opts;
+
+	uint64_t quirks;
+
+	/* Extra sleep time during controller initialization */
+	uint64_t sleep_timeout_tsc;
+
+	/** Track all the processes manage this controller */
+	TAILQ_HEAD(, spdk_client_ctrlr_process)
+	active_procs;
+
+	STAILQ_HEAD(, client_request)
+	queued_aborts;
+	uint32_t outstanding_aborts;
+
+	/* CB to notify the user when the ctrlr is removed/failed. */
+	spdk_client_remove_cb remove_cb;
+	void *cb_ctx;
+
+	/* scratchpad pointer that can be used to send data between two CLIENT_CTRLR_STATEs */
+	void *tmp_ptr;
+
+	/* maximum zone append size in bytes */
+	uint32_t max_zone_append_size;
+
+	/* PMR size in bytes */
+	uint64_t pmr_size;
+
+	/* Boot Partition Info */
+	enum client_bp_write_state bp_ws;
+	uint32_t bpid;
+	spdk_req_cmd_cb bp_write_cb_fn;
+	void *bp_write_cb_arg;
+
+	/* Firmware Download */
+	void *fw_payload;
+	unsigned int fw_size_remaining;
+	unsigned int fw_offset;
+	unsigned int fw_transfer_size;
+
+	/* Completed register operations */
+	STAILQ_HEAD(, client_register_completion)
+	register_operations;
+
+	union spdk_client_cc_register process_init_cc;
+
+	struct spdk_mempool *rpc_data_mp;
+	uint32_t io_unit_size;
+};
+
+struct spdk_client_probe_ctx
+{
+	struct spdk_client_transport_id trid;
+	void *cb_ctx;
+	spdk_client_probe_cb probe_cb;
+	spdk_client_attach_cb attach_cb;
+	spdk_client_remove_cb remove_cb;
+	TAILQ_HEAD(, spdk_client_ctrlr)
+	init_ctrlrs;
+};
+
+typedef void (*client_ctrlr_detach_cb)(struct spdk_client_ctrlr *ctrlr);
+
+enum client_ctrlr_detach_state
+{
+	CLIENT_CTRLR_DETACH_SET_CC,
+	CLIENT_CTRLR_DETACH_CHECK_CSTS,
+	CLIENT_CTRLR_DETACH_GET_CSTS,
+	CLIENT_CTRLR_DETACH_GET_CSTS_DONE,
+};
+
+struct client_ctrlr_detach_ctx
+{
+	struct spdk_client_ctrlr *ctrlr;
+	client_ctrlr_detach_cb cb_fn;
+	uint64_t shutdown_start_tsc;
+	uint32_t shutdown_timeout_ms;
+	bool shutdown_complete;
+	enum client_ctrlr_detach_state state;
+	union spdk_client_csts_register csts;
+	TAILQ_ENTRY(client_ctrlr_detach_ctx)
+	link;
+};
+
+struct spdk_client_detach_ctx
+{
+	TAILQ_HEAD(, client_ctrlr_detach_ctx)
+	head;
+};
+
+struct spdk_client_ctrlr_reset_ctx
+{
+	struct spdk_client_ctrlr *ctrlr;
+};
+
+struct client_driver
+{
+	pthread_mutex_t lock;
+
+	/** Multi-process shared attached controller list */
+	TAILQ_HEAD(, spdk_client_ctrlr)
+	shared_attached_ctrlrs;
+
+	bool initialized;
+	struct spdk_uuid default_extended_host_id;
+
+	/** netlink socket fd for hotplug messages */
+	int hotplug_fd;
+};
+
+extern struct client_driver *g_spdk_client_driver;
+
+int client_driver_init(void);
+
+#define client_delay usleep
+
+static inline bool
+client_qpair_is_admin_queue(struct spdk_client_qpair *qpair)
+{
+	return qpair->id == 0;
+}
+
+static inline bool
+client_qpair_is_io_queue(struct spdk_client_qpair *qpair)
+{
+	return qpair->id != 0;
+}
+
+/**
+ * Extract the Data Transfer bits from an Client opcode.
+ *
+ * This determines whether a command requires a data buffer and
+ * which direction (host to controller or controller to host) it is
+ * transferred.
+ */
+static inline enum spdk_client_data_transfer spdk_client_opc_get_data_transfer(uint8_t opc)
+{
+	return (enum spdk_client_data_transfer)(opc & 3);
+}
+
+static inline int
+client_robust_mutex_lock(pthread_mutex_t *mtx)
+{
+	int rc = pthread_mutex_lock(mtx);
+
+#ifndef __FreeBSD__
+	if (rc == EOWNERDEAD)
+	{
+		rc = pthread_mutex_consistent(mtx);
+	}
+#endif
+
+	return rc;
+}
+
+static inline int
+client_robust_mutex_unlock(pthread_mutex_t *mtx)
+{
+	return pthread_mutex_unlock(mtx);
+}
+
+/* Poll group management functions. */
+int client_poll_group_connect_qpair(struct spdk_client_qpair *qpair);
+int client_poll_group_disconnect_qpair(struct spdk_client_qpair *qpair);
+
+/* Admin functions */
+int client_ctrlr_cmd_identify(struct spdk_client_ctrlr *ctrlr,
+							  uint8_t cns, uint16_t cntid, uint32_t nsid,
+							  uint8_t csi, void *payload, size_t payload_size,
+							  spdk_req_cmd_cb cb_fn, void *cb_arg);
+int client_ctrlr_cmd_set_num_queues(struct spdk_client_ctrlr *ctrlr,
+									uint32_t num_queues, spdk_req_cmd_cb cb_fn,
+									void *cb_arg);
+int client_ctrlr_cmd_get_num_queues(struct spdk_client_ctrlr *ctrlr,
+									spdk_req_cmd_cb cb_fn, void *cb_arg);
+
+int client_ctrlr_cmd_set_host_id(struct spdk_client_ctrlr *ctrlr, void *host_id, uint32_t host_id_size,
+								 spdk_req_cmd_cb cb_fn, void *cb_arg);
+
+int client_ctrlr_cmd_format(struct spdk_client_ctrlr *ctrlr, uint32_t nsid,
+							struct spdk_client_format *format, spdk_req_cmd_cb cb_fn, void *cb_arg);
+
+void client_completion_poll_cb(void *arg, const struct spdk_req_cpl *cpl);
+int client_wait_for_completion(struct spdk_client_qpair *qpair,
+							   struct client_completion_poll_status *status);
+int client_wait_for_completion_robust_lock(struct spdk_client_qpair *qpair,
+										   struct client_completion_poll_status *status,
+										   pthread_mutex_t *robust_mutex);
+int client_wait_for_completion_timeout(struct spdk_client_qpair *qpair,
+									   struct client_completion_poll_status *status,
+									   uint64_t timeout_in_usecs);
+int client_wait_for_completion_robust_lock_timeout(struct spdk_client_qpair *qpair,
+												   struct client_completion_poll_status *status,
+												   pthread_mutex_t *robust_mutex,
+												   uint64_t timeout_in_usecs);
+int client_wait_for_completion_robust_lock_timeout_poll(struct spdk_client_qpair *qpair,
+														struct client_completion_poll_status *status,
+														pthread_mutex_t *robust_mutex);
+
+struct spdk_client_ctrlr_process *client_ctrlr_get_process(struct spdk_client_ctrlr *ctrlr,
+														   pid_t pid);
+struct spdk_client_ctrlr_process *client_ctrlr_get_current_process(struct spdk_client_ctrlr *ctrlr);
+int client_ctrlr_add_process(struct spdk_client_ctrlr *ctrlr, void *devhandle);
+void client_ctrlr_free_processes(struct spdk_client_ctrlr *ctrlr);
+struct spdk_pci_device *client_ctrlr_proc_get_devhandle(struct spdk_client_ctrlr *ctrlr);
+
+int client_ctrlr_probe(const struct spdk_client_transport_id *trid,
+					   struct spdk_client_probe_ctx *probe_ctx, void *devhandle);
+
+int client_ctrlr_construct(struct spdk_client_ctrlr *ctrlr);
+void client_ctrlr_destruct_finish(struct spdk_client_ctrlr *ctrlr);
+void client_ctrlr_destruct(struct spdk_client_ctrlr *ctrlr);
+void client_ctrlr_destruct_async(struct spdk_client_ctrlr *ctrlr,
+								 struct client_ctrlr_detach_ctx *ctx);
+int client_ctrlr_destruct_poll_async(struct spdk_client_ctrlr *ctrlr,
+									 struct client_ctrlr_detach_ctx *ctx);
+void client_ctrlr_fail(struct spdk_client_ctrlr *ctrlr, bool hot_remove);
+int client_ctrlr_process_init(struct spdk_client_ctrlr *ctrlr);
+void client_ctrlr_connected(struct spdk_client_probe_ctx *probe_ctx,
+							struct spdk_client_ctrlr *ctrlr);
+
+int client_ctrlr_submit_admin_request(struct spdk_client_ctrlr *ctrlr,
+									  struct client_request *req);
+int client_ctrlr_get_cap(struct spdk_client_ctrlr *ctrlr, union spdk_client_cap_register *cap);
+int client_ctrlr_get_vs(struct spdk_client_ctrlr *ctrlr, union spdk_client_vs_register *vs);
+int client_ctrlr_get_cmbsz(struct spdk_client_ctrlr *ctrlr, union spdk_client_cmbsz_register *cmbsz);
+int client_ctrlr_get_pmrcap(struct spdk_client_ctrlr *ctrlr, union spdk_client_pmrcap_register *pmrcap);
+int client_ctrlr_get_bpinfo(struct spdk_client_ctrlr *ctrlr, union spdk_client_bpinfo_register *bpinfo);
+int client_ctrlr_set_bpmbl(struct spdk_client_ctrlr *ctrlr, uint64_t bpmbl_value);
+bool client_ctrlr_multi_iocs_enabled(struct spdk_client_ctrlr *ctrlr);
+void client_ctrlr_process_async_event(struct spdk_client_ctrlr *ctrlr,
+									  const struct spdk_req_cpl *cpl);
+void client_ctrlr_disconnect_qpair(struct spdk_client_qpair *qpair);
+void client_ctrlr_complete_queued_async_events(struct spdk_client_ctrlr *ctrlr);
+int client_qpair_init(struct spdk_client_qpair *qpair, uint16_t id,
+					  struct spdk_client_ctrlr *ctrlr,
+					  enum spdk_client_qprio qprio,
+					  uint32_t num_requests, bool async);
+void client_qpair_deinit(struct spdk_client_qpair *qpair);
+void client_qpair_complete_error_reqs(struct spdk_client_qpair *qpair);
+int client_qpair_submit_request(struct spdk_client_qpair *qpair,
+								struct client_request *req);
+void client_qpair_abort_all_queued_reqs(struct spdk_client_qpair *qpair, uint32_t dnr);
+uint32_t client_qpair_abort_queued_reqs_with_cbarg(struct spdk_client_qpair *qpair, void *cmd_cb_arg);
+void client_qpair_abort_queued_reqs(struct spdk_client_qpair *qpair, uint32_t dnr);
+void client_qpair_resubmit_requests(struct spdk_client_qpair *qpair, uint32_t num_requests);
+int client_ctrlr_identify_active_ns(struct spdk_client_ctrlr *ctrlr);
+int client_ctrlr_construct_namespace(struct spdk_client_ctrlr *ctrlr, uint32_t nsid);
+void client_ns_set_identify_data(struct spdk_client_ns *ns);
+void client_ns_set_id_desc_list_data(struct spdk_client_ns *ns);
+void client_ns_free_zns_specific_data(struct spdk_client_ns *ns);
+void client_ns_free_iocs_specific_data(struct spdk_client_ns *ns);
+bool client_ns_has_supported_iocs_specific_data(struct spdk_client_ns *ns);
+int client_ns_construct(struct spdk_client_ns *ns, uint32_t id,
+						struct spdk_client_ctrlr *ctrlr);
+int client_ns_cmd_zone_append_with_md(struct spdk_client_ns *ns, struct spdk_client_qpair *qpair,
+									  void *buffer, void *metadata, uint64_t zslba,
+									  uint32_t lba_count, spdk_req_cmd_cb cb_fn, void *cb_arg,
+									  uint32_t io_flags, uint16_t apptag_mask, uint16_t apptag);
+int client_ns_cmd_zone_appendv_with_md(struct spdk_client_ns *ns, struct spdk_client_qpair *qpair,
+									   uint64_t zslba, uint32_t lba_count,
+									   spdk_req_cmd_cb cb_fn, void *cb_arg, uint32_t io_flags,
+									   spdk_client_req_reset_sgl_cb reset_sgl_fn,
+									   spdk_client_req_next_sge_cb next_sge_fn, void *metadata,
+									   uint16_t apptag_mask, uint16_t apptag);
+
+int client_fabric_ctrlr_set_reg_4(struct spdk_client_ctrlr *ctrlr, uint32_t offset, uint32_t value);
+int client_fabric_ctrlr_set_reg_8(struct spdk_client_ctrlr *ctrlr, uint32_t offset, uint64_t value);
+int client_fabric_ctrlr_get_reg_4(struct spdk_client_ctrlr *ctrlr, uint32_t offset, uint32_t *value);
+int client_fabric_ctrlr_get_reg_8(struct spdk_client_ctrlr *ctrlr, uint32_t offset, uint64_t *value);
+int client_fabric_ctrlr_set_reg_4_async(struct spdk_client_ctrlr *ctrlr, uint32_t offset,
+										uint32_t value, spdk_client_reg_cb cb_fn, void *cb_arg);
+int client_fabric_ctrlr_set_reg_8_async(struct spdk_client_ctrlr *ctrlr, uint32_t offset,
+										uint64_t value, spdk_client_reg_cb cb_fn, void *cb_arg);
+int client_fabric_ctrlr_get_reg_4_async(struct spdk_client_ctrlr *ctrlr, uint32_t offset,
+										spdk_client_reg_cb cb_fn, void *cb_arg);
+int client_fabric_ctrlr_get_reg_8_async(struct spdk_client_ctrlr *ctrlr, uint32_t offset,
+										spdk_client_reg_cb cb_fn, void *cb_arg);
+int client_fabric_ctrlr_scan(struct spdk_client_probe_ctx *probe_ctx, bool direct_connect);
+int client_fabric_ctrlr_discover(struct spdk_client_ctrlr *ctrlr,
+								 struct spdk_client_probe_ctx *probe_ctx);
+int client_fabric_qpair_connect(struct spdk_client_qpair *qpair, uint32_t num_entries);
+int client_fabric_qpair_connect_async(struct spdk_client_qpair *qpair, uint32_t num_entries);
+int client_fabric_qpair_connect_poll(struct spdk_client_qpair *qpair);
+
+#define CLIENT_INIT_REQUEST(req, _cb_fn, _cb_arg, _payload, _payload_size, _md_size) \
+	do                                                                               \
+	{                                                                                \
+		req->cb_fn = _cb_fn;                                                         \
+		req->cb_arg = _cb_arg;                                                       \
+		req->payload = _payload;                                                     \
+		req->payload_size = _payload_size;                                           \
+		req->md_size = _md_size;                                                     \
+		req->pid = g_spdk_client_pid;                                                \
+		req->submit_tick = 0;                                                        \
+	} while (0);
+
+static inline struct client_request *
+client_allocate_request(struct spdk_client_qpair *qpair,
+						const struct client_payload *payload, uint32_t payload_size, uint32_t md_size,
+						spdk_req_cmd_cb cb_fn, void *cb_arg)
+{
+	struct client_request *req;
+
+	req = STAILQ_FIRST(&qpair->free_req);
+	if (req == NULL)
+	{
+		return req;
+	}
+
+	STAILQ_REMOVE_HEAD(&qpair->free_req, stailq);
+
+	/*
+	 * Only memset/zero fields that need it.  All other fields
+	 *  will be initialized appropriately either later in this
+	 *  function, or before they are needed later in the
+	 *  submission patch.  For example, the children
+	 *  TAILQ_ENTRY and following members are
+	 *  only used as part of I/O splitting so we avoid
+	 *  memsetting them until it is actually needed.
+	 *  They will be initialized in client_request_add_child()
+	 *  if the request is split.
+	 */
+	memset(req, 0, offsetof(struct client_request, payload_size));
+
+	CLIENT_INIT_REQUEST(req, cb_fn, cb_arg, *payload, payload_size, md_size);
+
+	return req;
+}
+
+static inline struct client_request *
+client_allocate_request_contig(struct spdk_client_qpair *qpair,
+							   void *buffer, uint32_t payload_size,
+							   spdk_req_cmd_cb cb_fn, void *cb_arg)
+{
+	struct client_payload payload;
+
+	payload = CLIENT_PAYLOAD_CONTIG(buffer, NULL);
+
+	return client_allocate_request(qpair, &payload, payload_size, 0, cb_fn, cb_arg);
+}
+
+static inline struct client_request *
+client_allocate_request_null(struct spdk_client_qpair *qpair, spdk_req_cmd_cb cb_fn, void *cb_arg)
+{
+	return client_allocate_request_contig(qpair, NULL, 0, cb_fn, cb_arg);
+}
+
+struct client_request *client_allocate_request_user_copy(struct spdk_client_qpair *qpair,
+														 void *buffer, uint32_t payload_size,
+														 spdk_req_cmd_cb cb_fn, void *cb_arg, bool host_to_controller);
+
+static inline void
+client_complete_request(spdk_req_cmd_cb cb_fn, void *cb_arg, struct spdk_client_qpair *qpair,
+						struct client_request *req, struct spdk_req_cpl *cpl)
+{
+	struct spdk_req_cpl err_cpl;
+	struct client_error_cmd *cmd;
+
+	/* error injection at completion path,
+	 * only inject for successful completed commands
+	 */
+	if (spdk_unlikely(!TAILQ_EMPTY(&qpair->err_cmd_head) &&
+					  !spdk_req_cpl_is_error(cpl)))
+	{
+		TAILQ_FOREACH(cmd, &qpair->err_cmd_head, link)
+		{
+
+			if (cmd->do_not_submit)
+			{
+				continue;
+			}
+
+			if ((cmd->opc == req->cmd.opc) && cmd->err_count)
+			{
+
+				err_cpl = *cpl;
+				err_cpl.status.sct = cmd->status.sct;
+				err_cpl.status.sc = cmd->status.sc;
+
+				cpl = &err_cpl;
+				cmd->err_count--;
+				break;
+			}
+		}
+	}
+
+	if (cb_fn)
+	{
+		cb_fn(cb_arg, cpl);
+	}
+}
+
+static inline void
+client_free_request(struct client_request *req)
+{
+	assert(req != NULL);
+	assert(req->num_children == 0);
+	assert(req->qpair != NULL);
+
+	/* The reserved_req does not go in the free_req STAILQ - it is
+	 * saved only for use with a FABRICS/CONNECT command.
+	 */
+	if (spdk_likely(req->qpair->reserved_req != req))
+	{
+		STAILQ_INSERT_HEAD(&req->qpair->free_req, req, stailq);
+	}
+}
+
+static inline void
+client_qpair_set_state(struct spdk_client_qpair *qpair, enum client_qpair_state state)
+{
+	qpair->state = state;
+	if (state == CLIENT_QPAIR_ENABLED)
+	{
+		qpair->is_new_qpair = false;
+	}
+}
+
+static inline enum client_qpair_state
+client_qpair_get_state(struct spdk_client_qpair *qpair)
+{
+	return qpair->state;
+}
+
+static inline void
+client_qpair_free_request(struct spdk_client_qpair *qpair, struct client_request *req)
+{
+	assert(req != NULL);
+	assert(req->num_children == 0);
+
+	STAILQ_INSERT_HEAD(&qpair->free_req, req, stailq);
+}
+
+static inline void
+client_request_remove_child(struct client_request *parent, struct client_request *child)
+{
+	assert(parent != NULL);
+	assert(child != NULL);
+	assert(child->parent == parent);
+	assert(parent->num_children != 0);
+
+	parent->num_children--;
+	child->parent = NULL;
+	TAILQ_REMOVE(&parent->children, child, child_tailq);
+}
+
+static inline void
+client_cb_complete_child(void *child_arg, const struct spdk_req_cpl *cpl)
+{
+	struct client_request *child = child_arg;
+	struct client_request *parent = child->parent;
+
+	client_request_remove_child(parent, child);
+
+	memcpy(&parent->parent_status, cpl, sizeof(*cpl));
+
+	if (parent->num_children == 0)
+	{
+		client_complete_request(parent->cb_fn, parent->cb_arg, parent->qpair,
+								parent, &parent->parent_status);
+		client_free_request(parent);
+	}
+}
+
+static inline void
+client_request_add_child(struct client_request *parent, struct client_request *child)
+{
+	assert(parent->num_children != UINT16_MAX);
+
+	if (parent->num_children == 0)
+	{
+		/*
+		 * Defer initialization of the children TAILQ since it falls
+		 *  on a separate cacheline.  This ensures we do not touch this
+		 *  cacheline except on request splitting cases, which are
+		 *  relatively rare.
+		 */
+		TAILQ_INIT(&parent->children);
+		parent->parent = NULL;
+		memset(&parent->parent_status, 0, sizeof(struct spdk_req_cpl));
+	}
+
+	parent->num_children++;
+	TAILQ_INSERT_TAIL(&parent->children, child, child_tailq);
+	child->parent = parent;
+	child->cb_fn = client_cb_complete_child;
+	child->cb_arg = child;
+}
+
+static inline void
+client_request_free_children(struct client_request *req)
+{
+	struct client_request *child, *tmp;
+
+	if (req->num_children == 0)
+	{
+		return;
+	}
+
+	/* free all child client_request */
+	TAILQ_FOREACH_SAFE(child, &req->children, child_tailq, tmp)
+	{
+		client_request_remove_child(req, child);
+		client_request_free_children(child);
+		client_free_request(child);
+	}
+}
+
+int client_request_check_timeout(struct client_request *req, uint16_t cid,
+								 struct spdk_client_ctrlr_process *active_proc, uint64_t now_tick);
+uint64_t client_get_quirks(const struct spdk_pci_id *id);
+
+int client_robust_mutex_init_shared(pthread_mutex_t *mtx);
+int client_robust_mutex_init_recursive_shared(pthread_mutex_t *mtx);
+
+bool client_completion_is_retry(const struct spdk_req_cpl *cpl);
+
+struct spdk_client_ctrlr *client_get_ctrlr_by_trid_unsafe(
+	const struct spdk_client_transport_id *trid);
+
+const struct spdk_client_transport *client_get_transport(const char *transport_name);
+const struct spdk_client_transport *client_get_first_transport(void);
+const struct spdk_client_transport *client_get_next_transport(const struct spdk_client_transport
+																  *transport);
+void client_ctrlr_update_namespaces(struct spdk_client_ctrlr *ctrlr);
+
+/* Transport specific functions */
+struct spdk_client_ctrlr *client_transport_ctrlr_construct(const char *trstring,
+														   const struct spdk_client_ctrlr_opts *opts,
+														   void *devhandle);
+int client_transport_ctrlr_destruct(struct spdk_client_ctrlr *ctrlr);
+int client_transport_ctrlr_scan(struct spdk_client_probe_ctx *probe_ctx, bool direct_connect);
+int client_transport_ctrlr_enable(struct spdk_client_ctrlr *ctrlr);
+int client_transport_ctrlr_set_reg_4(struct spdk_client_ctrlr *ctrlr, uint32_t offset, uint32_t value);
+int client_transport_ctrlr_set_reg_8(struct spdk_client_ctrlr *ctrlr, uint32_t offset, uint64_t value);
+int client_transport_ctrlr_get_reg_4(struct spdk_client_ctrlr *ctrlr, uint32_t offset, uint32_t *value);
+int client_transport_ctrlr_get_reg_8(struct spdk_client_ctrlr *ctrlr, uint32_t offset, uint64_t *value);
+int client_transport_ctrlr_set_reg_4_async(struct spdk_client_ctrlr *ctrlr, uint32_t offset,
+										   uint32_t value, spdk_client_reg_cb cb_fn, void *cb_arg);
+int client_transport_ctrlr_set_reg_8_async(struct spdk_client_ctrlr *ctrlr, uint32_t offset,
+										   uint64_t value, spdk_client_reg_cb cb_fn, void *cb_arg);
+int client_transport_ctrlr_get_reg_4_async(struct spdk_client_ctrlr *ctrlr, uint32_t offset,
+										   spdk_client_reg_cb cb_fn, void *cb_arg);
+int client_transport_ctrlr_get_reg_8_async(struct spdk_client_ctrlr *ctrlr, uint32_t offset,
+										   spdk_client_reg_cb cb_fn, void *cb_arg);
+uint32_t client_transport_ctrlr_get_max_xfer_size(struct spdk_client_ctrlr *ctrlr);
+uint16_t client_transport_ctrlr_get_max_sges(struct spdk_client_ctrlr *ctrlr);
+struct spdk_client_qpair *client_transport_ctrlr_create_io_qpair(struct spdk_client_ctrlr *ctrlr,
+																 uint16_t qid, const struct spdk_client_io_qpair_opts *opts);
+int client_transport_ctrlr_reserve_cmb(struct spdk_client_ctrlr *ctrlr);
+void *client_transport_ctrlr_map_cmb(struct spdk_client_ctrlr *ctrlr, size_t *size);
+int client_transport_ctrlr_unmap_cmb(struct spdk_client_ctrlr *ctrlr);
+int client_transport_ctrlr_enable_pmr(struct spdk_client_ctrlr *ctrlr);
+int client_transport_ctrlr_disable_pmr(struct spdk_client_ctrlr *ctrlr);
+void *client_transport_ctrlr_map_pmr(struct spdk_client_ctrlr *ctrlr, size_t *size);
+int client_transport_ctrlr_unmap_pmr(struct spdk_client_ctrlr *ctrlr);
+void client_transport_ctrlr_delete_io_qpair(struct spdk_client_ctrlr *ctrlr,
+											struct spdk_client_qpair *qpair);
+int client_transport_ctrlr_connect_qpair(struct spdk_client_ctrlr *ctrlr,
+										 struct spdk_client_qpair *qpair);
+int client_transport_ctrlr_connect_qpair_async(struct spdk_client_ctrlr *ctrlr,
+											   struct spdk_client_qpair *qpair);
+void client_transport_ctrlr_disconnect_qpair(struct spdk_client_ctrlr *ctrlr,
+											 struct spdk_client_qpair *qpair);
+int client_transport_ctrlr_get_memory_domains(const struct spdk_client_ctrlr *ctrlr,
+											  struct spdk_memory_domain **domains, int array_size);
+void client_transport_qpair_abort_reqs(struct spdk_client_qpair *qpair, uint32_t dnr);
+int client_transport_qpair_reset(struct spdk_client_qpair *qpair);
+int client_transport_qpair_submit_request(struct spdk_client_qpair *qpair, struct client_request *req);
+int32_t client_transport_qpair_process_completions(struct spdk_client_qpair *qpair,
+												   uint32_t max_completions);
+void client_transport_admin_qpair_abort_aers(struct spdk_client_qpair *qpair);
+int client_transport_qpair_iterate_requests(struct spdk_client_qpair *qpair,
+											int (*iter_fn)(struct client_request *req, void *arg),
+											void *arg);
+
+struct spdk_client_transport_poll_group *client_transport_poll_group_create(
+	const struct spdk_client_transport *transport);
+struct spdk_client_transport_poll_group *client_transport_qpair_get_optimal_poll_group(
+	const struct spdk_client_transport *transport,
+	struct spdk_client_qpair *qpair);
+int client_transport_poll_group_add(struct spdk_client_transport_poll_group *tgroup,
+									struct spdk_client_qpair *qpair);
+int client_transport_poll_group_remove(struct spdk_client_transport_poll_group *tgroup,
+									   struct spdk_client_qpair *qpair);
+int client_transport_poll_group_disconnect_qpair(struct spdk_client_qpair *qpair);
+int client_transport_poll_group_connect_qpair(struct spdk_client_qpair *qpair);
+int64_t client_transport_poll_group_process_completions(struct spdk_client_transport_poll_group *tgroup,
+														uint32_t completions_per_qpair, spdk_client_disconnected_qpair_cb disconnected_qpair_cb);
+int client_transport_poll_group_destroy(struct spdk_client_transport_poll_group *tgroup);
+int client_transport_poll_group_get_stats(struct spdk_client_transport_poll_group *tgroup,
+										  struct spdk_client_transport_poll_group_stat **stats);
+void client_transport_poll_group_free_stats(struct spdk_client_transport_poll_group *tgroup,
+											struct spdk_client_transport_poll_group_stat *stats);
+enum spdk_client_transport_type client_transport_get_trtype(const struct spdk_client_transport
+																*transport);
+/*
+ * Below ref related functions must be called with the global
+ *  driver lock held for the multi-process condition.
+ *  Within these functions, the per ctrlr ctrlr_lock is also
+ *  acquired for the multi-thread condition.
+ */
+void client_ctrlr_proc_get_ref(struct spdk_client_ctrlr *ctrlr);
+void client_ctrlr_proc_put_ref(struct spdk_client_ctrlr *ctrlr);
+int client_ctrlr_get_ref_count(struct spdk_client_ctrlr *ctrlr);
+
+static inline bool
+_is_page_aligned(uint64_t address, uint64_t page_size)
+{
+	return (address & (page_size - 1)) == 0;
+}
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif /* __CLIENT_INTERNAL_H__ */
diff --git a/include/spdk_internal/rdma_server.h b/include/spdk_internal/rdma_server.h
new file mode 100644
index 000000000..3413a7992
--- /dev/null
+++ b/include/spdk_internal/rdma_server.h
@@ -0,0 +1,51 @@
+#ifndef SPDK_SRV_INTERNAL_H
+#define SPDK_SRV_INTERNAL_H
+
+#ifdef __cplusplus
+extern "C"
+{
+#endif
+
+#include "spdk/bdev.h"
+#include "spdk/memory.h"
+#include "spdk/likely.h"
+#include "spdk/rdma_server.h"
+
+int srv_transport_poll_group_add(struct spdk_srv_transport_poll_group *group,
+								 struct spdk_srv_conn *conn);
+
+int srv_transport_poll_group_poll(struct spdk_srv_transport_poll_group *group);
+
+struct spdk_srv_transport_poll_group *
+srv_transport_poll_group_create(struct spdk_srv_transport *transport);
+
+void srv_transport_poll_group_destroy(struct spdk_srv_transport_poll_group *group);
+
+struct spdk_srv_listener *
+srv_transport_find_listener(struct spdk_srv_transport *transport,
+							const struct spdk_srv_transport_id *trid);
+
+void srv_conn_set_state(struct spdk_srv_conn *conn,
+						enum spdk_srv_conn_state state);
+
+struct spdk_srv_transport_poll_group *
+srv_transport_get_optimal_poll_group(struct spdk_srv_transport *transport,
+									 struct spdk_srv_conn *conn);
+
+int srv_transport_poll_group_remove(struct spdk_srv_transport_poll_group *group,
+									struct spdk_srv_conn *conn);
+
+int srv_transport_conn_get_peer_trid(struct spdk_srv_conn *conn,
+									 struct spdk_srv_transport_id *trid);
+
+int srv_transport_conn_get_listen_trid(struct spdk_srv_conn *conn,
+									   struct spdk_srv_transport_id *trid);
+
+int srv_transport_conn_get_local_trid(struct spdk_srv_conn *conn,
+									  struct spdk_srv_transport_id *trid);
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif
\ No newline at end of file
diff --git a/lib/Makefile b/lib/Makefile
index 197fc63a6..792e18459 100644
--- a/lib/Makefile
+++ b/lib/Makefile
@@ -47,7 +47,7 @@ DIRS-$(CONFIG_IDXD) += idxd
 DIRS-$(CONFIG_VHOST) += vhost
 DIRS-$(CONFIG_VIRTIO) += virtio
 DIRS-$(CONFIG_REDUCE) += reduce
-DIRS-$(CONFIG_RDMA) += rdma
+DIRS-$(CONFIG_RDMA) += rdma rdma_server
 DIRS-$(CONFIG_VFIO_USER) += vfio_user
 
 # If CONFIG_ENV is pointing at a directory in lib, build it.
diff --git a/lib/rdma_server/Makefile b/lib/rdma_server/Makefile
new file mode 100644
index 000000000..b95df36bd
--- /dev/null
+++ b/lib/rdma_server/Makefile
@@ -0,0 +1,62 @@
+#
+#  BSD LICENSE
+#
+#  Copyright (c) Intel Corporation.
+#  All rights reserved.
+#
+#  Redistribution and use in source and binary forms, with or without
+#  modification, are permitted provided that the following conditions
+#  are met:
+#
+#    * Redistributions of source code must retain the above copyright
+#      notice, this list of conditions and the following disclaimer.
+#    * Redistributions in binary form must reproduce the above copyright
+#      notice, this list of conditions and the following disclaimer in
+#      the documentation and/or other materials provided with the
+#      distribution.
+#    * Neither the name of Intel Corporation nor the names of its
+#      contributors may be used to endorse or promote products derived
+#      from this software without specific prior written permission.
+#
+#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+#  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+#  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+#  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+#  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+#  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+#  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+#  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+#  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+#  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+#
+
+SPDK_ROOT_DIR := $(abspath $(CURDIR)/../..)
+include $(SPDK_ROOT_DIR)/mk/spdk.common.mk
+
+SO_VER := 1
+SO_MINOR := 1
+
+C_SRCS = rdma_s.c server.c transport_s.c client.c cmd.c conn.c rdma_c.c transport_c.c utils.c
+
+LIBNAME = rdma_server
+LOCAL_SYS_LIBS += -libverbs -lrdmacm
+#Attach only if FreeBSD and RDMA is specified with configure
+ifeq ($(OS),FreeBSD)
+# Mellanox - MLX4 HBA Userspace Library
+ifneq ("$(wildcard /usr/lib/libmlx4.*)","")
+LOCAL_SYS_LIBS += -lmlx4
+endif
+# Mellanox - MLX5 HBA Userspace Library
+ifneq ("$(wildcard /usr/lib/libmlx5.*)","")
+LOCAL_SYS_LIBS += -lmlx5
+endif
+# Chelsio HBA Userspace Library
+ifneq ("$(wildcard /usr/lib/libcxgb4.*)","")
+LOCAL_SYS_LIBS += -lcxgb4
+endif
+endif
+
+SPDK_MAP_FILE = $(abspath $(CURDIR)/spdk_rdma_server.map)
+
+include $(SPDK_ROOT_DIR)/mk/spdk.lib.mk
\ No newline at end of file
diff --git a/lib/rdma_server/client.c b/lib/rdma_server/client.c
new file mode 100644
index 000000000..668deebe5
--- /dev/null
+++ b/lib/rdma_server/client.c
@@ -0,0 +1,2296 @@
+/*-
+ *   BSD LICENSE
+ *
+ *   Copyright (c) Intel Corporation. All rights reserved.
+ *   Copyright (c) 2019-2021 Mellanox Technologies LTD. All rights reserved.
+ *   Copyright (c) 2021, 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ *
+ *   Redistribution and use in source and binary forms, with or without
+ *   modification, are permitted provided that the following conditions
+ *   are met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in
+ *       the documentation and/or other materials provided with the
+ *       distribution.
+ *     * Neither the name of Intel Corporation nor the names of its
+ *       contributors may be used to endorse or promote products derived
+ *       from this software without specific prior written permission.
+ *
+ *   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ *   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ *   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ *   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ *   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ *   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ *   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ *   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ *   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ *   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ *   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "spdk/stdinc.h"
+
+#include "spdk_internal/rdma_client.h"
+
+#include "spdk/env.h"
+#include "spdk/string.h"
+#include "spdk/endian.h"
+#include "spdk/rdma_common.h"
+
+struct client_active_ns_ctx;
+pid_t g_spdk_client_pid;
+static void client_ctrlr_init_cap(struct spdk_client_ctrlr *ctrlr);
+static void client_ctrlr_set_state(struct spdk_client_ctrlr *ctrlr, enum client_ctrlr_state state,
+								   uint64_t timeout_in_ms);
+
+static int
+client_ns_cmp(struct spdk_client_ns *ns1, struct spdk_client_ns *ns2)
+{
+	if (ns1->id < ns2->id)
+	{
+		return -1;
+	}
+	else if (ns1->id > ns2->id)
+	{
+		return 1;
+	}
+	else
+	{
+		return 0;
+	}
+}
+
+RB_GENERATE_STATIC(client_ns_tree, spdk_client_ns, node, client_ns_cmp);
+
+#define CTRLR_STRING(ctrlr) \
+	("")
+
+#define CLIENT_CTRLR_ERRLOG(ctrlr, format, ...) \
+	SPDK_ERRLOG("[%s] " format, CTRLR_STRING(ctrlr), ##__VA_ARGS__);
+
+#define CLIENT_CTRLR_WARNLOG(ctrlr, format, ...) \
+	SPDK_WARNLOG("[%s] " format, CTRLR_STRING(ctrlr), ##__VA_ARGS__);
+
+#define CLIENT_CTRLR_NOTICELOG(ctrlr, format, ...) \
+	SPDK_NOTICELOG("[%s] " format, CTRLR_STRING(ctrlr), ##__VA_ARGS__);
+
+#define CLIENT_CTRLR_INFOLOG(ctrlr, format, ...) \
+	SPDK_INFOLOG(client, "[%s] " format, CTRLR_STRING(ctrlr), ##__VA_ARGS__);
+
+#ifdef DEBUG
+#define CLIENT_CTRLR_DEBUGLOG(ctrlr, format, ...) \
+	SPDK_DEBUGLOG(client, "[%s] " format, CTRLR_STRING(ctrlr), ##__VA_ARGS__);
+#else
+#define CLIENT_CTRLR_DEBUGLOG(ctrlr, ...) \
+	do                                    \
+	{                                     \
+	} while (0)
+#endif
+
+/* When the field in spdk_client_ctrlr_opts are changed and you change this function, please
+ * also update the client_ctrl_opts_init function in client_ctrlr.c
+ */
+void spdk_client_ctrlr_get_default_ctrlr_opts(struct spdk_client_ctrlr_opts *opts, size_t opts_size)
+{
+	char host_id_str[SPDK_UUID_STRING_LEN];
+
+	assert(opts);
+
+	opts->opts_size = opts_size;
+
+#define FIELD_OK(field) \
+	offsetof(struct spdk_client_ctrlr_opts, field) + sizeof(opts->field) <= opts_size
+
+#define SET_FIELD(field, value)                                                            \
+	if (offsetof(struct spdk_client_ctrlr_opts, field) + sizeof(opts->field) <= opts_size) \
+	{                                                                                      \
+		opts->field = value;                                                               \
+	}
+
+	SET_FIELD(num_io_queues, DEFAULT_MAX_IO_QUEUES);
+	SET_FIELD(use_cmb_sqs, false);
+	SET_FIELD(no_shn_notification, false);
+	SET_FIELD(arbitration_burst, 0);
+	SET_FIELD(low_priority_weight, 0);
+	SET_FIELD(medium_priority_weight, 0);
+	SET_FIELD(high_priority_weight, 0);
+	SET_FIELD(keep_alive_timeout_ms, MIN_KEEP_ALIVE_TIMEOUT_IN_MS);
+	SET_FIELD(transport_retry_count, SPDK_CLIENT_DEFAULT_RETRY_COUNT);
+	SET_FIELD(io_queue_size, DEFAULT_IO_QUEUE_SIZE);
+
+	g_spdk_client_pid = getpid();
+
+	SET_FIELD(io_queue_requests, DEFAULT_IO_QUEUE_REQUESTS);
+
+	if (FIELD_OK(src_addr))
+	{
+		memset(opts->src_addr, 0, sizeof(opts->src_addr));
+	}
+
+	if (FIELD_OK(src_svcid))
+	{
+		memset(opts->src_svcid, 0, sizeof(opts->src_svcid));
+	}
+
+	if (FIELD_OK(host_id))
+	{
+		memset(opts->host_id, 0, sizeof(opts->host_id));
+	}
+
+	SET_FIELD(command_set, CHAR_BIT);
+	SET_FIELD(admin_timeout_ms, CLIENT_MAX_ADMIN_TIMEOUT_IN_SECS * 1000);
+	SET_FIELD(header_digest, false);
+	SET_FIELD(data_digest, false);
+	SET_FIELD(disable_error_logging, false);
+	SET_FIELD(transport_ack_timeout, SPDK_CLIENT_DEFAULT_TRANSPORT_ACK_TIMEOUT);
+	SET_FIELD(admin_queue_size, DEFAULT_ADMIN_QUEUE_SIZE);
+	SET_FIELD(fabrics_connect_timeout_us, CLIENT_FABRIC_CONNECT_COMMAND_TIMEOUT);
+	SET_FIELD(disable_read_ana_log_page, false);
+	SET_FIELD(sector_size, DEFAULT_SECTOR_SIZE);
+	SET_FIELD(extended_lba_size, DEFAULT_EXTENDED_LBA_SIZE);
+	SET_FIELD(md_size, DEFAULT_MD_SIZE);
+	SET_FIELD(sectors_per_max_io, DEFAULT_SECTORS_PER_MAX_IO);
+	SET_FIELD(sectors_per_stripe, DEFAULT_SECTORS_PER_STRIPE);
+
+#undef FIELD_OK
+#undef SET_FIELD
+}
+
+struct spdk_client_ctrlr *spdk_client_transport_ctrlr_construct(const char *trstring,
+																const struct spdk_client_ctrlr_opts *opts,
+																void *devhandle)
+{
+	return client_transport_ctrlr_construct(trstring, opts, devhandle);
+}
+
+const struct spdk_client_ctrlr_opts *
+spdk_client_ctrlr_get_opts(struct spdk_client_ctrlr *ctrlr)
+{
+	return &ctrlr->opts;
+}
+
+/**
+ * This function will be called when the process allocates the IO qpair.
+ * Note: the ctrlr_lock must be held when calling this function.
+ */
+static void
+client_ctrlr_proc_add_io_qpair(struct spdk_client_qpair *qpair)
+{
+	struct spdk_client_ctrlr_process *active_proc;
+	struct spdk_client_ctrlr *ctrlr = qpair->ctrlr;
+
+	active_proc = client_ctrlr_get_current_process(ctrlr);
+	if (active_proc)
+	{
+		TAILQ_INSERT_TAIL(&active_proc->allocated_io_qpairs, qpair, per_process_tailq);
+		qpair->active_proc = active_proc;
+	}
+}
+
+/**
+ * This function will be called when the process frees the IO qpair.
+ * Note: the ctrlr_lock must be held when calling this function.
+ */
+static void
+client_ctrlr_proc_remove_io_qpair(struct spdk_client_qpair *qpair)
+{
+	struct spdk_client_ctrlr_process *active_proc;
+	struct spdk_client_ctrlr *ctrlr = qpair->ctrlr;
+	struct spdk_client_qpair *active_qpair, *tmp_qpair;
+
+	active_proc = client_ctrlr_get_current_process(ctrlr);
+	if (!active_proc)
+	{
+		return;
+	}
+
+	TAILQ_FOREACH_SAFE(active_qpair, &active_proc->allocated_io_qpairs,
+					   per_process_tailq, tmp_qpair)
+	{
+		if (active_qpair == qpair)
+		{
+			TAILQ_REMOVE(&active_proc->allocated_io_qpairs,
+						 active_qpair, per_process_tailq);
+
+			break;
+		}
+	}
+}
+
+void spdk_client_ctrlr_get_default_io_qpair_opts(struct spdk_client_ctrlr *ctrlr,
+												 struct spdk_client_io_qpair_opts *opts,
+												 size_t opts_size)
+{
+	assert(ctrlr);
+
+	assert(opts);
+
+	memset(opts, 0, opts_size);
+
+#define FIELD_OK(field) \
+	offsetof(struct spdk_client_io_qpair_opts, field) + sizeof(opts->field) <= opts_size
+
+	if (FIELD_OK(io_queue_size))
+	{
+		opts->io_queue_size = ctrlr->opts.io_queue_size;
+	}
+
+	if (FIELD_OK(io_queue_requests))
+	{
+		opts->io_queue_requests = ctrlr->opts.io_queue_requests;
+	}
+
+	if (FIELD_OK(delay_cmd_submit))
+	{
+		opts->delay_cmd_submit = false;
+	}
+
+	if (FIELD_OK(sq.vaddr))
+	{
+		opts->sq.vaddr = NULL;
+	}
+
+	if (FIELD_OK(sq.paddr))
+	{
+		opts->sq.paddr = 0;
+	}
+
+	if (FIELD_OK(sq.buffer_size))
+	{
+		opts->sq.buffer_size = 0;
+	}
+
+	if (FIELD_OK(cq.vaddr))
+	{
+		opts->cq.vaddr = NULL;
+	}
+
+	if (FIELD_OK(cq.paddr))
+	{
+		opts->cq.paddr = 0;
+	}
+
+	if (FIELD_OK(cq.buffer_size))
+	{
+		opts->cq.buffer_size = 0;
+	}
+
+	if (FIELD_OK(create_only))
+	{
+		opts->create_only = false;
+	}
+
+	if (FIELD_OK(async_mode))
+	{
+		opts->async_mode = false;
+	}
+
+#undef FIELD_OK
+}
+
+static struct spdk_client_qpair *
+client_ctrlr_create_io_qpair(struct spdk_client_ctrlr *ctrlr,
+							 const struct spdk_client_io_qpair_opts *opts)
+{
+	int32_t qid;
+	struct spdk_client_qpair *qpair;
+
+	if (!ctrlr)
+	{
+		return NULL;
+	}
+
+	client_robust_mutex_lock(&ctrlr->ctrlr_lock);
+
+	qid = spdk_client_ctrlr_alloc_qid(ctrlr);
+	if (qid < 0)
+	{
+		client_robust_mutex_unlock(&ctrlr->ctrlr_lock);
+		return NULL;
+	}
+
+	qpair = client_transport_ctrlr_create_io_qpair(ctrlr, qid, opts);
+	if (qpair == NULL)
+	{
+		CLIENT_CTRLR_ERRLOG(ctrlr, "client_transport_ctrlr_create_io_qpair() failed\n");
+		spdk_client_ctrlr_free_qid(ctrlr, qid);
+		client_robust_mutex_unlock(&ctrlr->ctrlr_lock);
+		return NULL;
+	}
+
+	TAILQ_INSERT_TAIL(&ctrlr->active_io_qpairs, qpair, tailq);
+
+	client_ctrlr_proc_add_io_qpair(qpair);
+
+	client_robust_mutex_unlock(&ctrlr->ctrlr_lock);
+
+	return qpair;
+}
+
+int spdk_client_ctrlr_connect_io_qpair(struct spdk_client_ctrlr *ctrlr, struct spdk_client_qpair *qpair)
+{
+	int rc;
+
+	if (client_qpair_get_state(qpair) != CLIENT_QPAIR_DISCONNECTED)
+	{
+		return -EISCONN;
+	}
+
+	client_robust_mutex_lock(&ctrlr->ctrlr_lock);
+	rc = client_transport_ctrlr_connect_qpair(ctrlr, qpair);
+	client_robust_mutex_unlock(&ctrlr->ctrlr_lock);
+
+	if (ctrlr->quirks & CLIENT_QUIRK_DELAY_AFTER_QUEUE_ALLOC)
+	{
+		spdk_delay_us(100);
+	}
+
+	return rc;
+}
+
+int spdk_client_ctrlr_connect_io_qpair_async(struct spdk_client_ctrlr *ctrlr, struct spdk_client_qpair *qpair)
+{
+	int rc;
+
+	if (client_qpair_get_state(qpair) != CLIENT_QPAIR_DISCONNECTED)
+	{
+		return -EISCONN;
+	}
+
+	client_robust_mutex_lock(&ctrlr->ctrlr_lock);
+	rc = client_transport_ctrlr_connect_qpair_async(ctrlr, qpair);
+	client_robust_mutex_unlock(&ctrlr->ctrlr_lock);
+
+	if (ctrlr->quirks & CLIENT_QUIRK_DELAY_AFTER_QUEUE_ALLOC)
+	{
+		spdk_delay_us(100);
+	}
+
+	return rc;
+}
+
+void spdk_client_ctrlr_disconnect_io_qpair(struct spdk_client_qpair *qpair)
+{
+	struct spdk_client_ctrlr *ctrlr = qpair->ctrlr;
+
+	client_robust_mutex_lock(&ctrlr->ctrlr_lock);
+	client_transport_ctrlr_disconnect_qpair(ctrlr, qpair);
+	client_robust_mutex_unlock(&ctrlr->ctrlr_lock);
+}
+
+bool spdk_client_ctrlr_has_free_memory(struct spdk_client_qpair *qpair, size_t size) {
+    size_t pool_size = spdk_mempool_count(qpair->ctrlr->rpc_data_mp);
+    return size <= pool_size * qpair->ctrlr->io_unit_size;
+}
+
+struct spdk_client_qpair *
+spdk_client_ctrlr_alloc_io_qpair(struct spdk_client_ctrlr *ctrlr,
+								 const struct spdk_client_io_qpair_opts *user_opts,
+								 size_t opts_size, struct spdk_client_transport_id *id, struct spdk_client_poll_group *client_pg)
+{
+
+	struct spdk_client_qpair *qpair;
+	struct spdk_client_io_qpair_opts opts;
+	int rc;
+
+	if (spdk_unlikely(ctrlr->state != CLIENT_CTRLR_STATE_READY))
+	{
+		/* When controller is resetting or initializing, free_io_qids is deleted or not created yet.
+		 * We can't create IO qpair in that case */
+		return NULL;
+	}
+
+	/*
+	 * Get the default options, then overwrite them with the user-provided options
+	 * up to opts_size.
+	 *
+	 * This allows for extensions of the opts structure without breaking
+	 * ABI compatibility.
+	 */
+	spdk_client_ctrlr_get_default_io_qpair_opts(ctrlr, &opts, sizeof(opts));
+	if (user_opts)
+	{
+		memcpy(&opts, user_opts, spdk_min(sizeof(opts), opts_size));
+
+		/* If user passes buffers, make sure they're big enough for the requested queue size */
+		if (opts.sq.vaddr)
+		{
+			if (opts.sq.buffer_size < (opts.io_queue_size * sizeof(struct spdk_req_cmd)))
+			{
+				CLIENT_CTRLR_ERRLOG(ctrlr, "sq buffer size %" PRIx64 " is too small for sq size %zx\n",
+									opts.sq.buffer_size, (opts.io_queue_size * sizeof(struct spdk_req_cmd)));
+				return NULL;
+			}
+		}
+		if (opts.cq.vaddr)
+		{
+			if (opts.cq.buffer_size < (opts.io_queue_size * sizeof(struct spdk_req_cpl)))
+			{
+				CLIENT_CTRLR_ERRLOG(ctrlr, "cq buffer size %" PRIx64 " is too small for cq size %zx\n",
+									opts.cq.buffer_size, (opts.io_queue_size * sizeof(struct spdk_req_cpl)));
+				return NULL;
+			}
+		}
+	}
+	SPDK_NOTICELOG("spdk_client_ctrlr_alloc_io_qpair : io_queue_size %d\n", opts.io_queue_size);
+	qpair = client_ctrlr_create_io_qpair(ctrlr, &opts);
+
+	if (qpair == NULL || opts.create_only == true)
+	{
+		return qpair;
+	}
+	// save id in qpair
+	qpair->trid = id;
+
+	rc = spdk_client_poll_group_add(client_pg, qpair);
+	if (rc != 0)
+	{
+		CLIENT_CTRLR_ERRLOG(ctrlr, "spdk_client_poll_group_add() failed\n");
+		goto err;
+	}
+	rc = spdk_client_ctrlr_connect_io_qpair(ctrlr, qpair);
+	if (rc != 0)
+	{
+		CLIENT_CTRLR_ERRLOG(ctrlr, "spdk_client_ctrlr_connect_io_qpair() failed\n");
+		goto err;
+	}
+	return qpair;
+err:
+	client_robust_mutex_lock(&ctrlr->ctrlr_lock);
+	client_ctrlr_proc_remove_io_qpair(qpair);
+	TAILQ_REMOVE(&ctrlr->active_io_qpairs, qpair, tailq);
+	spdk_bit_array_set(ctrlr->free_io_qids, qpair->id);
+	client_transport_ctrlr_delete_io_qpair(ctrlr, qpair);
+	client_robust_mutex_unlock(&ctrlr->ctrlr_lock);
+	return NULL;
+}
+
+struct spdk_client_qpair *
+spdk_client_ctrlr_alloc_io_qpair_async(struct spdk_client_ctrlr *ctrlr,
+									   const struct spdk_client_io_qpair_opts *user_opts,
+									   size_t opts_size, struct spdk_client_transport_id *id, struct spdk_client_poll_group *client_pg, spdk_connected_cb cb_fn, void *cb_arg)
+{
+
+	struct spdk_client_qpair *qpair;
+	struct spdk_client_io_qpair_opts opts;
+	int rc;
+
+	if (spdk_unlikely(ctrlr->state != CLIENT_CTRLR_STATE_READY))
+	{
+		/* When controller is resetting or initializing, free_io_qids is deleted or not created yet.
+		 * We can't create IO qpair in that case */
+		return NULL;
+	}
+
+	/*
+	 * Get the default options, then overwrite them with the user-provided options
+	 * up to opts_size.
+	 *
+	 * This allows for extensions of the opts structure without breaking
+	 * ABI compatibility.
+	 */
+	spdk_client_ctrlr_get_default_io_qpair_opts(ctrlr, &opts, sizeof(opts));
+	if (user_opts)
+	{
+		memcpy(&opts, user_opts, spdk_min(sizeof(opts), opts_size));
+
+		/* If user passes buffers, make sure they're big enough for the requested queue size */
+		if (opts.sq.vaddr)
+		{
+			if (opts.sq.buffer_size < (opts.io_queue_size * sizeof(struct spdk_req_cmd)))
+			{
+				CLIENT_CTRLR_ERRLOG(ctrlr, "sq buffer size %" PRIx64 " is too small for sq size %zx\n",
+									opts.sq.buffer_size, (opts.io_queue_size * sizeof(struct spdk_req_cmd)));
+				return NULL;
+			}
+		}
+		if (opts.cq.vaddr)
+		{
+			if (opts.cq.buffer_size < (opts.io_queue_size * sizeof(struct spdk_req_cpl)))
+			{
+				CLIENT_CTRLR_ERRLOG(ctrlr, "cq buffer size %" PRIx64 " is too small for cq size %zx\n",
+									opts.cq.buffer_size, (opts.io_queue_size * sizeof(struct spdk_req_cpl)));
+				return NULL;
+			}
+		}
+        opts.io_queue_size = user_opts->io_queue_size;
+        opts.io_queue_requests = user_opts->io_queue_requests;
+	}
+	SPDK_NOTICELOG("spdk_client_ctrlr_alloc_io_qpair : io_queue_size %d\n", opts.io_queue_size);
+	qpair = client_ctrlr_create_io_qpair(ctrlr, &opts);
+
+	if (qpair == NULL || opts.create_only == true)
+	{
+		return qpair;
+	}
+	// save id in qpair
+	qpair->trid = id;
+
+	rc = spdk_client_poll_group_add(client_pg, qpair);
+	if (rc != 0)
+	{
+		CLIENT_CTRLR_ERRLOG(ctrlr, "spdk_client_poll_group_add() failed\n");
+		goto err;
+	}
+
+	qpair->cb = cb_fn;
+	qpair->cb_args = cb_arg;
+
+	rc = spdk_client_ctrlr_connect_io_qpair_async(ctrlr, qpair);
+	if (rc != 0)
+	{
+		CLIENT_CTRLR_ERRLOG(ctrlr, "spdk_client_ctrlr_connect_io_qpair() failed\n");
+		goto err;
+	}
+	return qpair;
+err:
+	client_robust_mutex_lock(&ctrlr->ctrlr_lock);
+	client_ctrlr_proc_remove_io_qpair(qpair);
+	TAILQ_REMOVE(&ctrlr->active_io_qpairs, qpair, tailq);
+	spdk_bit_array_set(ctrlr->free_io_qids, qpair->id);
+	client_transport_ctrlr_delete_io_qpair(ctrlr, qpair);
+	client_robust_mutex_unlock(&ctrlr->ctrlr_lock);
+	return NULL;
+}
+
+int spdk_client_ctrlr_reconnect_io_qpair(struct spdk_client_qpair *qpair)
+{
+	struct spdk_client_ctrlr *ctrlr;
+	enum client_qpair_state qpair_state;
+	int rc;
+
+	assert(qpair != NULL);
+	assert(client_qpair_is_admin_queue(qpair) == false);
+	assert(qpair->ctrlr != NULL);
+
+	ctrlr = qpair->ctrlr;
+	client_robust_mutex_lock(&ctrlr->ctrlr_lock);
+	qpair_state = client_qpair_get_state(qpair);
+
+	if (ctrlr->is_removed)
+	{
+		rc = -ENODEV;
+		goto out;
+	}
+
+	if (ctrlr->is_resetting || qpair_state == CLIENT_QPAIR_DISCONNECTING)
+	{
+		rc = -EAGAIN;
+		goto out;
+	}
+
+	if (ctrlr->is_failed || qpair_state == CLIENT_QPAIR_DESTROYING)
+	{
+		rc = -ENXIO;
+		goto out;
+	}
+
+	if (qpair_state != CLIENT_QPAIR_DISCONNECTED)
+	{
+		rc = 0;
+		goto out;
+	}
+
+	rc = client_transport_ctrlr_connect_qpair(ctrlr, qpair);
+	if (rc)
+	{
+		rc = -EAGAIN;
+		goto out;
+	}
+
+out:
+	client_robust_mutex_unlock(&ctrlr->ctrlr_lock);
+	return rc;
+}
+
+spdk_client_qp_failure_reason
+spdk_client_ctrlr_get_admin_qp_failure_reason(struct spdk_client_ctrlr *ctrlr)
+{
+	return ctrlr->adminq->transport_failure_reason;
+}
+
+/*
+ * This internal function will attempt to take the controller
+ * lock before calling disconnect on a controller qpair.
+ * Functions already holding the controller lock should
+ * call client_transport_ctrlr_disconnect_qpair directly.
+ */
+void client_ctrlr_disconnect_qpair(struct spdk_client_qpair *qpair)
+{
+	struct spdk_client_ctrlr *ctrlr = qpair->ctrlr;
+
+	assert(ctrlr != NULL);
+	client_robust_mutex_lock(&ctrlr->ctrlr_lock);
+	client_transport_ctrlr_disconnect_qpair(ctrlr, qpair);
+	client_robust_mutex_unlock(&ctrlr->ctrlr_lock);
+}
+
+int spdk_client_ctrlr_free_io_qpair(struct spdk_client_qpair *qpair)
+{
+	struct spdk_client_ctrlr *ctrlr;
+
+	if (qpair == NULL)
+	{
+		return 0;
+	}
+
+	ctrlr = qpair->ctrlr;
+
+	if (qpair->in_completion_context)
+	{
+		/*
+		 * There are many cases where it is convenient to delete an io qpair in the context
+		 *  of that qpair's completion routine.  To handle this properly, set a flag here
+		 *  so that the completion routine will perform an actual delete after the context
+		 *  unwinds.
+		 */
+		qpair->delete_after_completion_context = 1;
+		return 0;
+	}
+
+	if (qpair->poll_group && qpair->poll_group->in_completion_context)
+	{
+		/* Same as above, but in a poll group. */
+		qpair->poll_group->num_qpairs_to_delete++;
+		qpair->delete_after_completion_context = 1;
+		return 0;
+	}
+
+	client_transport_ctrlr_disconnect_qpair(ctrlr, qpair);
+
+	if (qpair->poll_group)
+	{
+		spdk_client_poll_group_remove(qpair->poll_group->group, qpair);
+	}
+
+	/* Do not retry. */
+	client_qpair_set_state(qpair, CLIENT_QPAIR_DESTROYING);
+
+	/* In the multi-process case, a process may call this function on a foreign
+	 * I/O qpair (i.e. one that this process did not create) when that qpairs process
+	 * exits unexpectedly.  In that case, we must not try to abort any reqs associated
+	 * with that qpair, since the callbacks will also be foreign to this process.
+	 */
+	if (qpair->active_proc == client_ctrlr_get_current_process(ctrlr))
+	{
+		client_qpair_abort_all_queued_reqs(qpair, 0);
+	}
+
+	client_robust_mutex_lock(&ctrlr->ctrlr_lock);
+
+	client_ctrlr_proc_remove_io_qpair(qpair);
+
+	TAILQ_REMOVE(&ctrlr->active_io_qpairs, qpair, tailq);
+	spdk_client_ctrlr_free_qid(ctrlr, qpair->id);
+
+	client_transport_ctrlr_delete_io_qpair(ctrlr, qpair);
+	client_robust_mutex_unlock(&ctrlr->ctrlr_lock);
+	return 0;
+}
+
+bool spdk_client_ctrlr_is_failed(struct spdk_client_ctrlr *ctrlr)
+{
+	return ctrlr->is_failed;
+}
+
+void client_ctrlr_fail(struct spdk_client_ctrlr *ctrlr, bool hot_remove)
+{
+	/*
+	 * Set the flag here and leave the work failure of qpairs to
+	 * spdk_client_qpair_process_completions().
+	 */
+	if (hot_remove)
+	{
+		ctrlr->is_removed = true;
+	}
+
+	if (ctrlr->is_failed)
+	{
+		CLIENT_CTRLR_NOTICELOG(ctrlr, "already in failed state\n");
+		return;
+	}
+
+	ctrlr->is_failed = true;
+	client_transport_ctrlr_disconnect_qpair(ctrlr, ctrlr->adminq);
+	CLIENT_CTRLR_ERRLOG(ctrlr, "in failed state.\n");
+}
+
+/**
+ * This public API function will try to take the controller lock.
+ * Any private functions being called from a thread already holding
+ * the ctrlr lock should call client_ctrlr_fail directly.
+ */
+void spdk_client_ctrlr_fail(struct spdk_client_ctrlr *ctrlr)
+{
+	client_robust_mutex_lock(&ctrlr->ctrlr_lock);
+	client_ctrlr_fail(ctrlr, false);
+	client_robust_mutex_unlock(&ctrlr->ctrlr_lock);
+}
+
+static void
+client_ctrlr_shutdown_async(struct spdk_client_ctrlr *ctrlr,
+							struct client_ctrlr_detach_ctx *ctx)
+{
+	int rc;
+
+	if (ctrlr->is_removed)
+	{
+		ctx->shutdown_complete = true;
+		return;
+	}
+}
+
+static int
+client_ctrlr_shutdown_poll_async(struct spdk_client_ctrlr *ctrlr,
+								 struct client_ctrlr_detach_ctx *ctx)
+{
+	union spdk_client_csts_register csts;
+	uint32_t ms_waited;
+
+	switch (ctx->state)
+	{
+	case CLIENT_CTRLR_DETACH_SET_CC:
+	case CLIENT_CTRLR_DETACH_GET_CSTS:
+		/* We're still waiting for the register operation to complete */
+		spdk_client_qpair_process_completions(ctrlr->adminq, 0);
+		return -EAGAIN;
+
+	case CLIENT_CTRLR_DETACH_GET_CSTS_DONE:
+		ctx->state = CLIENT_CTRLR_DETACH_CHECK_CSTS;
+		break;
+
+	default:
+		assert(0 && "Should never happen");
+		return -EINVAL;
+	}
+
+	ms_waited = (spdk_get_ticks() - ctx->shutdown_start_tsc) * 1000 / spdk_get_ticks_hz();
+	csts.raw = ctx->csts.raw;
+
+	if (csts.bits.shst == SPDK_CLIENT_SHST_COMPLETE)
+	{
+		CLIENT_CTRLR_DEBUGLOG(ctrlr, "shutdown complete in %u milliseconds\n", ms_waited);
+		return 0;
+	}
+
+	if (ms_waited < ctx->shutdown_timeout_ms)
+	{
+		return -EAGAIN;
+	}
+
+	CLIENT_CTRLR_ERRLOG(ctrlr, "did not shutdown within %u milliseconds\n",
+						ctx->shutdown_timeout_ms);
+	if (ctrlr->quirks & CLIENT_QUIRK_SHST_COMPLETE)
+	{
+		CLIENT_CTRLR_ERRLOG(ctrlr, "likely due to shutdown handling in the VMWare emulated Client SSD\n");
+	}
+
+	return 0;
+}
+
+static int
+client_ctrlr_enable(struct spdk_client_ctrlr *ctrlr)
+{
+	int rc;
+
+	rc = client_transport_ctrlr_enable(ctrlr);
+	if (rc != 0)
+	{
+		CLIENT_CTRLR_ERRLOG(ctrlr, "transport ctrlr_enable failed\n");
+		return rc;
+	}
+
+	return 0;
+}
+
+static const char *
+client_ctrlr_state_string(enum client_ctrlr_state state)
+{
+	switch (state)
+	{
+	case CLIENT_CTRLR_STATE_INIT_DELAY:
+		return "delay init";
+	case CLIENT_CTRLR_STATE_CONNECT_ADMINQ:
+		return "connect adminq";
+	case CLIENT_CTRLR_STATE_WAIT_FOR_CONNECT_ADMINQ:
+		return "wait for connect adminq";
+	case CLIENT_CTRLR_STATE_READ_VS:
+		return "read vs";
+	case CLIENT_CTRLR_STATE_READ_VS_WAIT_FOR_VS:
+		return "read vs wait for vs";
+	case CLIENT_CTRLR_STATE_READ_CAP:
+		return "read cap";
+	case CLIENT_CTRLR_STATE_READ_CAP_WAIT_FOR_CAP:
+		return "read cap wait for cap";
+	case CLIENT_CTRLR_STATE_CHECK_EN:
+		return "check en";
+	case CLIENT_CTRLR_STATE_CHECK_EN_WAIT_FOR_CC:
+		return "check en wait for cc";
+	case CLIENT_CTRLR_STATE_DISABLE_WAIT_FOR_READY_1:
+		return "disable and wait for CSTS.RDY = 1";
+	case CLIENT_CTRLR_STATE_DISABLE_WAIT_FOR_READY_1_WAIT_FOR_CSTS:
+		return "disable and wait for CSTS.RDY = 1 reg";
+	case CLIENT_CTRLR_STATE_SET_EN_0:
+		return "set CC.EN = 0";
+	case CLIENT_CTRLR_STATE_SET_EN_0_WAIT_FOR_CC:
+		return "set CC.EN = 0 wait for cc";
+	case CLIENT_CTRLR_STATE_DISABLE_WAIT_FOR_READY_0:
+		return "disable and wait for CSTS.RDY = 0";
+	case CLIENT_CTRLR_STATE_DISABLE_WAIT_FOR_READY_0_WAIT_FOR_CSTS:
+		return "disable and wait for CSTS.RDY = 0 reg";
+	case CLIENT_CTRLR_STATE_ENABLE:
+		return "enable controller by writing CC.EN = 1";
+	case CLIENT_CTRLR_STATE_ENABLE_WAIT_FOR_CC:
+		return "enable controller by writing CC.EN = 1 reg";
+	case CLIENT_CTRLR_STATE_ENABLE_WAIT_FOR_READY_1:
+		return "wait for CSTS.RDY = 1";
+	case CLIENT_CTRLR_STATE_ENABLE_WAIT_FOR_READY_1_WAIT_FOR_CSTS:
+		return "wait for CSTS.RDY = 1 reg";
+	case CLIENT_CTRLR_STATE_RESET_ADMIN_QUEUE:
+		return "reset admin queue";
+	case CLIENT_CTRLR_STATE_IDENTIFY:
+		return "identify controller";
+	case CLIENT_CTRLR_STATE_WAIT_FOR_IDENTIFY:
+		return "wait for identify controller";
+	case CLIENT_CTRLR_STATE_CONFIGURE_AER:
+		return "configure AER";
+	case CLIENT_CTRLR_STATE_WAIT_FOR_CONFIGURE_AER:
+		return "wait for configure aer";
+	case CLIENT_CTRLR_STATE_SET_KEEP_ALIVE_TIMEOUT:
+		return "set keep alive timeout";
+	case CLIENT_CTRLR_STATE_WAIT_FOR_KEEP_ALIVE_TIMEOUT:
+		return "wait for set keep alive timeout";
+	case CLIENT_CTRLR_STATE_IDENTIFY_IOCS_SPECIFIC:
+		return "identify controller iocs specific";
+	case CLIENT_CTRLR_STATE_WAIT_FOR_IDENTIFY_IOCS_SPECIFIC:
+		return "wait for identify controller iocs specific";
+	case CLIENT_CTRLR_STATE_GET_ZNS_CMD_EFFECTS_LOG:
+		return "get zns cmd and effects log page";
+	case CLIENT_CTRLR_STATE_WAIT_FOR_GET_ZNS_CMD_EFFECTS_LOG:
+		return "wait for get zns cmd and effects log page";
+	case CLIENT_CTRLR_STATE_SET_NUM_QUEUES:
+		return "set number of queues";
+	case CLIENT_CTRLR_STATE_WAIT_FOR_SET_NUM_QUEUES:
+		return "wait for set number of queues";
+	case CLIENT_CTRLR_STATE_IDENTIFY_ACTIVE_NS:
+		return "identify active ns";
+	case CLIENT_CTRLR_STATE_WAIT_FOR_IDENTIFY_ACTIVE_NS:
+		return "wait for identify active ns";
+	case CLIENT_CTRLR_STATE_IDENTIFY_NS:
+		return "identify ns";
+	case CLIENT_CTRLR_STATE_WAIT_FOR_IDENTIFY_NS:
+		return "wait for identify ns";
+	case CLIENT_CTRLR_STATE_IDENTIFY_ID_DESCS:
+		return "identify namespace id descriptors";
+	case CLIENT_CTRLR_STATE_WAIT_FOR_IDENTIFY_ID_DESCS:
+		return "wait for identify namespace id descriptors";
+	case CLIENT_CTRLR_STATE_IDENTIFY_NS_IOCS_SPECIFIC:
+		return "identify ns iocs specific";
+	case CLIENT_CTRLR_STATE_WAIT_FOR_IDENTIFY_NS_IOCS_SPECIFIC:
+		return "wait for identify ns iocs specific";
+	case CLIENT_CTRLR_STATE_SET_SUPPORTED_LOG_PAGES:
+		return "set supported log pages";
+	case CLIENT_CTRLR_STATE_SET_SUPPORTED_INTEL_LOG_PAGES:
+		return "set supported INTEL log pages";
+	case CLIENT_CTRLR_STATE_WAIT_FOR_SUPPORTED_INTEL_LOG_PAGES:
+		return "wait for supported INTEL log pages";
+	case CLIENT_CTRLR_STATE_SET_SUPPORTED_FEATURES:
+		return "set supported features";
+	case CLIENT_CTRLR_STATE_SET_DB_BUF_CFG:
+		return "set doorbell buffer config";
+	case CLIENT_CTRLR_STATE_WAIT_FOR_DB_BUF_CFG:
+		return "wait for doorbell buffer config";
+	case CLIENT_CTRLR_STATE_SET_HOST_ID:
+		return "set host ID";
+	case CLIENT_CTRLR_STATE_WAIT_FOR_HOST_ID:
+		return "wait for set host ID";
+	case CLIENT_CTRLR_STATE_READY:
+		return "ready";
+	case CLIENT_CTRLR_STATE_ERROR:
+		return "error";
+	}
+	return "unknown";
+};
+
+static void
+_client_ctrlr_set_state(struct spdk_client_ctrlr *ctrlr, enum client_ctrlr_state state,
+						uint64_t timeout_in_ms, bool quiet)
+{
+	uint64_t ticks_per_ms, timeout_in_ticks, now_ticks;
+
+	ctrlr->state = state;
+	if (timeout_in_ms == CLIENT_TIMEOUT_KEEP_EXISTING)
+	{
+		if (!quiet)
+		{
+			CLIENT_CTRLR_DEBUGLOG(ctrlr, "setting state to %s (keeping existing timeout)\n",
+								  client_ctrlr_state_string(ctrlr->state));
+		}
+		return;
+	}
+
+	if (timeout_in_ms == CLIENT_TIMEOUT_INFINITE)
+	{
+		goto inf;
+	}
+
+	ticks_per_ms = spdk_get_ticks_hz() / 1000;
+	if (timeout_in_ms > UINT64_MAX / ticks_per_ms)
+	{
+		CLIENT_CTRLR_ERRLOG(ctrlr,
+							"Specified timeout would cause integer overflow. Defaulting to no timeout.\n");
+		goto inf;
+	}
+
+	now_ticks = spdk_get_ticks();
+	timeout_in_ticks = timeout_in_ms * ticks_per_ms;
+	if (timeout_in_ticks > UINT64_MAX - now_ticks)
+	{
+		CLIENT_CTRLR_ERRLOG(ctrlr,
+							"Specified timeout would cause integer overflow. Defaulting to no timeout.\n");
+		goto inf;
+	}
+
+	ctrlr->state_timeout_tsc = timeout_in_ticks + now_ticks;
+	if (!quiet)
+	{
+		CLIENT_CTRLR_DEBUGLOG(ctrlr, "setting state to %s (timeout %" PRIu64 " ms)\n",
+							  client_ctrlr_state_string(ctrlr->state), timeout_in_ms);
+	}
+	return;
+inf:
+	if (!quiet)
+	{
+		CLIENT_CTRLR_DEBUGLOG(ctrlr, "setting state to %s (no timeout)\n",
+							  client_ctrlr_state_string(ctrlr->state));
+	}
+	ctrlr->state_timeout_tsc = CLIENT_TIMEOUT_INFINITE;
+}
+
+static void
+client_ctrlr_set_state(struct spdk_client_ctrlr *ctrlr, enum client_ctrlr_state state,
+					   uint64_t timeout_in_ms)
+{
+	_client_ctrlr_set_state(ctrlr, state, timeout_in_ms, false);
+}
+
+static void
+client_ctrlr_set_state_quiet(struct spdk_client_ctrlr *ctrlr, enum client_ctrlr_state state,
+							 uint64_t timeout_in_ms)
+{
+	_client_ctrlr_set_state(ctrlr, state, timeout_in_ms, true);
+}
+
+static void
+client_ctrlr_abort_queued_aborts(struct spdk_client_ctrlr *ctrlr)
+{
+	struct client_request *req, *tmp;
+	struct spdk_req_cpl cpl = {};
+
+	cpl.status.sc = SPDK_CLIENT_SC_ABORTED_SQ_DELETION;
+	cpl.status.sct = SPDK_CLIENT_SCT_GENERIC;
+
+	STAILQ_FOREACH_SAFE(req, &ctrlr->queued_aborts, stailq, tmp)
+	{
+		STAILQ_REMOVE_HEAD(&ctrlr->queued_aborts, stailq);
+
+		client_complete_request(req->cb_fn, req->cb_arg, req->qpair, req, &cpl);
+		client_free_request(req);
+	}
+}
+
+int spdk_client_ctrlr_disconnect(struct spdk_client_ctrlr *ctrlr)
+{
+	struct spdk_client_qpair *qpair;
+
+	client_robust_mutex_lock(&ctrlr->ctrlr_lock);
+	ctrlr->prepare_for_reset = false;
+
+	if (ctrlr->is_resetting || ctrlr->is_removed)
+	{
+		/*
+		 * Controller is already resetting or has been removed. Return
+		 *  immediately since there is no need to kick off another
+		 *  reset in these cases.
+		 */
+		client_robust_mutex_unlock(&ctrlr->ctrlr_lock);
+		return ctrlr->is_resetting ? -EBUSY : -ENXIO;
+	}
+
+	ctrlr->is_resetting = true;
+	ctrlr->is_failed = false;
+
+	CLIENT_CTRLR_NOTICELOG(ctrlr, "resetting controller\n");
+
+	/* Disable keep-alive, it'll be re-enabled as part of the init process */
+	ctrlr->keep_alive_interval_ticks = 0;
+
+	/* Abort all of the queued abort requests */
+	client_ctrlr_abort_queued_aborts(ctrlr);
+
+	client_transport_admin_qpair_abort_aers(ctrlr->adminq);
+
+	/* Disable all queues before disabling the controller hardware. */
+	TAILQ_FOREACH(qpair, &ctrlr->active_io_qpairs, tailq)
+	{
+		qpair->transport_failure_reason = SPDK_CLIENT_QPAIR_FAILURE_LOCAL;
+	}
+
+	ctrlr->adminq->transport_failure_reason = SPDK_CLIENT_QPAIR_FAILURE_LOCAL;
+	client_transport_ctrlr_disconnect_qpair(ctrlr, ctrlr->adminq);
+
+	client_robust_mutex_unlock(&ctrlr->ctrlr_lock);
+	return 0;
+}
+
+void spdk_client_ctrlr_reconnect_async(struct spdk_client_ctrlr *ctrlr)
+{
+	client_robust_mutex_lock(&ctrlr->ctrlr_lock);
+
+	/* Set the state back to INIT to cause a full hardware reset. */
+	client_ctrlr_set_state(ctrlr, CLIENT_CTRLR_STATE_INIT, CLIENT_TIMEOUT_INFINITE);
+
+	/* Return without releasing ctrlr_lock. ctrlr_lock will be released when
+	 * spdk_client_ctrlr_reset_poll_async() returns 0.
+	 */
+}
+
+static int
+client_ctrlr_reset_pre(struct spdk_client_ctrlr *ctrlr)
+{
+	int rc;
+
+	rc = spdk_client_ctrlr_disconnect(ctrlr);
+	if (rc != 0)
+	{
+		return rc;
+	}
+
+	spdk_client_ctrlr_reconnect_async(ctrlr);
+	return 0;
+}
+
+/**
+ * This function will be called when the controller is being reinitialized.
+ * Note: the ctrlr_lock must be held when calling this function.
+ */
+int spdk_client_ctrlr_reconnect_poll_async(struct spdk_client_ctrlr *ctrlr)
+{
+	struct spdk_client_ns *ns, *tmp_ns;
+	struct spdk_client_qpair *qpair;
+	int rc = 0, rc_tmp = 0;
+	bool async;
+
+	if (client_ctrlr_process_init(ctrlr) != 0)
+	{
+		CLIENT_CTRLR_ERRLOG(ctrlr, "controller reinitialization failed\n");
+		rc = -1;
+	}
+	if (ctrlr->state != CLIENT_CTRLR_STATE_READY && rc != -1)
+	{
+		return -EAGAIN;
+	}
+
+	/*
+	 * For non-fabrics controllers, the memory locations of the transport qpair
+	 * don't change when the controller is reset. They simply need to be
+	 * re-enabled with admin commands to the controller. For fabric
+	 * controllers we need to disconnect and reconnect the qpair on its
+	 * own thread outside of the context of the reset.
+	 */
+	if (rc == 0 && !spdk_client_ctrlr_is_fabrics(ctrlr))
+	{
+		/* Reinitialize qpairs */
+		TAILQ_FOREACH(qpair, &ctrlr->active_io_qpairs, tailq)
+		{
+			assert(spdk_bit_array_get(ctrlr->free_io_qids, qpair->id));
+			spdk_bit_array_clear(ctrlr->free_io_qids, qpair->id);
+
+			/* Force a synchronous connect. We can't currently handle an asynchronous
+			 * operation here. */
+			async = qpair->async;
+			qpair->async = false;
+			rc_tmp = client_transport_ctrlr_connect_qpair(ctrlr, qpair);
+			qpair->async = async;
+
+			if (rc_tmp != 0)
+			{
+				rc = rc_tmp;
+				qpair->transport_failure_reason = SPDK_CLIENT_QPAIR_FAILURE_LOCAL;
+				continue;
+			}
+		}
+	}
+
+	/*
+	 * Take this opportunity to remove inactive namespaces. During a reset namespace
+	 * handles can be invalidated.
+	 */
+	RB_FOREACH_SAFE(ns, client_ns_tree, &ctrlr->ns, tmp_ns)
+	{
+		if (!ns->active)
+		{
+			RB_REMOVE(client_ns_tree, &ctrlr->ns, ns);
+			spdk_free(ns);
+		}
+	}
+
+	if (rc)
+	{
+		client_ctrlr_fail(ctrlr, false);
+	}
+	ctrlr->is_resetting = false;
+
+	client_robust_mutex_unlock(&ctrlr->ctrlr_lock);
+
+	return rc;
+}
+
+enum client_active_ns_state
+{
+	CLIENT_ACTIVE_NS_STATE_IDLE,
+	CLIENT_ACTIVE_NS_STATE_PROCESSING,
+	CLIENT_ACTIVE_NS_STATE_DONE,
+	CLIENT_ACTIVE_NS_STATE_ERROR
+};
+
+typedef void (*client_active_ns_ctx_deleter)(struct client_active_ns_ctx *);
+
+struct client_active_ns_ctx
+{
+	struct spdk_client_ctrlr *ctrlr;
+	uint32_t page_count;
+	uint32_t next_nsid;
+	uint32_t *new_ns_list;
+	client_active_ns_ctx_deleter deleter;
+
+	enum client_active_ns_state state;
+};
+
+static struct client_active_ns_ctx *
+client_active_ns_ctx_create(struct spdk_client_ctrlr *ctrlr, client_active_ns_ctx_deleter deleter)
+{
+	struct client_active_ns_ctx *ctx;
+	uint32_t *new_ns_list = NULL;
+
+	ctx = calloc(1, sizeof(*ctx));
+	if (!ctx)
+	{
+		CLIENT_CTRLR_ERRLOG(ctrlr, "Failed to allocate client_active_ns_ctx!\n");
+		return NULL;
+	}
+
+	new_ns_list = spdk_zmalloc(sizeof(struct spdk_client_ns_list), ctrlr->page_size,
+							   NULL, SPDK_ENV_LCORE_ID_ANY, SPDK_MALLOC_SHARE);
+	if (!new_ns_list)
+	{
+		CLIENT_CTRLR_ERRLOG(ctrlr, "Failed to allocate active_ns_list!\n");
+		free(ctx);
+		return NULL;
+	}
+
+	ctx->page_count = 1;
+	ctx->new_ns_list = new_ns_list;
+	ctx->ctrlr = ctrlr;
+	ctx->deleter = deleter;
+
+	return ctx;
+}
+
+static void
+client_active_ns_ctx_destroy(struct client_active_ns_ctx *ctx)
+{
+	spdk_free(ctx->new_ns_list);
+	free(ctx);
+}
+
+static int
+client_ctrlr_destruct_namespace(struct spdk_client_ctrlr *ctrlr, uint32_t nsid)
+{
+	struct spdk_client_ns tmp, *ns;
+
+	assert(ctrlr != NULL);
+
+	tmp.id = nsid;
+	ns = RB_FIND(client_ns_tree, &ctrlr->ns, &tmp);
+	if (ns == NULL)
+	{
+		return -EINVAL;
+	}
+
+	ns->active = false;
+
+	return 0;
+}
+
+int client_ctrlr_construct_namespace(struct spdk_client_ctrlr *ctrlr, uint32_t nsid)
+{
+	struct spdk_client_ns *ns;
+
+	if (nsid < 1 || nsid > 255)
+	{
+		return -EINVAL;
+	}
+
+	/* Namespaces are constructed on demand, so simply request it. */
+	ns = spdk_client_ctrlr_get_ns(ctrlr, nsid);
+	if (ns == NULL)
+	{
+		return -ENOMEM;
+	}
+
+	ns->active = true;
+
+	return 0;
+}
+
+struct spdk_client_ctrlr_process *
+client_ctrlr_get_process(struct spdk_client_ctrlr *ctrlr, pid_t pid)
+{
+	struct spdk_client_ctrlr_process *active_proc;
+
+	TAILQ_FOREACH(active_proc, &ctrlr->active_procs, tailq)
+	{
+		if (active_proc->pid == pid)
+		{
+			return active_proc;
+		}
+	}
+
+	return NULL;
+}
+
+struct spdk_client_ctrlr_process *
+client_ctrlr_get_current_process(struct spdk_client_ctrlr *ctrlr)
+{
+	return client_ctrlr_get_process(ctrlr, getpid());
+}
+
+/**
+ * This function will be called when a process is using the controller.
+ *  1. For the primary process, it is called when constructing the controller.
+ *  2. For the secondary process, it is called at probing the controller.
+ * Note: will check whether the process is already added for the same process.
+ */
+int client_ctrlr_add_process(struct spdk_client_ctrlr *ctrlr, void *devhandle)
+{
+	struct spdk_client_ctrlr_process *ctrlr_proc;
+	pid_t pid = getpid();
+
+	/* Check whether the process is already added or not */
+	if (client_ctrlr_get_process(ctrlr, pid))
+	{
+		return 0;
+	}
+
+	/* Initialize the per process properties for this ctrlr */
+	ctrlr_proc = spdk_zmalloc(sizeof(struct spdk_client_ctrlr_process),
+							  64, NULL, SPDK_ENV_SOCKET_ID_ANY, SPDK_MALLOC_SHARE);
+	if (ctrlr_proc == NULL)
+	{
+		CLIENT_CTRLR_ERRLOG(ctrlr, "failed to allocate memory to track the process props\n");
+
+		return -1;
+	}
+
+	ctrlr_proc->is_primary = spdk_process_is_primary();
+	ctrlr_proc->pid = pid;
+	STAILQ_INIT(&ctrlr_proc->active_reqs);
+	ctrlr_proc->devhandle = devhandle;
+	ctrlr_proc->ref = 0;
+	TAILQ_INIT(&ctrlr_proc->allocated_io_qpairs);
+	STAILQ_INIT(&ctrlr_proc->async_events);
+
+	TAILQ_INSERT_TAIL(&ctrlr->active_procs, ctrlr_proc, tailq);
+
+	return 0;
+}
+
+/**
+ * This function will be called when the process detaches the controller.
+ * Note: the ctrlr_lock must be held when calling this function.
+ */
+static void
+client_ctrlr_remove_process(struct spdk_client_ctrlr *ctrlr,
+							struct spdk_client_ctrlr_process *proc)
+{
+	struct spdk_client_qpair *qpair, *tmp_qpair;
+
+	assert(STAILQ_EMPTY(&proc->active_reqs));
+
+	TAILQ_FOREACH_SAFE(qpair, &proc->allocated_io_qpairs, per_process_tailq, tmp_qpair)
+	{
+		spdk_client_ctrlr_free_io_qpair(qpair);
+	}
+
+	TAILQ_REMOVE(&ctrlr->active_procs, proc, tailq);
+
+	spdk_free(proc);
+}
+
+/**
+ * This function will be called when the process exited unexpectedly
+ *  in order to free any incomplete client request, allocated IO qpairs
+ *  and allocated memory.
+ * Note: the ctrlr_lock must be held when calling this function.
+ */
+static void
+client_ctrlr_cleanup_process(struct spdk_client_ctrlr_process *proc)
+{
+	struct client_request *req, *tmp_req;
+	struct spdk_client_qpair *qpair, *tmp_qpair;
+	struct spdk_client_ctrlr_aer_completion_list *event;
+
+	STAILQ_FOREACH_SAFE(req, &proc->active_reqs, stailq, tmp_req)
+	{
+		STAILQ_REMOVE(&proc->active_reqs, req, client_request, stailq);
+
+		assert(req->pid == proc->pid);
+
+		client_free_request(req);
+	}
+
+	/* Remove async event from each process objects event list */
+	while (!STAILQ_EMPTY(&proc->async_events))
+	{
+		event = STAILQ_FIRST(&proc->async_events);
+		STAILQ_REMOVE_HEAD(&proc->async_events, link);
+		spdk_free(event);
+	}
+
+	TAILQ_FOREACH_SAFE(qpair, &proc->allocated_io_qpairs, per_process_tailq, tmp_qpair)
+	{
+		TAILQ_REMOVE(&proc->allocated_io_qpairs, qpair, per_process_tailq);
+
+		/*
+		 * The process may have been killed while some qpairs were in their
+		 *  completion context.  Clear that flag here to allow these IO
+		 *  qpairs to be deleted.
+		 */
+		qpair->in_completion_context = 0;
+
+		qpair->no_deletion_notification_needed = 1;
+
+		spdk_client_ctrlr_free_io_qpair(qpair);
+	}
+
+	spdk_free(proc);
+}
+
+/**
+ * This function will be called when destructing the controller.
+ *  1. There is no more admin request on this controller.
+ *  2. Clean up any left resource allocation when its associated process is gone.
+ */
+void client_ctrlr_free_processes(struct spdk_client_ctrlr *ctrlr)
+{
+	struct spdk_client_ctrlr_process *active_proc, *tmp;
+
+	/* Free all the processes' properties and make sure no pending admin IOs */
+	TAILQ_FOREACH_SAFE(active_proc, &ctrlr->active_procs, tailq, tmp)
+	{
+		TAILQ_REMOVE(&ctrlr->active_procs, active_proc, tailq);
+
+		assert(STAILQ_EMPTY(&active_proc->active_reqs));
+
+		spdk_free(active_proc);
+	}
+}
+
+/**
+ * This function will be called when any other process attaches or
+ *  detaches the controller in order to cleanup those unexpectedly
+ *  terminated processes.
+ * Note: the ctrlr_lock must be held when calling this function.
+ */
+static int
+client_ctrlr_remove_inactive_proc(struct spdk_client_ctrlr *ctrlr)
+{
+	struct spdk_client_ctrlr_process *active_proc, *tmp;
+	int active_proc_count = 0;
+
+	TAILQ_FOREACH_SAFE(active_proc, &ctrlr->active_procs, tailq, tmp)
+	{
+		if ((kill(active_proc->pid, 0) == -1) && (errno == ESRCH))
+		{
+			CLIENT_CTRLR_ERRLOG(ctrlr, "process %d terminated unexpected\n", active_proc->pid);
+
+			TAILQ_REMOVE(&ctrlr->active_procs, active_proc, tailq);
+
+			client_ctrlr_cleanup_process(active_proc);
+		}
+		else
+		{
+			active_proc_count++;
+		}
+	}
+
+	return active_proc_count;
+}
+
+void client_ctrlr_proc_get_ref(struct spdk_client_ctrlr *ctrlr)
+{
+	struct spdk_client_ctrlr_process *active_proc;
+
+	client_robust_mutex_lock(&ctrlr->ctrlr_lock);
+
+	client_ctrlr_remove_inactive_proc(ctrlr);
+
+	active_proc = client_ctrlr_get_current_process(ctrlr);
+	if (active_proc)
+	{
+		active_proc->ref++;
+	}
+
+	client_robust_mutex_unlock(&ctrlr->ctrlr_lock);
+}
+
+void client_ctrlr_proc_put_ref(struct spdk_client_ctrlr *ctrlr)
+{
+	struct spdk_client_ctrlr_process *active_proc;
+	int proc_count;
+
+	client_robust_mutex_lock(&ctrlr->ctrlr_lock);
+
+	proc_count = client_ctrlr_remove_inactive_proc(ctrlr);
+
+	active_proc = client_ctrlr_get_current_process(ctrlr);
+	if (active_proc)
+	{
+		active_proc->ref--;
+		assert(active_proc->ref >= 0);
+
+		/*
+		 * The last active process will be removed at the end of
+		 * the destruction of the controller.
+		 */
+		if (active_proc->ref == 0 && proc_count != 1)
+		{
+			client_ctrlr_remove_process(ctrlr, active_proc);
+		}
+	}
+
+	client_robust_mutex_unlock(&ctrlr->ctrlr_lock);
+}
+
+int client_ctrlr_get_ref_count(struct spdk_client_ctrlr *ctrlr)
+{
+	struct spdk_client_ctrlr_process *active_proc;
+	int ref = 0;
+
+	client_robust_mutex_lock(&ctrlr->ctrlr_lock);
+
+	client_ctrlr_remove_inactive_proc(ctrlr);
+
+	TAILQ_FOREACH(active_proc, &ctrlr->active_procs, tailq)
+	{
+		ref += active_proc->ref;
+	}
+
+	client_robust_mutex_unlock(&ctrlr->ctrlr_lock);
+
+	return ref;
+}
+
+/**
+ * This function will be called repeatedly during initialization until the controller is ready.
+ */
+int client_ctrlr_process_init(struct spdk_client_ctrlr *ctrlr)
+{
+	uint32_t ready_timeout_in_ms;
+	uint64_t ticks;
+	int rc = 0;
+
+	ticks = spdk_get_ticks();
+
+	/*
+	 * May need to avoid accessing any register on the target controller
+	 * for a while. Return early without touching the FSM.
+	 * Check sleep_timeout_tsc > 0 for unit test.
+	 */
+	if ((ctrlr->sleep_timeout_tsc > 0) &&
+		(ticks <= ctrlr->sleep_timeout_tsc))
+	{
+		return 0;
+	}
+	ctrlr->sleep_timeout_tsc = 0;
+
+	/*
+	 * Check if the current initialization step is done or has timed out.
+	 */
+	switch (ctrlr->state)
+	{
+	case CLIENT_CTRLR_STATE_INIT_DELAY:
+		client_ctrlr_set_state(ctrlr, CLIENT_CTRLR_STATE_INIT, CLIENT_TIMEOUT_INFINITE);
+		if (ctrlr->quirks & CLIENT_QUIRK_DELAY_BEFORE_INIT)
+		{
+			/*
+			 * Controller may need some delay before it's enabled.
+			 *
+			 * This is a workaround for an issue where the PCIe-attached Client controller
+			 * is not ready after VFIO reset. We delay the initialization rather than the
+			 * enabling itself, because this is required only for the very first enabling
+			 * - directly after a VFIO reset.
+			 */
+			CLIENT_CTRLR_DEBUGLOG(ctrlr, "Adding 2 second delay before initializing the controller\n");
+			ctrlr->sleep_timeout_tsc = ticks + (2000 * spdk_get_ticks_hz() / 1000);
+		}
+		break;
+
+	case CLIENT_CTRLR_STATE_READY:
+		CLIENT_CTRLR_DEBUGLOG(ctrlr, "Ctrlr already in ready state\n");
+		return 0;
+
+	case CLIENT_CTRLR_STATE_ERROR:
+		CLIENT_CTRLR_ERRLOG(ctrlr, "Ctrlr is in error state\n");
+		return -1;
+
+	case CLIENT_CTRLR_STATE_READ_VS_WAIT_FOR_VS:
+	case CLIENT_CTRLR_STATE_READ_CAP_WAIT_FOR_CAP:
+	case CLIENT_CTRLR_STATE_CHECK_EN_WAIT_FOR_CC:
+	case CLIENT_CTRLR_STATE_SET_EN_0_WAIT_FOR_CC:
+	case CLIENT_CTRLR_STATE_DISABLE_WAIT_FOR_READY_1_WAIT_FOR_CSTS:
+	case CLIENT_CTRLR_STATE_DISABLE_WAIT_FOR_READY_0_WAIT_FOR_CSTS:
+	case CLIENT_CTRLR_STATE_ENABLE_WAIT_FOR_CC:
+	case CLIENT_CTRLR_STATE_ENABLE_WAIT_FOR_READY_1_WAIT_FOR_CSTS:
+	case CLIENT_CTRLR_STATE_WAIT_FOR_IDENTIFY:
+	case CLIENT_CTRLR_STATE_WAIT_FOR_CONFIGURE_AER:
+	case CLIENT_CTRLR_STATE_WAIT_FOR_KEEP_ALIVE_TIMEOUT:
+	case CLIENT_CTRLR_STATE_WAIT_FOR_IDENTIFY_IOCS_SPECIFIC:
+	case CLIENT_CTRLR_STATE_WAIT_FOR_GET_ZNS_CMD_EFFECTS_LOG:
+	case CLIENT_CTRLR_STATE_WAIT_FOR_SET_NUM_QUEUES:
+	case CLIENT_CTRLR_STATE_WAIT_FOR_IDENTIFY_ACTIVE_NS:
+	case CLIENT_CTRLR_STATE_WAIT_FOR_IDENTIFY_NS:
+	case CLIENT_CTRLR_STATE_WAIT_FOR_IDENTIFY_ID_DESCS:
+	case CLIENT_CTRLR_STATE_WAIT_FOR_IDENTIFY_NS_IOCS_SPECIFIC:
+	case CLIENT_CTRLR_STATE_WAIT_FOR_SUPPORTED_INTEL_LOG_PAGES:
+	case CLIENT_CTRLR_STATE_WAIT_FOR_DB_BUF_CFG:
+	case CLIENT_CTRLR_STATE_WAIT_FOR_HOST_ID:
+		spdk_client_qpair_process_completions(ctrlr->adminq, 0);
+		break;
+
+	default:
+		assert(0);
+		return -1;
+	}
+
+	/* Note: we use the ticks captured when we entered this function.
+	 * This covers environments where the SPDK process gets swapped out after
+	 * we tried to advance the state but before we check the timeout here.
+	 * It is not normal for this to happen, but harmless to handle it in this
+	 * way.
+	 */
+	if (ctrlr->state_timeout_tsc != CLIENT_TIMEOUT_INFINITE &&
+		ticks > ctrlr->state_timeout_tsc)
+	{
+		CLIENT_CTRLR_ERRLOG(ctrlr, "Initialization timed out in state %d (%s)\n",
+							ctrlr->state, client_ctrlr_state_string(ctrlr->state));
+		return -1;
+	}
+
+	return rc;
+}
+
+int client_robust_mutex_init_recursive_shared(pthread_mutex_t *mtx)
+{
+	pthread_mutexattr_t attr;
+	int rc = 0;
+
+	if (pthread_mutexattr_init(&attr))
+	{
+		return -1;
+	}
+	if (pthread_mutexattr_settype(&attr, PTHREAD_MUTEX_RECURSIVE) ||
+#ifndef __FreeBSD__
+		pthread_mutexattr_setrobust(&attr, PTHREAD_MUTEX_ROBUST) ||
+		pthread_mutexattr_setpshared(&attr, PTHREAD_PROCESS_SHARED) ||
+#endif
+		pthread_mutex_init(mtx, &attr))
+	{
+		rc = -1;
+	}
+	pthread_mutexattr_destroy(&attr);
+	return rc;
+}
+
+int client_ctrlr_construct(struct spdk_client_ctrlr *ctrlr)
+{
+	int rc;
+
+	client_ctrlr_set_state(ctrlr, CLIENT_CTRLR_STATE_INIT, CLIENT_TIMEOUT_INFINITE);
+
+	ctrlr->flags = 0;
+	ctrlr->free_io_qids = NULL;
+	ctrlr->is_resetting = false;
+	ctrlr->is_failed = false;
+	ctrlr->is_destructed = false;
+
+	ctrlr->free_io_qids = spdk_bit_array_create(ctrlr->opts.num_io_queues + 1);
+	if (ctrlr->free_io_qids == NULL)
+	{
+		client_ctrlr_set_state(ctrlr, CLIENT_CTRLR_STATE_ERROR, CLIENT_TIMEOUT_INFINITE);
+		return -1;
+	}
+
+	for (int i = 1; i <= ctrlr->opts.num_io_queues; i++)
+	{
+		spdk_client_ctrlr_free_qid(ctrlr, i);
+	}
+
+	TAILQ_INIT(&ctrlr->active_io_qpairs);
+	STAILQ_INIT(&ctrlr->queued_aborts);
+	STAILQ_INIT(&ctrlr->pending_rpc_requests);
+	ctrlr->outstanding_aborts = 0;
+
+	rc = client_robust_mutex_init_recursive_shared(&ctrlr->ctrlr_lock);
+	if (rc != 0)
+	{
+		return rc;
+	}
+
+	TAILQ_INIT(&ctrlr->active_procs);
+	STAILQ_INIT(&ctrlr->register_operations);
+
+	RB_INIT(&ctrlr->ns);
+	client_ctrlr_set_state(ctrlr, CLIENT_CTRLR_STATE_READY, CLIENT_TIMEOUT_INFINITE);
+	return rc;
+}
+
+static void
+client_ctrlr_init_cap(struct spdk_client_ctrlr *ctrlr)
+{
+
+	ctrlr->opts.io_queue_size = spdk_max(ctrlr->opts.io_queue_size, SPDK_CLIENT_IO_QUEUE_MIN_ENTRIES);
+	ctrlr->opts.io_queue_size = spdk_min(ctrlr->opts.io_queue_size, MAX_IO_QUEUE_ENTRIES);
+}
+
+void client_ctrlr_destruct_finish(struct spdk_client_ctrlr *ctrlr)
+{
+	pthread_mutex_destroy(&ctrlr->ctrlr_lock);
+}
+
+void client_ctrlr_destruct_async(struct spdk_client_ctrlr *ctrlr,
+								 struct client_ctrlr_detach_ctx *ctx)
+{
+	struct spdk_client_qpair *qpair, *tmp;
+
+	CLIENT_CTRLR_DEBUGLOG(ctrlr, "Prepare to destruct SSD\n");
+
+	ctrlr->is_destructed = true;
+
+	client_ctrlr_abort_queued_aborts(ctrlr);
+
+	TAILQ_FOREACH_SAFE(qpair, &ctrlr->active_io_qpairs, tailq, tmp)
+	{
+		spdk_client_ctrlr_free_io_qpair(qpair);
+	}
+
+	client_ctrlr_shutdown_async(ctrlr, ctx);
+}
+
+int client_ctrlr_destruct_poll_async(struct spdk_client_ctrlr *ctrlr,
+									 struct client_ctrlr_detach_ctx *ctx)
+{
+	struct spdk_client_ns *ns, *tmp_ns;
+	int rc = 0;
+
+	if (!ctx->shutdown_complete)
+	{
+		rc = client_ctrlr_shutdown_poll_async(ctrlr, ctx);
+		if (rc == -EAGAIN)
+		{
+			return -EAGAIN;
+		}
+		/* Destruct ctrlr forcefully for any other error. */
+	}
+
+	if (ctx->cb_fn)
+	{
+		ctx->cb_fn(ctrlr);
+	}
+
+	RB_FOREACH_SAFE(ns, client_ns_tree, &ctrlr->ns, tmp_ns)
+	{
+		client_ctrlr_destruct_namespace(ctrlr, ns->id);
+		RB_REMOVE(client_ns_tree, &ctrlr->ns, ns);
+		spdk_free(ns);
+	}
+
+	ctrlr->active_ns_count = 0;
+
+	spdk_bit_array_free(&ctrlr->free_io_qids);
+
+	client_transport_ctrlr_destruct(ctrlr);
+
+	return rc;
+}
+
+void client_ctrlr_destruct(struct spdk_client_ctrlr *ctrlr)
+{
+	struct client_ctrlr_detach_ctx ctx = {.ctrlr = ctrlr};
+	int rc;
+
+	client_ctrlr_destruct_async(ctrlr, &ctx);
+
+	while (1)
+	{
+		rc = client_ctrlr_destruct_poll_async(ctrlr, &ctx);
+		if (rc != -EAGAIN)
+		{
+			break;
+		}
+		client_delay(1000);
+	}
+}
+
+int client_ctrlr_submit_admin_request(struct spdk_client_ctrlr *ctrlr,
+									  struct client_request *req)
+{
+	return client_qpair_submit_request(ctrlr->adminq, req);
+}
+
+static void
+client_keep_alive_completion(void *cb_ctx, const struct spdk_req_cpl *cpl)
+{
+	/* Do nothing */
+}
+
+/*
+ * Check if we need to send a Keep Alive command.
+ * Caller must hold ctrlr->ctrlr_lock.
+ */
+static int
+client_ctrlr_keep_alive(struct spdk_client_ctrlr *ctrlr)
+{
+	uint64_t now;
+	struct client_request *req;
+	struct spdk_req_cmd *cmd;
+	int rc = 0;
+
+	now = spdk_get_ticks();
+	if (now < ctrlr->next_keep_alive_tick)
+	{
+		return rc;
+	}
+
+	req = client_allocate_request_null(ctrlr->adminq, client_keep_alive_completion, NULL);
+	if (req == NULL)
+	{
+		return rc;
+	}
+
+	cmd = &req->cmd;
+	cmd->opc = SPDK_CLIENT_OPC_KEEP_ALIVE;
+
+	rc = client_ctrlr_submit_admin_request(ctrlr, req);
+	if (rc != 0)
+	{
+		CLIENT_CTRLR_ERRLOG(ctrlr, "Submitting Keep Alive failed\n");
+		rc = -ENXIO;
+	}
+
+	ctrlr->next_keep_alive_tick = now + ctrlr->keep_alive_interval_ticks;
+	return rc;
+}
+
+uint64_t
+spdk_client_ctrlr_get_pmrsz(struct spdk_client_ctrlr *ctrlr)
+{
+	return ctrlr->pmr_size;
+}
+
+bool spdk_client_ctrlr_is_active_ns(struct spdk_client_ctrlr *ctrlr, uint32_t nsid)
+{
+	struct spdk_client_ns tmp, *ns;
+
+	tmp.id = nsid;
+	ns = RB_FIND(client_ns_tree, &ctrlr->ns, &tmp);
+
+	if (ns != NULL)
+	{
+		return ns->active;
+	}
+
+	return false;
+}
+
+uint32_t
+spdk_client_ctrlr_get_first_active_ns(struct spdk_client_ctrlr *ctrlr)
+{
+	struct spdk_client_ns *ns;
+
+	ns = RB_MIN(client_ns_tree, &ctrlr->ns);
+	if (ns == NULL)
+	{
+		return 0;
+	}
+
+	while (ns != NULL)
+	{
+		if (ns->active)
+		{
+			return ns->id;
+		}
+
+		ns = RB_NEXT(client_ns_tree, &ctrlr->ns, ns);
+	}
+
+	return 0;
+}
+
+uint32_t
+spdk_client_ctrlr_get_next_active_ns(struct spdk_client_ctrlr *ctrlr, uint32_t prev_nsid)
+{
+	struct spdk_client_ns tmp, *ns;
+
+	tmp.id = prev_nsid;
+	ns = RB_FIND(client_ns_tree, &ctrlr->ns, &tmp);
+	if (ns == NULL)
+	{
+		return 0;
+	}
+
+	ns = RB_NEXT(client_ns_tree, &ctrlr->ns, ns);
+	while (ns != NULL)
+	{
+		if (ns->active)
+		{
+			return ns->id;
+		}
+
+		ns = RB_NEXT(client_ns_tree, &ctrlr->ns, ns);
+	}
+
+	return 0;
+}
+
+struct spdk_client_ns *
+spdk_client_ctrlr_get_ns(struct spdk_client_ctrlr *ctrlr, uint32_t nsid)
+{
+	struct spdk_client_ns tmp;
+	struct spdk_client_ns *ns;
+
+	if (nsid < 1 || nsid > 255)
+	{
+		return NULL;
+	}
+
+	client_robust_mutex_lock(&ctrlr->ctrlr_lock);
+
+	tmp.id = nsid;
+	ns = RB_FIND(client_ns_tree, &ctrlr->ns, &tmp);
+
+	if (ns == NULL)
+	{
+		ns = spdk_zmalloc(sizeof(struct spdk_client_ns), 64, NULL, SPDK_ENV_SOCKET_ID_ANY, SPDK_MALLOC_SHARE);
+		if (ns == NULL)
+		{
+			client_robust_mutex_unlock(&ctrlr->ctrlr_lock);
+			return NULL;
+		}
+
+		CLIENT_CTRLR_DEBUGLOG(ctrlr, "Namespace %u was added\n", nsid);
+		ns->id = nsid;
+		RB_INSERT(client_ns_tree, &ctrlr->ns, ns);
+	}
+
+	client_robust_mutex_unlock(&ctrlr->ctrlr_lock);
+
+	return ns;
+}
+
+uint32_t
+spdk_client_ctrlr_get_max_xfer_size(const struct spdk_client_ctrlr *ctrlr)
+{
+	return ctrlr->max_xfer_size;
+}
+
+void spdk_client_ctrlr_register_timeout_callback(struct spdk_client_ctrlr *ctrlr,
+												 uint64_t timeout_io_us, uint64_t timeout_admin_us,
+												 spdk_client_timeout_cb cb_fn, void *cb_arg)
+{
+	struct spdk_client_ctrlr_process *active_proc;
+
+	client_robust_mutex_lock(&ctrlr->ctrlr_lock);
+
+	active_proc = client_ctrlr_get_current_process(ctrlr);
+	if (active_proc)
+	{
+		active_proc->timeout_io_ticks = timeout_io_us * spdk_get_ticks_hz() / 1000000ULL;
+		active_proc->timeout_admin_ticks = timeout_admin_us * spdk_get_ticks_hz() / 1000000ULL;
+		active_proc->timeout_cb_fn = cb_fn;
+		active_proc->timeout_cb_arg = cb_arg;
+	}
+
+	ctrlr->timeout_enabled = true;
+
+	client_robust_mutex_unlock(&ctrlr->ctrlr_lock);
+}
+
+int client_request_check_timeout(struct client_request *req, uint16_t cid,
+								 struct spdk_client_ctrlr_process *active_proc,
+								 uint64_t now_tick)
+{
+	struct spdk_client_qpair *qpair = req->qpair;
+	struct spdk_client_ctrlr *ctrlr = qpair->ctrlr;
+	uint64_t timeout_ticks = client_qpair_is_admin_queue(qpair) ? active_proc->timeout_admin_ticks : active_proc->timeout_io_ticks;
+
+	assert(active_proc->timeout_cb_fn != NULL);
+
+	if (req->timed_out || req->submit_tick == 0)
+	{
+		return 0;
+	}
+
+	if (req->pid != g_spdk_client_pid)
+	{
+		return 0;
+	}
+
+	if (client_qpair_is_admin_queue(qpair) &&
+		req->cmd.opc == SPDK_CLIENT_OPC_ASYNC_EVENT_REQUEST)
+	{
+		return 0;
+	}
+
+	if (req->submit_tick + timeout_ticks > now_tick)
+	{
+		return 1;
+	}
+
+	req->timed_out = true;
+
+	/*
+	 * We don't want to expose the admin queue to the user,
+	 * so when we're timing out admin commands set the
+	 * qpair to NULL.
+	 */
+	active_proc->timeout_cb_fn(active_proc->timeout_cb_arg, ctrlr,
+							   client_qpair_is_admin_queue(qpair) ? NULL : qpair,
+							   cid);
+	return 0;
+}
+
+bool spdk_client_ctrlr_is_fabrics(struct spdk_client_ctrlr *ctrlr)
+{
+	assert(ctrlr);
+
+	return true;
+}
+
+uint64_t
+spdk_client_ctrlr_get_flags(struct spdk_client_ctrlr *ctrlr)
+{
+	return ctrlr->flags;
+}
+
+int32_t
+spdk_client_ctrlr_alloc_qid(struct spdk_client_ctrlr *ctrlr)
+{
+	uint32_t qid;
+
+	assert(ctrlr->free_io_qids);
+	client_robust_mutex_lock(&ctrlr->ctrlr_lock);
+	qid = spdk_bit_array_find_first_set(ctrlr->free_io_qids, 1);
+	if (qid > ctrlr->opts.num_io_queues)
+	{
+		CLIENT_CTRLR_ERRLOG(ctrlr, "No free I/O queue IDs %d %d\n", qid, ctrlr->opts.num_io_queues);
+		client_robust_mutex_unlock(&ctrlr->ctrlr_lock);
+		return -1;
+	}
+
+	spdk_bit_array_clear(ctrlr->free_io_qids, qid);
+	client_robust_mutex_unlock(&ctrlr->ctrlr_lock);
+	return qid;
+}
+
+void spdk_client_ctrlr_free_qid(struct spdk_client_ctrlr *ctrlr, uint16_t qid)
+{
+	assert(qid <= ctrlr->opts.num_io_queues);
+
+	client_robust_mutex_lock(&ctrlr->ctrlr_lock);
+
+	if (spdk_likely(ctrlr->free_io_qids))
+	{
+		spdk_bit_array_set(ctrlr->free_io_qids, qid);
+	}
+
+	client_robust_mutex_unlock(&ctrlr->ctrlr_lock);
+}
+
+int spdk_client_ctrlr_get_memory_domains(const struct spdk_client_ctrlr *ctrlr,
+										 struct spdk_memory_domain **domains, int array_size)
+{
+	return client_transport_ctrlr_get_memory_domains(ctrlr, domains, array_size);
+}
+
+struct spdk_client_poll_group *
+spdk_client_poll_group_create(void *ctx, struct spdk_client_accel_fn_table *table)
+{
+	struct spdk_client_poll_group *group;
+
+	group = calloc(1, sizeof(*group));
+	if (group == NULL)
+	{
+		return NULL;
+	}
+
+	group->accel_fn_table.table_size = sizeof(struct spdk_client_accel_fn_table);
+	if (table && table->table_size != 0)
+	{
+		group->accel_fn_table.table_size = table->table_size;
+#define SET_FIELD(field)                                                                                \
+	if (offsetof(struct spdk_client_accel_fn_table, field) + sizeof(table->field) <= table->table_size) \
+	{                                                                                                   \
+		group->accel_fn_table.field = table->field;                                                     \
+	}
+
+		SET_FIELD(submit_accel_crc32c);
+		/* Do not remove this statement, you should always update this statement when you adding a new field,
+		 * and do not forget to add the SET_FIELD statement for your added field. */
+		SPDK_STATIC_ASSERT(sizeof(struct spdk_client_accel_fn_table) == 16, "Incorrect size");
+
+#undef SET_FIELD
+	}
+
+	group->ctx = ctx;
+	STAILQ_INIT(&group->tgroups);
+
+	return group;
+}
+
+struct spdk_client_poll_group *
+spdk_client_qpair_get_optimal_poll_group(struct spdk_client_qpair *qpair)
+{
+	struct spdk_client_transport_poll_group *tgroup;
+
+	tgroup = client_transport_qpair_get_optimal_poll_group(qpair->transport, qpair);
+
+	if (tgroup == NULL)
+	{
+		return NULL;
+	}
+
+	return tgroup->group;
+}
+
+int spdk_client_poll_group_add(struct spdk_client_poll_group *group, struct spdk_client_qpair *qpair)
+{
+	struct spdk_client_transport_poll_group *tgroup;
+	const struct spdk_client_transport *transport;
+
+	if (client_qpair_get_state(qpair) != CLIENT_QPAIR_DISCONNECTED)
+	{
+		return -EINVAL;
+	}
+
+	STAILQ_FOREACH(tgroup, &group->tgroups, link)
+	{
+		if (tgroup->transport == qpair->transport)
+		{
+			break;
+		}
+	}
+
+	/* See if a new transport has been added (dlopen style) and we need to update the poll group */
+	if (!tgroup)
+	{
+		transport = client_get_first_transport();
+		while (transport != NULL)
+		{
+			if (transport == qpair->transport)
+			{
+				tgroup = client_transport_poll_group_create(transport);
+				if (tgroup == NULL)
+				{
+					return -ENOMEM;
+				}
+				tgroup->group = group;
+				STAILQ_INSERT_TAIL(&group->tgroups, tgroup, link);
+				break;
+			}
+			transport = client_get_next_transport(transport);
+		}
+	}
+
+	return tgroup ? client_transport_poll_group_add(tgroup, qpair) : -ENODEV;
+}
+
+int spdk_client_poll_group_remove(struct spdk_client_poll_group *group, struct spdk_client_qpair *qpair)
+{
+	struct spdk_client_transport_poll_group *tgroup;
+
+	STAILQ_FOREACH(tgroup, &group->tgroups, link)
+	{
+		if (tgroup->transport == qpair->transport)
+		{
+			return client_transport_poll_group_remove(tgroup, qpair);
+		}
+	}
+
+	return -ENODEV;
+}
+
+int client_poll_group_connect_qpair(struct spdk_client_qpair *qpair)
+{
+	return client_transport_poll_group_connect_qpair(qpair);
+}
+
+int client_poll_group_disconnect_qpair(struct spdk_client_qpair *qpair)
+{
+	return client_transport_poll_group_disconnect_qpair(qpair);
+}
+
+int64_t
+spdk_client_poll_group_process_completions(struct spdk_client_poll_group *group,
+										   uint32_t completions_per_qpair, spdk_client_disconnected_qpair_cb disconnected_qpair_cb)
+{
+	struct spdk_client_transport_poll_group *tgroup;
+	int64_t local_completions = 0, error_reason = 0, num_completions = 0;
+
+	if (disconnected_qpair_cb == NULL)
+	{
+		return -EINVAL;
+	}
+
+	STAILQ_FOREACH(tgroup, &group->tgroups, link)
+	{
+		local_completions = client_transport_poll_group_process_completions(tgroup, completions_per_qpair,
+																			disconnected_qpair_cb);
+		if (local_completions < 0 && error_reason == 0)
+		{
+			error_reason = local_completions;
+		}
+		else
+		{
+			num_completions += local_completions;
+			/* Just to be safe */
+			assert(num_completions >= 0);
+		}
+	}
+
+	return error_reason ? error_reason : num_completions;
+}
+
+void *
+spdk_client_poll_group_get_ctx(struct spdk_client_poll_group *group)
+{
+	return group->ctx;
+}
+
+int spdk_client_poll_group_destroy(struct spdk_client_poll_group *group)
+{
+	struct spdk_client_transport_poll_group *tgroup, *tmp_tgroup;
+
+	STAILQ_FOREACH_SAFE(tgroup, &group->tgroups, link, tmp_tgroup)
+	{
+		STAILQ_REMOVE(&group->tgroups, tgroup, spdk_client_transport_poll_group, link);
+		if (client_transport_poll_group_destroy(tgroup) != 0)
+		{
+			STAILQ_INSERT_TAIL(&group->tgroups, tgroup, link);
+			return -EBUSY;
+		}
+	}
+
+	free(group);
+
+	return 0;
+}
+
+int spdk_client_poll_group_get_stats(struct spdk_client_poll_group *group,
+									 struct spdk_client_poll_group_stat **stats)
+{
+	struct spdk_client_transport_poll_group *tgroup;
+	struct spdk_client_poll_group_stat *result;
+	uint32_t transports_count = 0;
+	/* Not all transports used by this poll group may support statistics reporting */
+	uint32_t reported_stats_count = 0;
+	int rc;
+
+	assert(group);
+	assert(stats);
+
+	result = calloc(1, sizeof(*result));
+	if (!result)
+	{
+		SPDK_ERRLOG("Failed to allocate memory for poll group statistics\n");
+		return -ENOMEM;
+	}
+
+	STAILQ_FOREACH(tgroup, &group->tgroups, link)
+	{
+		transports_count++;
+	}
+
+	result->transport_stat = calloc(transports_count, sizeof(*result->transport_stat));
+	if (!result->transport_stat)
+	{
+		SPDK_ERRLOG("Failed to allocate memory for poll group statistics\n");
+		free(result);
+		return -ENOMEM;
+	}
+
+	STAILQ_FOREACH(tgroup, &group->tgroups, link)
+	{
+		rc = client_transport_poll_group_get_stats(tgroup, &result->transport_stat[reported_stats_count]);
+		if (rc == 0)
+		{
+			reported_stats_count++;
+		}
+	}
+
+	if (reported_stats_count == 0)
+	{
+		free(result->transport_stat);
+		free(result);
+		SPDK_DEBUGLOG(client, "No transport statistics available\n");
+		return -ENOTSUP;
+	}
+
+	result->num_transports = reported_stats_count;
+	*stats = result;
+
+	return 0;
+}
+
+void spdk_client_poll_group_free_stats(struct spdk_client_poll_group *group,
+									   struct spdk_client_poll_group_stat *stat)
+{
+	struct spdk_client_transport_poll_group *tgroup;
+	uint32_t i;
+	uint32_t freed_stats __attribute__((unused)) = 0;
+
+	assert(group);
+	assert(stat);
+
+	for (i = 0; i < stat->num_transports; i++)
+	{
+		STAILQ_FOREACH(tgroup, &group->tgroups, link)
+		{
+			if (client_transport_get_trtype(tgroup->transport) == stat->transport_stat[i]->trtype)
+			{
+				client_transport_poll_group_free_stats(tgroup, stat->transport_stat[i]);
+				freed_stats++;
+				break;
+			}
+		}
+	}
+
+	assert(freed_stats == stat->num_transports);
+
+	free(stat->transport_stat);
+	free(stat);
+}
+
+void client_disconnected_qpair_cb(struct spdk_client_qpair *qpair, void *poll_group_ctx)
+{
+	struct client_poll_group *group = poll_group_ctx;
+
+	client_transport_ctrlr_delete_io_qpair(group->ctrlr, qpair);
+}
+
+SPDK_LOG_REGISTER_COMPONENT(client)
\ No newline at end of file
diff --git a/lib/rdma_server/cmd.c b/lib/rdma_server/cmd.c
new file mode 100644
index 000000000..e4ec8502f
--- /dev/null
+++ b/lib/rdma_server/cmd.c
@@ -0,0 +1,1277 @@
+/*-
+ *   BSD LICENSE
+ *
+ *   Copyright (c) Intel Corporation.
+ *   All rights reserved.
+ *   Copyright (c) 2021 Mellanox Technologies LTD. All rights reserved.
+ *   Copyright (c) 2021 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ *
+ *   Redistribution and use in source and binary forms, with or without
+ *   modification, are permitted provided that the following conditions
+ *   are met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in
+ *       the documentation and/or other materials provided with the
+ *       distribution.
+ *     * Neither the name of Intel Corporation nor the names of its
+ *       contributors may be used to endorse or promote products derived
+ *       from this software without specific prior written permission.
+ *
+ *   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ *   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ *   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ *   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ *   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ *   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ *   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ *   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ *   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ *   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ *   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "spdk/rdma_client.h"
+#include "spdk_internal/rdma_client.h"
+
+static void print_time_cost_for_rpc_req(struct rpc_request *req, const char *func_name)
+{
+	uint64_t now, passed;
+	now = spdk_get_ticks();
+	passed = now - req->tsc_last;
+	req->tsc_last = now;
+	SPDK_DEBUGLOG(rdma, "time passed %ld us at %s\n", passed * SPDK_SEC_TO_USEC / spdk_get_ticks_hz(), func_name);
+}
+
+static inline struct client_request *_client_ns_cmd_rw(
+	struct spdk_client_qpair *qpair,
+	const struct client_payload *payload, uint32_t payload_offset, uint32_t md_offset,
+	uint64_t lba, uint32_t lba_count, spdk_req_cmd_cb cb_fn,
+	void *cb_arg, uint32_t opc, uint32_t io_flags,
+	uint16_t apptag_mask, uint16_t apptag, bool check_sgl, int *rc);
+
+static bool
+client_ns_check_request_length(uint32_t lba_count, uint32_t sectors_per_max_io,
+							   uint32_t sectors_per_stripe, uint32_t qdepth)
+{
+	uint32_t child_per_io = UINT32_MAX;
+
+	/* After a namespace is destroyed(e.g. hotplug), all the fields associated with the
+	 * namespace will be cleared to zero, the function will return TRUE for this case,
+	 * and -EINVAL will be returned to caller.
+	 */
+	if (sectors_per_stripe > 0)
+	{
+		child_per_io = (lba_count + sectors_per_stripe - 1) / sectors_per_stripe;
+	}
+	else if (sectors_per_max_io > 0)
+	{
+		child_per_io = (lba_count + sectors_per_max_io - 1) / sectors_per_max_io;
+	}
+
+	SPDK_DEBUGLOG(client, "checking maximum i/o length %d\n", child_per_io);
+
+	return child_per_io >= qdepth;
+}
+
+static inline int
+client_ns_map_failure_rc(uint32_t lba_count, uint32_t sectors_per_max_io,
+						 uint32_t sectors_per_stripe, uint32_t qdepth, int rc)
+{
+	assert(rc);
+	if (rc == -ENOMEM &&
+		client_ns_check_request_length(lba_count, sectors_per_max_io, sectors_per_stripe, qdepth))
+	{
+		return -EINVAL;
+	}
+	return rc;
+}
+
+static inline uint32_t
+_client_get_host_buffer_sector_size(struct spdk_client_ns *ns, uint32_t io_flags)
+{
+	return ns->sector_size;
+}
+
+static inline uint32_t
+_client_get_sectors_per_max_io(struct spdk_client_ns *ns, uint32_t io_flags)
+{
+	return ns->sectors_per_max_io;
+}
+
+static struct client_request *
+_client_add_child_request(struct spdk_client_qpair *qpair,
+						  const struct client_payload *payload,
+						  uint32_t payload_offset, uint32_t md_offset,
+						  uint64_t lba, uint32_t lba_count, spdk_req_cmd_cb cb_fn, void *cb_arg, uint32_t opc,
+						  uint32_t io_flags, uint16_t apptag_mask, uint16_t apptag,
+						  struct client_request *parent, bool check_sgl, int *rc)
+{
+	struct client_request *child;
+
+	child = _client_ns_cmd_rw(qpair, payload, payload_offset, md_offset, lba, lba_count, cb_fn,
+							  cb_arg, opc, io_flags, apptag_mask, apptag, check_sgl, rc);
+	if (child == NULL)
+	{
+		client_request_free_children(parent);
+		client_free_request(parent);
+		return NULL;
+	}
+
+	client_request_add_child(parent, child);
+	return child;
+}
+
+static struct client_request *
+_client_ns_cmd_split_request(
+	struct spdk_client_qpair *qpair,
+	const struct client_payload *payload,
+	uint32_t payload_offset, uint32_t md_offset,
+	uint64_t lba, uint32_t lba_count,
+	spdk_req_cmd_cb cb_fn, void *cb_arg, uint32_t opc,
+	uint32_t io_flags, struct client_request *req,
+	uint32_t sectors_per_max_io, uint32_t sector_mask,
+	uint16_t apptag_mask, uint16_t apptag, int *rc)
+{
+	uint32_t sector_size = qpair->ctrlr->opts.sector_size;
+	uint32_t remaining_lba_count = lba_count;
+	struct client_request *child;
+
+	while (remaining_lba_count > 0)
+	{
+		lba_count = sectors_per_max_io - (lba & sector_mask);
+		lba_count = spdk_min(remaining_lba_count, lba_count);
+
+		child = _client_add_child_request(qpair, payload, payload_offset, md_offset,
+										  lba, lba_count, cb_fn, cb_arg, opc,
+										  io_flags, apptag_mask, apptag, req, true, rc);
+		if (child == NULL)
+		{
+			return NULL;
+		}
+
+		remaining_lba_count -= lba_count;
+		lba += lba_count;
+		payload_offset += lba_count * sector_size;
+		md_offset += lba_count * qpair->ctrlr->opts.md_size;
+	}
+
+	return req;
+}
+
+// FIXME:
+static inline bool
+_is_io_flags_valid(uint32_t io_flags)
+{
+
+	return true;
+}
+
+static void
+_client_ns_cmd_setup_request(struct client_request *req,
+							 uint32_t opc, uint64_t lba, uint32_t lba_count,
+							 uint32_t io_flags, uint16_t apptag_mask, uint16_t apptag)
+{
+	struct spdk_req_cmd *cmd;
+
+	assert(_is_io_flags_valid(io_flags));
+
+	cmd = &req->cmd;
+	cmd->opc = opc;
+
+	*(uint64_t *)&cmd->cdw10 = lba;
+
+	cmd->cdw12 = lba_count - 1;
+
+	cmd->cdw15 = apptag_mask;
+	cmd->cdw15 = (cmd->cdw15 << 16 | apptag);
+	if (opc == SPDK_CLIENT_OPC_RPC_READ || opc == SPDK_CLIENT_OPC_RPC_WRITE)
+	{
+		cmd->rsvd2 = req->payload.rpc_request_id;
+		cmd->rsvd3 = req->payload.data_length;
+		cmd->cdw13 = req->payload.submit_type;
+		cmd->rpc_opc = req->payload.rpc_opc;
+		if (req->payload.md5sum != NULL)
+		{
+			cmd->cdw14 = (uint32_t)1;
+			memcpy(cmd->md5sum, req->payload.md5sum, 16);
+		}
+	}
+}
+
+static struct client_request *
+_client_ns_cmd_split_request_prp(
+	struct spdk_client_qpair *qpair,
+	const struct client_payload *payload,
+	uint32_t payload_offset, uint32_t md_offset,
+	uint64_t lba, uint32_t lba_count,
+	spdk_req_cmd_cb cb_fn, void *cb_arg, uint32_t opc,
+	uint32_t io_flags, struct client_request *req,
+	uint16_t apptag_mask, uint16_t apptag, int *rc)
+{
+	spdk_client_req_reset_sgl_cb reset_sgl_fn = req->payload.reset_sgl_fn;
+	spdk_client_req_next_sge_cb next_sge_fn = req->payload.next_sge_fn;
+	void *sgl_cb_arg = req->payload.contig_or_cb_arg;
+	bool start_valid, end_valid, last_sge, child_equals_parent;
+	uint64_t child_lba = lba;
+	uint32_t req_current_length = 0;
+	uint32_t child_length = 0;
+	uint32_t sge_length;
+	uint32_t page_size = qpair->ctrlr->page_size;
+	uintptr_t address;
+
+	reset_sgl_fn(sgl_cb_arg, payload_offset);
+	next_sge_fn(sgl_cb_arg, (void **)&address, &sge_length);
+	while (req_current_length < req->payload_size)
+	{
+
+		if (sge_length == 0)
+		{
+			continue;
+		}
+		else if (req_current_length + sge_length > req->payload_size)
+		{
+			sge_length = req->payload_size - req_current_length;
+		}
+
+		/*
+		 * The start of the SGE is invalid if the start address is not page aligned,
+		 *  unless it is the first SGE in the child request.
+		 */
+		start_valid = child_length == 0 || _is_page_aligned(address, page_size);
+
+		/* Boolean for whether this is the last SGE in the parent request. */
+		last_sge = (req_current_length + sge_length == req->payload_size);
+
+		/*
+		 * The end of the SGE is invalid if the end address is not page aligned,
+		 *  unless it is the last SGE in the parent request.
+		 */
+		end_valid = last_sge || _is_page_aligned(address + sge_length, page_size);
+
+		/*
+		 * This child request equals the parent request, meaning that no splitting
+		 *  was required for the parent request (the one passed into this function).
+		 *  In this case, we do not create a child request at all - we just send
+		 *  the original request as a single request at the end of this function.
+		 */
+		child_equals_parent = (child_length + sge_length == req->payload_size);
+
+		if (start_valid)
+		{
+			/*
+			 * The start of the SGE is valid, so advance the length parameters,
+			 *  to include this SGE with previous SGEs for this child request
+			 *  (if any).  If it is not valid, we do not advance the length
+			 *  parameters nor get the next SGE, because we must send what has
+			 *  been collected before this SGE as a child request.
+			 */
+			child_length += sge_length;
+			req_current_length += sge_length;
+			if (req_current_length < req->payload_size)
+			{
+				next_sge_fn(sgl_cb_arg, (void **)&address, &sge_length);
+				/*
+				 * If the next SGE is not page aligned, we will need to create a
+				 *  child request for what we have so far, and then start a new
+				 *  child request for the next SGE.
+				 */
+				start_valid = _is_page_aligned(address, page_size);
+			}
+		}
+
+		if (start_valid && end_valid && !last_sge)
+		{
+			continue;
+		}
+
+		/*
+		 * We need to create a split here.  Send what we have accumulated so far as a child
+		 *  request.  Checking if child_equals_parent allows us to *not* create a child request
+		 *  when no splitting is required - in that case we will fall-through and just create
+		 *  a single request with no children for the entire I/O.
+		 */
+		if (!child_equals_parent)
+		{
+			struct client_request *child;
+			uint32_t child_lba_count;
+
+			if ((child_length % qpair->ctrlr->opts.extended_lba_size) != 0)
+			{
+				SPDK_ERRLOG("child_length %u not even multiple of lba_size %u\n",
+							child_length, qpair->ctrlr->opts.extended_lba_size);
+				*rc = -EINVAL;
+				return NULL;
+			}
+			child_lba_count = child_length / qpair->ctrlr->opts.extended_lba_size;
+			/*
+			 * Note the last parameter is set to "false" - this tells the recursive
+			 *  call to _client_ns_cmd_rw() to not bother with checking for SGL splitting
+			 *  since we have already verified it here.
+			 */
+			child = _client_add_child_request(qpair, payload, payload_offset, md_offset,
+											  child_lba, child_lba_count,
+											  cb_fn, cb_arg, opc, io_flags,
+											  apptag_mask, apptag, req, false, rc);
+			if (child == NULL)
+			{
+				return NULL;
+			}
+			payload_offset += child_length;
+			md_offset += child_lba_count * qpair->ctrlr->opts.md_size;
+			child_lba += child_lba_count;
+			child_length = 0;
+		}
+	}
+
+	if (child_length == req->payload_size)
+	{
+		/* No splitting was required, so setup the whole payload as one request. */
+		_client_ns_cmd_setup_request(req, opc, lba, lba_count, io_flags, apptag_mask, apptag);
+	}
+
+	return req;
+}
+
+static struct client_request *
+_client_ns_cmd_split_request_sgl(
+	struct spdk_client_qpair *qpair,
+	const struct client_payload *payload,
+	uint32_t payload_offset, uint32_t md_offset,
+	uint64_t lba, uint32_t lba_count,
+	spdk_req_cmd_cb cb_fn, void *cb_arg, uint32_t opc,
+	uint32_t io_flags, struct client_request *req,
+	uint16_t apptag_mask, uint16_t apptag, int *rc)
+{
+	spdk_client_req_reset_sgl_cb reset_sgl_fn = req->payload.reset_sgl_fn;
+	spdk_client_req_next_sge_cb next_sge_fn = req->payload.next_sge_fn;
+	void *sgl_cb_arg = req->payload.contig_or_cb_arg;
+	uint64_t child_lba = lba;
+	uint32_t req_current_length = 0;
+	uint32_t child_length = 0;
+	uint32_t sge_length;
+	uint16_t max_sges, num_sges;
+	uintptr_t address;
+
+	max_sges = qpair->ctrlr->max_sges;
+
+	reset_sgl_fn(sgl_cb_arg, payload_offset);
+	num_sges = 0;
+
+	while (req_current_length < req->payload_size)
+	{
+		next_sge_fn(sgl_cb_arg, (void **)&address, &sge_length);
+
+		if (req_current_length + sge_length > req->payload_size)
+		{
+			sge_length = req->payload_size - req_current_length;
+		}
+
+		child_length += sge_length;
+		req_current_length += sge_length;
+		num_sges++;
+
+		if (num_sges < max_sges && req_current_length < req->payload_size)
+		{
+			continue;
+		}
+
+		/*
+		 * We need to create a split here.  Send what we have accumulated so far as a child
+		 *  request.  Checking if the child equals the full payload allows us to *not*
+		 *  create a child request when no splitting is required - in that case we will
+		 *  fall-through and just create a single request with no children for the entire I/O.
+		 */
+		if (child_length != req->payload_size)
+		{
+			struct client_request *child;
+			uint32_t child_lba_count;
+
+			child_lba_count = SPDK_CEIL_DIV(child_length, qpair->ctrlr->opts.sector_size);
+			/*
+			 * Note the last parameter is set to "false" - this tells the recursive
+			 *  call to _client_ns_cmd_rw() to not bother with checking for SGL splitting
+			 *  since we have already verified it here.
+			 */
+			child = _client_add_child_request(qpair, payload, payload_offset, md_offset,
+											  child_lba, child_lba_count,
+											  cb_fn, cb_arg, opc, io_flags,
+											  apptag_mask, apptag, req, false, rc);
+			if (child == NULL)
+			{
+				return NULL;
+			}
+			payload_offset += child_length;
+			md_offset += child_lba_count * qpair->ctrlr->opts.md_size;
+			child_lba += child_lba_count;
+			child_length = 0;
+			num_sges = 0;
+		}
+	}
+
+	if (child_length == req->payload_size)
+	{
+		/* No splitting was required, so setup the whole payload as one request. */
+		_client_ns_cmd_setup_request(req, opc, lba, lba_count, io_flags, apptag_mask, apptag);
+	}
+
+	return req;
+}
+
+static inline struct client_request *
+_client_ns_cmd_rw(struct spdk_client_qpair *qpair,
+				  const struct client_payload *payload, uint32_t payload_offset, uint32_t md_offset,
+				  uint64_t lba, uint32_t lba_count, spdk_req_cmd_cb cb_fn, void *cb_arg, uint32_t opc,
+				  uint32_t io_flags, uint16_t apptag_mask, uint16_t apptag, bool check_sgl, int *rc)
+{
+	struct client_request *req;
+	uint32_t sector_size = qpair->ctrlr->opts.sector_size;
+	uint32_t sectors_per_max_io = qpair->ctrlr->opts.sectors_per_max_io;
+	uint32_t sectors_per_stripe = qpair->ctrlr->opts.sectors_per_stripe;
+
+	assert(rc != NULL);
+	assert(*rc == 0);
+
+	uint32_t max_payload_size = lba_count * sector_size;
+	max_payload_size = spdk_min(max_payload_size, payload->data_length - payload_offset);
+	req = client_allocate_request(qpair, payload, max_payload_size, lba_count * qpair->ctrlr->opts.md_size,
+								  cb_fn, cb_arg);
+	if (req == NULL)
+	{
+		*rc = -ENOMEM;
+		return NULL;
+	}
+
+	req->payload_offset = payload_offset;
+	req->md_offset = md_offset;
+
+	/*
+	 * Intel DC P3*00 Client controllers benefit from driver-assisted striping.
+	 * If this controller defines a stripe boundary and this I/O spans a stripe
+	 *  boundary, split the request into multiple requests and submit each
+	 *  separately to hardware.
+	 */
+	if (sectors_per_stripe > 0 &&
+		(((lba & (sectors_per_stripe - 1)) + lba_count) > sectors_per_stripe))
+	{
+
+		return _client_ns_cmd_split_request(qpair, payload, payload_offset, md_offset, lba, lba_count,
+											cb_fn,
+											cb_arg, opc,
+											io_flags, req, sectors_per_stripe, sectors_per_stripe - 1, apptag_mask, apptag, rc);
+	}
+	else if (lba_count > sectors_per_max_io)
+	{
+		return _client_ns_cmd_split_request(qpair, payload, payload_offset, md_offset, lba, lba_count,
+											cb_fn,
+											cb_arg, opc,
+											io_flags, req, sectors_per_max_io, 0, apptag_mask, apptag, rc);
+	}
+	else if (client_payload_type(&req->payload) == CLIENT_PAYLOAD_TYPE_SGL && check_sgl)
+	{
+		return _client_ns_cmd_split_request_sgl(qpair, payload, payload_offset, md_offset,
+												lba, lba_count, cb_fn, cb_arg, opc, io_flags,
+												req, apptag_mask, apptag, rc);
+	}
+
+	_client_ns_cmd_setup_request(req, opc, lba, lba_count, io_flags, apptag_mask, apptag);
+	return req;
+}
+
+int spdk_client_ns_cmd_read(struct spdk_client_qpair *qpair, void *buffer,
+							uint64_t lba,
+							uint32_t lba_count, spdk_req_cmd_cb cb_fn, void *cb_arg,
+							uint32_t io_flags)
+{
+	struct client_request *req;
+	struct client_payload payload;
+	int rc = 0;
+
+	if (!_is_io_flags_valid(io_flags))
+	{
+		return -EINVAL;
+	}
+
+	payload = CLIENT_PAYLOAD_CONTIG(buffer, NULL);
+
+	req = _client_ns_cmd_rw(qpair, &payload, 0, 0, lba, lba_count, cb_fn, cb_arg, SPDK_CLIENT_OPC_READ,
+							io_flags, 0,
+							0, false, &rc);
+	if (req != NULL)
+	{
+		return client_qpair_submit_request(qpair, req);
+	}
+	else
+	{
+		return client_ns_map_failure_rc(lba_count,
+										qpair->ctrlr->opts.sectors_per_max_io,
+										qpair->ctrlr->opts.sectors_per_stripe,
+										qpair->ctrlr->opts.io_queue_requests,
+										rc);
+	}
+}
+
+int spdk_client_ns_cmd_readv(struct spdk_client_qpair *qpair,
+							 uint64_t lba, uint32_t lba_count,
+							 spdk_req_cmd_cb cb_fn, void *cb_arg, uint32_t io_flags,
+							 spdk_client_req_reset_sgl_cb reset_sgl_fn,
+							 spdk_client_req_next_sge_cb next_sge_fn)
+{
+	struct client_request *req;
+	struct client_payload payload;
+	int rc = 0;
+
+	if (!_is_io_flags_valid(io_flags))
+	{
+		return -EINVAL;
+	}
+
+	if (reset_sgl_fn == NULL || next_sge_fn == NULL)
+	{
+		return -EINVAL;
+	}
+
+	payload = CLIENT_PAYLOAD_SGL(reset_sgl_fn, next_sge_fn, cb_arg, NULL, 0, 0, 0, 0, NULL);
+
+	req = _client_ns_cmd_rw(qpair, &payload, 0, 0, lba, lba_count, cb_fn, cb_arg, SPDK_CLIENT_OPC_READ,
+							io_flags, 0, 0, true, &rc);
+	if (req != NULL)
+	{
+		return client_qpair_submit_request(qpair, req);
+	}
+	else
+	{
+		return client_ns_map_failure_rc(lba_count,
+										qpair->ctrlr->opts.sectors_per_max_io,
+										qpair->ctrlr->opts.sectors_per_stripe,
+										qpair->ctrlr->opts.io_queue_requests,
+										rc);
+	}
+}
+
+int spdk_client_ns_cmd_write(struct spdk_client_qpair *qpair,
+							 void *buffer, uint64_t lba,
+							 uint32_t lba_count, spdk_req_cmd_cb cb_fn, void *cb_arg,
+							 uint32_t io_flags)
+{
+	struct client_request *req;
+	struct client_payload payload;
+	int rc = 0;
+
+	payload = CLIENT_PAYLOAD_CONTIG(buffer, NULL);
+
+	req = _client_ns_cmd_rw(qpair, &payload, 0, 0, lba, lba_count, cb_fn, cb_arg, SPDK_CLIENT_OPC_WRITE,
+							io_flags, 0, 0, false, &rc);
+	if (req != NULL)
+	{
+		return client_qpair_submit_request(qpair, req);
+	}
+	else
+	{
+		return client_ns_map_failure_rc(lba_count,
+										qpair->ctrlr->opts.sectors_per_max_io,
+										qpair->ctrlr->opts.sectors_per_stripe,
+										qpair->ctrlr->opts.io_queue_requests,
+										rc);
+	}
+}
+
+int spdk_client_ns_cmd_write_with_md(struct spdk_client_qpair *qpair,
+									 void *buffer, void *metadata, uint64_t lba,
+									 uint32_t lba_count, spdk_req_cmd_cb cb_fn, void *cb_arg,
+									 uint32_t io_flags, uint16_t apptag_mask, uint16_t apptag)
+{
+	struct client_request *req;
+	struct client_payload payload;
+	int rc = 0;
+
+	if (!_is_io_flags_valid(io_flags))
+	{
+		return -EINVAL;
+	}
+
+	payload = CLIENT_PAYLOAD_CONTIG(buffer, metadata);
+
+	req = _client_ns_cmd_rw(qpair, &payload, 0, 0, lba, lba_count, cb_fn, cb_arg, SPDK_CLIENT_OPC_WRITE,
+							io_flags, apptag_mask, apptag, false, &rc);
+	if (req != NULL)
+	{
+		return client_qpair_submit_request(qpair, req);
+	}
+	else
+	{
+		return client_ns_map_failure_rc(lba_count,
+										qpair->ctrlr->opts.sectors_per_max_io,
+										qpair->ctrlr->opts.sectors_per_stripe,
+										qpair->ctrlr->opts.io_queue_requests,
+										rc);
+	}
+}
+
+int spdk_client_ns_cmd_writev(struct spdk_client_qpair *qpair,
+							  uint64_t lba, uint32_t lba_count,
+							  spdk_req_cmd_cb cb_fn, void *cb_arg, uint32_t io_flags,
+							  spdk_client_req_reset_sgl_cb reset_sgl_fn,
+							  spdk_client_req_next_sge_cb next_sge_fn)
+{
+	struct client_request *req;
+	struct client_payload payload;
+	int rc = 0;
+
+	if (reset_sgl_fn == NULL || next_sge_fn == NULL)
+	{
+		return -EINVAL;
+	}
+
+	payload = CLIENT_PAYLOAD_SGL(reset_sgl_fn, next_sge_fn, cb_arg, NULL, 0, 0, 0, 0, NULL);
+
+	req = _client_ns_cmd_rw(qpair, &payload, 0, 0, lba, lba_count, cb_fn, cb_arg, SPDK_CLIENT_OPC_WRITE,
+							io_flags, 0, 0, true, &rc);
+	if (req != NULL)
+	{
+		return client_qpair_submit_request(qpair, req);
+	}
+	else
+	{
+		return client_ns_map_failure_rc(lba_count,
+										qpair->ctrlr->opts.sectors_per_max_io,
+										qpair->ctrlr->opts.sectors_per_stripe,
+										qpair->ctrlr->opts.io_queue_requests,
+										rc);
+	}
+}
+
+int spdk_client_ns_cmd_flush(struct spdk_client_qpair *qpair,
+							 spdk_req_cmd_cb cb_fn, void *cb_arg)
+{
+	struct client_request *req;
+	struct spdk_req_cmd *cmd;
+
+	req = client_allocate_request_null(qpair, cb_fn, cb_arg);
+	if (req == NULL)
+	{
+		return -ENOMEM;
+	}
+
+	cmd = &req->cmd;
+	cmd->opc = SPDK_CLIENT_OPC_FLUSH;
+
+	return client_qpair_submit_request(qpair, req);
+}
+
+void rpc_reclaim_out_iovs(struct rpc_request *req)
+{
+	struct iovec *iov;
+	int iovpos = 0;
+	for (int i = 0; i < req->out_iovcnt; i++)
+	{
+		iov = &req->out_iovs[i];
+		if (iov->iov_base != NULL)
+		{
+			spdk_mempool_put(req->qpair->ctrlr->rpc_data_mp, iov->iov_base);
+		}
+		else
+		{
+			break;
+		}
+	}
+	return;
+}
+
+void rpc_reclaim_in_iovs(struct rpc_request *req)
+{
+	struct iovec *iov;
+	int iovpos = 0;
+	for (int i = 0; i < req->in_iovcnt; i++)
+	{
+		iov = &req->in_iovs[i];
+		if (iov->iov_base != NULL)
+		{
+			spdk_mempool_put(req->qpair->ctrlr->rpc_data_mp, iov->iov_base);
+		}
+		else
+		{
+			break;
+		}
+	}
+	return;
+}
+
+int rpc_prepare_out_iovs(struct rpc_request *req)
+{
+	void *raw_data = req->raw_data;
+	uint32_t length = req->out_length;
+	int iovpos = 0;
+
+	// used when submit_type == SPDK_CLIENT_SUBMIT_IOVES
+	int iovpos_dst = 0;
+	int iovpos_src = 0;
+	int offset_dst = 0;
+	int offset_src = 0;
+	int iov_remain_length_dst = 0;
+	int iov_remain_length_src = 0;
+	struct iovec *iov_src;
+	struct iovec *iov_dst;
+	int copy_len = 0;
+	int copy_len_total = 0;
+
+	struct iovec *iov;
+	uint32_t offset = 0;
+	int sectors = 0;
+	char *addr = 0;
+	uint64_t io_unit_size = req->qpair->ctrlr->io_unit_size;
+	uint64_t sector_size = req->qpair->ctrlr->opts.sector_size;
+	req->out_iovcnt = SPDK_CEIL_DIV(length, io_unit_size);
+	req->out_iovs = calloc(req->out_iovcnt, sizeof(struct iovec));
+	if (!req->out_iovs)
+	{
+		return -1;
+	}
+	while (length > 0)
+	{
+		if (req->submit_type == SPDK_CLIENT_SUBMIT_CONTING)
+		{
+			iov = &req->out_iovs[iovpos];
+			iov->iov_len = spdk_min(length, io_unit_size);
+			addr = req->raw_data + offset;
+			iov->iov_base = spdk_mempool_get(req->qpair->ctrlr->rpc_data_mp);
+			if (iov->iov_base == NULL)
+			{
+				rpc_reclaim_out_iovs(req);
+				free(req->out_iovs);
+				return -1;
+			}
+			memcpy(iov->iov_base, addr, iov->iov_len);
+			copy_len_total = copy_len_total + iov->iov_len;
+			length -= iov->iov_len;
+			offset += iov->iov_len;
+			iovpos++;
+		}
+		else
+		{
+			iov_dst = &req->out_iovs[iovpos_dst];
+			iov_dst->iov_len = spdk_min(length, io_unit_size);
+			iov_dst->iov_base = spdk_mempool_get(req->qpair->ctrlr->rpc_data_mp);
+			if (iov_dst->iov_base == NULL)
+			{
+				rpc_reclaim_out_iovs(req);
+				free(req->out_iovs);
+				return -1;
+			}
+			iov_remain_length_dst = iov_dst->iov_len - offset_dst;
+			while (iov_remain_length_dst > 0)
+			{
+				iov_src = &req->raw_ioves[iovpos_src];
+				iov_remain_length_src = iov_src->iov_len - offset_src;
+				copy_len = spdk_min(iov_remain_length_dst, iov_remain_length_src);
+				memcpy(iov_dst->iov_base + offset_dst, iov_src->iov_base + offset_src, copy_len);
+				copy_len_total = copy_len_total + copy_len;
+				offset_dst = offset_dst + copy_len;
+				offset_src = offset_src + copy_len;
+				iov_remain_length_dst = iov_remain_length_dst - copy_len;
+				iov_remain_length_src = iov_remain_length_src - copy_len;
+				length = length - copy_len;
+				if (iov_remain_length_dst == 0)
+				{
+					iovpos_dst++;
+					offset_dst = 0;
+				}
+				else if (iov_remain_length_src == 0)
+				{
+					iovpos_src++;
+					offset_src = 0;
+				}
+				else
+				{
+					assert(iov_remain_length_dst == 0 || iov_remain_length_src == 0);
+					SPDK_ERRLOG("rpc_prepare_out_iovs HIT CRITIAL ERROR\n");
+				}
+			}
+		}
+	}
+	assert(copy_len_total == req->out_length);
+
+	for (int i = 0; i < req->out_iovcnt; i++)
+	{
+		req->out_payload_length += req->out_iovs[i].iov_len;
+	}
+	SPDK_DEBUGLOG(rdma, "req->out_payload_length=%d, iov_cnt=%d\n", req->out_payload_length, req->out_iovcnt);
+	return 0;
+}
+
+int rpc_prepare_in_iovs(struct rpc_request *req)
+{
+	void *raw_data = req->raw_data;
+	uint32_t length = req->in_length;
+	int iovpos = 0;
+	struct iovec *iov;
+	uint64_t io_unit_size = req->qpair->ctrlr->io_unit_size;
+	uint64_t sector_size = req->qpair->ctrlr->opts.sector_size;
+	int sectors;
+	req->in_iovcnt = SPDK_CEIL_DIV(length, io_unit_size);
+	req->in_iovs = calloc(req->in_iovcnt, sizeof(struct iovec));
+	if (!req->in_iovs)
+	{
+		return -EAGAIN;
+	}
+	while (length > 0)
+	{
+		iov = &req->in_iovs[iovpos];
+		iov->iov_len = spdk_min(length, io_unit_size);
+		iov->iov_base = spdk_mempool_get(req->qpair->ctrlr->rpc_data_mp);
+		if (iov->iov_base == NULL)
+		{
+			rpc_reclaim_in_iovs(req);
+			free(req->in_iovs);
+			return -EAGAIN;
+		}
+		length -= iov->iov_len;
+		iovpos++;
+	}
+
+	for (int i = 0; i < req->in_iovcnt; i++)
+	{
+		req->in_payload_length += req->in_iovs[i].iov_len;
+	}
+	SPDK_DEBUGLOG(rdma, "req->in_payload_length=%d, iov_cnt=%d\n", req->in_payload_length, req->in_iovcnt);
+
+	return 0;
+}
+
+static int md5init(struct spdk_md5ctx *md5ctx)
+{
+	int rc;
+
+	if (md5ctx == NULL)
+	{
+		return -1;
+	}
+
+	md5ctx->md5ctx = EVP_MD_CTX_create();
+	if (md5ctx->md5ctx == NULL)
+	{
+		return -1;
+	}
+
+	rc = EVP_DigestInit_ex(md5ctx->md5ctx, EVP_md5(), NULL);
+	/* For EVP_DigestInit_ex, 1 == success, 0 == failure. */
+	if (rc == 0)
+	{
+		EVP_MD_CTX_destroy(md5ctx->md5ctx);
+		md5ctx->md5ctx = NULL;
+	}
+	return rc;
+}
+
+static int md5final(void *md5, struct spdk_md5ctx *md5ctx)
+{
+	int rc;
+
+	if (md5ctx == NULL || md5 == NULL)
+	{
+		return -1;
+	}
+	rc = EVP_DigestFinal_ex(md5ctx->md5ctx, (unsigned char *)md5, NULL);
+	EVP_MD_CTX_destroy(md5ctx->md5ctx);
+	md5ctx->md5ctx = NULL;
+	return rc;
+}
+
+static int md5update(struct spdk_md5ctx *md5ctx, const void *data, size_t len)
+{
+	int rc;
+
+	if (md5ctx == NULL)
+	{
+		return -1;
+	}
+	if (data == NULL || len == 0)
+	{
+		return 0;
+	}
+	rc = EVP_DigestUpdate(md5ctx->md5ctx, data, len);
+	return rc;
+}
+
+void spdk_client_reclaim_rpc_request(struct rpc_request *req)
+{
+	rpc_reclaim_in_iovs(req);
+	free(req->in_iovs);
+	STAILQ_INSERT_TAIL(&req->qpair->free_rpc_req, req, stailq);
+}
+
+static void
+rpc_request_reset_out_sgl(void *ref, uint32_t sgl_offset)
+{
+	struct iovec *iov;
+	struct rpc_request *req = (struct rpc_request *)ref;
+
+	req->iov_offset = sgl_offset;
+	for (req->iovpos = 0; req->iovpos < req->out_iovcnt; req->iovpos++)
+	{
+		iov = &req->out_iovs[req->iovpos];
+		if (req->iov_offset < iov->iov_len)
+		{
+			break;
+		}
+
+		req->iov_offset -= iov->iov_len;
+	}
+}
+
+static int
+rpc_request_next_out_sge(void *ref, void **address, uint32_t *length)
+{
+	struct iovec *iov;
+	struct rpc_request *req = (struct rpc_request *)ref;
+	SPDK_DEBUGLOG(rdma, "rpc_request_next_out_sge iovpos:%d, out_iovcnt:%d\n", req->iovpos, req->out_iovcnt);
+	assert(req->iovpos < req->out_iovcnt);
+
+	iov = &req->out_iovs[req->iovpos];
+	assert(req->iov_offset <= iov->iov_len);
+
+	*address = iov->iov_base + req->iov_offset;
+	*length = iov->iov_len - req->iov_offset;
+	req->iovpos++;
+	req->iov_offset = 0;
+
+	return 0;
+}
+
+static void
+rpc_request_reset_in_sgl(void *ref, uint32_t sgl_offset)
+{
+	struct iovec *iov;
+	struct rpc_request *req = (struct rpc_request *)ref;
+
+	req->iov_offset = sgl_offset;
+	for (req->iovpos = 0; req->iovpos < req->in_iovcnt; req->iovpos++)
+	{
+		iov = &req->in_iovs[req->iovpos];
+		if (req->iov_offset < iov->iov_len)
+		{
+			break;
+		}
+
+		req->iov_offset -= iov->iov_len;
+	}
+}
+
+static int
+rpc_request_next_in_sge(void *ref, void **address, uint32_t *length)
+{
+	struct iovec *iov;
+	struct rpc_request *req = (struct rpc_request *)ref;
+
+	assert(req->iovpos < req->in_iovcnt);
+
+	iov = &req->in_iovs[req->iovpos];
+	assert(req->iov_offset <= iov->iov_len);
+
+	*address = iov->iov_base + req->iov_offset;
+	*length = iov->iov_len - req->iov_offset;
+	req->iovpos++;
+	req->iov_offset = 0;
+
+	return 0;
+}
+
+int spdk_client_rpc_request_write(struct spdk_client_qpair *qpair,
+								  uint64_t lba, uint32_t lba_count, uint32_t rpc_request_id, uint32_t data_length,
+								  uint32_t rpc_opc,
+								  uint32_t submit_type,
+								  uint8_t *md5sum,
+								  spdk_req_cmd_cb cb_fn, void *cb_arg, uint32_t io_flags,
+								  spdk_client_req_reset_sgl_cb reset_sgl_fn,
+								  spdk_client_req_next_sge_cb next_sge_fn)
+{
+	struct client_request *req;
+	struct client_payload payload;
+	int rc = 0;
+
+	if (reset_sgl_fn == NULL || next_sge_fn == NULL)
+	{
+		return -EINVAL;
+	}
+
+	payload = CLIENT_PAYLOAD_SGL(reset_sgl_fn, next_sge_fn, cb_arg, NULL, rpc_request_id, data_length, rpc_opc, submit_type, md5sum);
+	SPDK_DEBUGLOG(rdma, "spdk_client_rpc_request_write send parent request lba_start=%d, lba_end=%d\n", 0, lba_count - 1);
+	req = _client_ns_cmd_rw(qpair, &payload, 0, 0, 0, lba_count, cb_fn, cb_arg, SPDK_CLIENT_OPC_RPC_WRITE,
+							io_flags, 0, 0, true, &rc);
+	if (req != NULL)
+	{
+		return client_qpair_submit_request(qpair, req);
+	}
+	else
+	{
+		return client_ns_map_failure_rc(lba_count,
+										qpair->ctrlr->opts.sectors_per_max_io,
+										qpair->ctrlr->opts.sectors_per_stripe,
+										qpair->ctrlr->opts.io_queue_requests,
+										rc);
+	}
+}
+
+int spdk_client_rpc_request_read(struct spdk_client_qpair *qpair,
+								 uint64_t lba, uint32_t lba_count, uint32_t rpc_request_id, uint32_t data_length,
+								 uint32_t rpc_opc,
+								 uint32_t submit_type,
+								 spdk_req_cmd_cb cb_fn, void *cb_arg, uint32_t io_flags,
+								 spdk_client_req_reset_sgl_cb reset_sgl_fn,
+								 spdk_client_req_next_sge_cb next_sge_fn)
+{
+	struct client_request *req;
+	struct client_payload payload;
+	int rc = 0;
+
+	if (!_is_io_flags_valid(io_flags))
+	{
+		return -EINVAL;
+	}
+
+	if (reset_sgl_fn == NULL || next_sge_fn == NULL)
+	{
+		return -EINVAL;
+	}
+
+	payload = CLIENT_PAYLOAD_SGL(reset_sgl_fn, next_sge_fn, cb_arg, NULL, rpc_request_id, data_length, rpc_opc, submit_type, NULL);
+
+	req = _client_ns_cmd_rw(qpair, &payload, 0, 0, lba, lba_count, cb_fn, cb_arg, SPDK_CLIENT_OPC_RPC_READ,
+							io_flags, 0, 0, true, &rc);
+	if (req != NULL)
+	{
+		return client_qpair_submit_request(qpair, req);
+	}
+	else
+	{
+		return client_ns_map_failure_rc(lba_count,
+										qpair->ctrlr->opts.sectors_per_max_io,
+										qpair->ctrlr->opts.sectors_per_stripe,
+										qpair->ctrlr->opts.io_queue_requests,
+										rc);
+	}
+}
+
+void rpc_read_cb(void *ctx, const struct spdk_req_cpl *cpl)
+{
+	int ret = 0;
+	struct spdk_md5ctx md5ctx;
+	uint8_t md5sum[SPDK_MD5DIGEST_LEN];
+	// TODO: check rpc status,  maybe use sqhd field
+	int status = cpl->cdw0;
+	struct rpc_request *req = (struct rpc_request *)ctx;
+	int md5_batch_len = 0;
+	int md5_total_len = req->in_length;
+	if (req->check_md5)
+	{
+		md5init(&md5ctx);
+		for (int i = 0; i < req->in_iovcnt; i++)
+		{
+			md5_batch_len = spdk_min(req->in_iovs[i].iov_len, md5_total_len);
+			md5update(&md5ctx, req->in_iovs[i].iov_base, md5_batch_len);
+			md5_total_len -= md5_batch_len;
+			SPDK_DEBUGLOG(rdma, "checking md5sum iov_len:%d, md5_total_len:%d\n", req->in_iovs[i].iov_len, md5_total_len);
+		}
+		assert(md5_total_len == 0);
+		md5final(md5sum, &md5ctx);
+		for (int i = 0; i < SPDK_MD5DIGEST_LEN; i++)
+		{
+			assert(md5sum[i] == cpl->md5sum[i]);
+			SPDK_DEBUGLOG(rdma, "check md5sum compare caled:%d, receved:%d\n", md5sum[i], cpl->md5sum[i]);
+		}
+		SPDK_DEBUGLOG(rdma, "check md5sum success\n");
+	}
+
+	req->cb(req->cb_args, status, req->in_iovs, req->in_iovcnt, req->in_length);
+	rpc_reclaim_in_iovs(req);
+	free(req->in_iovs);
+	STAILQ_INSERT_TAIL(&req->qpair->free_rpc_req, req, stailq);
+
+	return;
+}
+
+void rpc_write_cb(void *ctx, const struct spdk_req_cpl *cpl)
+{
+	int ret = 0;
+	int lba_count = 0;
+	uint32_t required_data_length = cpl->cdw1;
+	int status = cpl->cdw0;
+	struct rpc_request *req = (struct rpc_request *)ctx;
+	if (required_data_length == 0)
+	{
+		req->cb(req->cb_args, status, NULL, 0, 0);
+		return;
+	}
+	assert(required_data_length != 0);
+
+	uint32_t sector_size = req->qpair->ctrlr->opts.sector_size;
+	// free out_iovs now to save memory
+	rpc_reclaim_out_iovs(req);
+	free(req->out_iovs);
+
+	req->in_length = required_data_length;
+	if (rpc_prepare_in_iovs(req) == -EAGAIN)
+	{
+		SPDK_ERRLOG("rpc_write_cb get buffer failed %d\n", required_data_length);
+		STAILQ_INSERT_TAIL(&req->qpair->ctrlr->pending_rpc_requests, req, stailq);
+		return;
+	}
+
+	lba_count = SPDK_CEIL_DIV(req->in_payload_length, sector_size);
+	ret = spdk_client_rpc_request_read(req->qpair, 0, lba_count, req->request_id, req->in_length, req->opc, req->submit_type, rpc_read_cb, req, 0, rpc_request_reset_in_sgl, rpc_request_next_in_sge);
+	if (ret != 0)
+	{
+		SPDK_ERRLOG("spdk_client_rpc_request_read failed %d\n", ret);
+		// TODO: 关闭这个qpair
+		assert(0);
+	}
+	return;
+}
+int spdk_client_submit_rpc_request(struct spdk_client_qpair *qpair, uint32_t opc, char *raw_data, uint32_t length,
+								   spdk_rpc_request_cb cb_fn, void *cb_arg, bool chek_md5)
+{
+	struct rpc_request *req;
+	struct spdk_md5ctx md5ctx;
+	int lba_count = 0;
+	req = STAILQ_FIRST(&qpair->free_rpc_req);
+	if (req == NULL)
+	{
+		printf("no enough rpc req\n");
+		return -EAGAIN;
+	}
+	STAILQ_REMOVE_HEAD(&qpair->free_rpc_req, stailq);
+	memset(req, 0, offsetof(struct rpc_request, request_id));
+
+	req->cb = cb_fn;
+	req->cb_args = cb_arg;
+	req->qpair = qpair;
+	req->raw_data = raw_data;
+	req->submit_type = SPDK_CLIENT_SUBMIT_CONTING;
+	req->out_length = length;
+	req->opc = opc;
+	req->qpair = qpair;
+	req->tsc_last = spdk_get_ticks();
+
+	if (rpc_prepare_out_iovs(req) != 0)
+	{
+		STAILQ_INSERT_HEAD(&qpair->free_rpc_req, req, stailq);
+		return NULL;
+	}
+
+	if (chek_md5)
+	{
+		req->check_md5 = true;
+		md5init(&md5ctx);
+		md5update(&md5ctx, raw_data, length);
+		md5final(req->md5sum, &md5ctx);
+	}
+	else
+	{
+		req->check_md5 = false;
+	}
+
+	uint32_t sector_size = req->qpair->ctrlr->opts.sector_size;
+	lba_count = SPDK_CEIL_DIV(req->out_payload_length, sector_size);
+	uint8_t *md5sum = NULL;
+	if (req->check_md5)
+	{
+		md5sum = req->md5sum;
+	}
+	return spdk_client_rpc_request_write(req->qpair, 0, lba_count, req->request_id, req->out_length, req->opc, req->submit_type, md5sum, rpc_write_cb, req, 0, rpc_request_reset_out_sgl, rpc_request_next_out_sge);
+}
+
+int spdk_client_empty_free_request(struct spdk_client_qpair *qpair) {
+	return STAILQ_EMPTY(&qpair->free_rpc_req);
+}
+
+int spdk_client_submit_rpc_request_iovs_directly(struct spdk_client_qpair *qpair, struct iovec *out_ioves, int out_iov_cnt, uint32_t length, spdk_rpc_request_cb cb_fn, void *cb_arg)
+{
+	struct rpc_request *req;
+	int lba_count = 0;
+	req = STAILQ_FIRST(&qpair->free_rpc_req);
+	if (req == NULL)
+	{
+		printf("no enough rpc req\n");
+		return -EAGAIN;
+	}
+
+	STAILQ_REMOVE_HEAD(&qpair->free_rpc_req, stailq);
+	memset(req, 0, offsetof(struct rpc_request, request_id));
+
+	req->cb = cb_fn;
+	req->cb_args = cb_arg;
+	req->qpair = qpair;
+	req->submit_type = SPDK_CLIENT_SUBMIT_IOVES;
+	req->qpair = qpair;
+	req->out_iovcnt = out_iov_cnt;
+	req->out_iovs = out_ioves;
+	req->out_payload_length = length;
+	req->check_md5 = false;
+	req->tsc_last = spdk_get_ticks();
+
+	SPDK_DEBUGLOG(rdma, "req->out_payload_length=%d, iov_cnt=%d\n", req->out_payload_length, req->out_iovcnt);
+
+	uint32_t sector_size = req->qpair->ctrlr->opts.sector_size;
+	lba_count = SPDK_CEIL_DIV(req->out_payload_length, sector_size);
+
+	return spdk_client_rpc_request_write(req->qpair, 0, lba_count, req->request_id, req->out_payload_length, req->opc, req->submit_type, NULL, rpc_write_cb, req, 0, rpc_request_reset_out_sgl, rpc_request_next_out_sge);
+}
+
+int spdk_client_submit_rpc_request_iovs(struct spdk_client_qpair *qpair, uint32_t opc, struct iovec *raw_ioves, int raw_iov_cnt, uint32_t length,
+										spdk_rpc_request_cb cb_fn, void *cb_arg, bool chek_md5)
+{
+	struct rpc_request *req;
+	struct spdk_md5ctx md5ctx;
+	int lba_count = 0;
+	req = STAILQ_FIRST(&qpair->free_rpc_req);
+	if (req == NULL)
+	{
+		printf("no enough rpc req\n");
+		return -EAGAIN;
+	}
+	STAILQ_REMOVE_HEAD(&qpair->free_rpc_req, stailq);
+	memset(req, 0, offsetof(struct rpc_request, request_id));
+
+	req->cb = cb_fn;
+	req->cb_args = cb_arg;
+	req->qpair = qpair;
+	req->raw_ioves = raw_ioves;
+	req->raw_iov_cnt = raw_iov_cnt;
+	req->submit_type = SPDK_CLIENT_SUBMIT_IOVES;
+	req->out_length = length;
+	req->opc = opc;
+	req->qpair = qpair;
+	req->tsc_last = spdk_get_ticks();
+
+	if (rpc_prepare_out_iovs(req) != 0)
+	{
+		STAILQ_INSERT_HEAD(&qpair->free_rpc_req, req, stailq);
+		return NULL;
+	}
+
+	if (chek_md5)
+	{
+		req->check_md5 = true;
+		md5init(&md5ctx);
+		for (int i = 0; i < raw_iov_cnt; i++)
+		{
+			md5update(&md5ctx, raw_ioves[i].iov_base, raw_ioves[i].iov_len);
+		}
+		md5final(req->md5sum, &md5ctx);
+	}
+	else
+	{
+		req->check_md5 = false;
+	}
+
+	uint32_t sector_size = req->qpair->ctrlr->opts.sector_size;
+	lba_count = SPDK_CEIL_DIV(req->out_payload_length, sector_size);
+	uint8_t *md5sum = NULL;
+	if (req->check_md5)
+	{
+		md5sum = req->md5sum;
+	}
+	return spdk_client_rpc_request_write(req->qpair, 0, lba_count, req->request_id, req->out_length, req->opc, req->submit_type, md5sum, rpc_write_cb, req, 0, rpc_request_reset_out_sgl, rpc_request_next_out_sge);
+}
\ No newline at end of file
diff --git a/lib/rdma_server/conn.c b/lib/rdma_server/conn.c
new file mode 100644
index 000000000..b96e9604e
--- /dev/null
+++ b/lib/rdma_server/conn.c
@@ -0,0 +1,1122 @@
+/*-
+ *   BSD LICENSE
+ *
+ *   Copyright (c) Intel Corporation.
+ *   All rights reserved.
+ *   Copyright (c) 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ *
+ *   Redistribution and use in source and binary forms, with or without
+ *   modification, are permitted provided that the following conditions
+ *   are met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in
+ *       the documentation and/or other materials provided with the
+ *       distribution.
+ *     * Neither the name of Intel Corporation nor the names of its
+ *       contributors may be used to endorse or promote products derived
+ *       from this software without specific prior written permission.
+ *
+ *   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ *   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ *   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ *   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ *   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ *   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ *   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ *   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ *   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ *   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ *   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "spdk_internal/rdma_client.h"
+#include "spdk/rdma_client.h"
+#include "spdk/string.h"
+
+#define CLIENT_CMD_DPTR_STR_SIZE 256
+
+static int client_qpair_resubmit_request(struct spdk_client_qpair *qpair, struct client_request *req);
+
+struct client_string
+{
+	uint16_t value;
+	const char *str;
+};
+
+static const struct client_string io_opcode[] = {
+	{SPDK_CLIENT_OPC_FLUSH, "FLUSH"},
+	{SPDK_CLIENT_OPC_WRITE, "WRITE"},
+	{SPDK_CLIENT_OPC_READ, "READ"},
+	{SPDK_CLIENT_OPC_RPC_WRITE, "RPC WRITE"},
+	{SPDK_CLIENT_OPC_RPC_READ, "RPC READ"},
+
+	{0xFFFF, "IO COMMAND"}};
+
+static const struct client_string sgl_type[] = {
+	{SPDK_CLIENT_SGL_TYPE_DATA_BLOCK, "DATA BLOCK"},
+	{SPDK_CLIENT_SGL_TYPE_BIT_BUCKET, "BIT BUCKET"},
+	{SPDK_CLIENT_SGL_TYPE_SEGMENT, "SEGMENT"},
+	{SPDK_CLIENT_SGL_TYPE_LAST_SEGMENT, "LAST SEGMENT"},
+	{SPDK_CLIENT_SGL_TYPE_TRANSPORT_DATA_BLOCK, "TRANSPORT DATA BLOCK"},
+	{SPDK_CLIENT_SGL_TYPE_VENDOR_SPECIFIC, "VENDOR SPECIFIC"},
+	{0xFFFF, "RESERVED"}};
+
+static const struct client_string sgl_subtype[] = {
+	{SPDK_CLIENT_SGL_SUBTYPE_ADDRESS, "ADDRESS"},
+	{SPDK_CLIENT_SGL_SUBTYPE_OFFSET, "OFFSET"},
+	{SPDK_CLIENT_SGL_SUBTYPE_TRANSPORT, "TRANSPORT"},
+	{SPDK_CLIENT_SGL_SUBTYPE_INVALIDATE_KEY, "INVALIDATE KEY"},
+	{0xFFFF, "RESERVED"}};
+
+static const char *
+client_get_string(const struct client_string *strings, uint16_t value)
+{
+	const struct client_string *entry;
+
+	entry = strings;
+
+	while (entry->value != 0xFFFF)
+	{
+		if (entry->value == value)
+		{
+			return entry->str;
+		}
+		entry++;
+	}
+	return entry->str;
+}
+
+static void
+client_get_sgl_unkeyed(char *buf, size_t size, struct spdk_req_cmd *cmd)
+{
+	struct spdk_req_sgl_descriptor *sgl = &cmd->dptr.sgl1;
+
+	snprintf(buf, size, " len:0x%x", sgl->unkeyed.length);
+}
+
+static void
+client_get_sgl_keyed(char *buf, size_t size, struct spdk_req_cmd *cmd)
+{
+	struct spdk_req_sgl_descriptor *sgl = &cmd->dptr.sgl1;
+
+	snprintf(buf, size, " len:0x%x key:0x%x", sgl->keyed.length, sgl->keyed.key);
+}
+
+static void
+client_get_sgl(char *buf, size_t size, struct spdk_req_cmd *cmd)
+{
+	struct spdk_req_sgl_descriptor *sgl = &cmd->dptr.sgl1;
+	int c;
+
+	c = snprintf(buf, size, "SGL %s %s 0x%" PRIx64, client_get_string(sgl_type, sgl->generic.type),
+				 client_get_string(sgl_subtype, sgl->generic.subtype), sgl->address);
+	assert(c >= 0 && (size_t)c < size);
+
+	if (sgl->generic.type == SPDK_CLIENT_SGL_TYPE_KEYED_DATA_BLOCK)
+	{
+		client_get_sgl_unkeyed(buf + c, size - c, cmd);
+	}
+
+	if (sgl->generic.type == SPDK_CLIENT_SGL_TYPE_DATA_BLOCK)
+	{
+		client_get_sgl_keyed(buf + c, size - c, cmd);
+	}
+}
+
+static void
+client_get_prp(char *buf, size_t size, struct spdk_req_cmd *cmd)
+{
+	snprintf(buf, size, "PRP1 0x%" PRIx64 " PRP2 0x%" PRIx64, cmd->dptr.prp.prp1, cmd->dptr.prp.prp2);
+}
+
+static void
+client_get_dptr(char *buf, size_t size, struct spdk_req_cmd *cmd)
+{
+	if (spdk_client_opc_get_data_transfer(cmd->opc) != SPDK_CLIENT_DATA_NONE)
+	{
+		switch (cmd->psdt)
+		{
+		case SPDK_CLIENT_PSDT_PRP:
+			client_get_prp(buf, size, cmd);
+			break;
+		case SPDK_CLIENT_PSDT_SGL_MPTR_CONTIG:
+		case SPDK_CLIENT_PSDT_SGL_MPTR_SGL:
+			client_get_sgl(buf, size, cmd);
+			break;
+		default:;
+		}
+	}
+}
+
+static void
+client_io_qpair_print_command(uint16_t qid, struct spdk_req_cmd *cmd)
+{
+	char dptr[CLIENT_CMD_DPTR_STR_SIZE] = {'\0'};
+
+	assert(cmd != NULL);
+
+	client_get_dptr(dptr, sizeof(dptr), cmd);
+
+	switch ((int)cmd->opc)
+	{
+	case SPDK_CLIENT_OPC_WRITE:
+	case SPDK_CLIENT_OPC_READ:
+	case SPDK_CLIENT_OPC_RPC_WRITE:
+	case SPDK_CLIENT_OPC_RPC_READ:
+		SPDK_NOTICELOG("%s sqid:%d cid:%d "
+					   "lba:%llu len:%d %s\n",
+					   client_get_string(io_opcode, cmd->opc), qid, cmd->cid,
+					   ((unsigned long long)cmd->cdw11 << 32) + cmd->cdw10,
+					   (cmd->cdw12 & 0xFFFF) + 1, dptr);
+		break;
+	case SPDK_CLIENT_OPC_FLUSH:
+		SPDK_NOTICELOG("%s sqid:%d cid:%d\n",
+					   client_get_string(io_opcode, cmd->opc), qid, cmd->cid);
+		break;
+	default:
+		SPDK_NOTICELOG("%s (%02x) sqid:%d cid:%d\n",
+					   client_get_string(io_opcode, cmd->opc), cmd->opc, qid, cmd->cid);
+		break;
+	}
+}
+
+void spdk_client_print_command(uint16_t qid, struct spdk_req_cmd *cmd)
+{
+	assert(cmd != NULL);
+
+	client_io_qpair_print_command(qid, cmd);
+}
+
+void spdk_client_qpair_print_command(struct spdk_client_qpair *qpair, struct spdk_req_cmd *cmd)
+{
+	assert(qpair != NULL);
+	assert(cmd != NULL);
+
+	spdk_client_print_command(qpair->id, cmd);
+}
+
+static const struct client_string generic_status[] = {
+	{SPDK_CLIENT_SC_SUCCESS, "SUCCESS"},
+	{SPDK_CLIENT_SC_INVALID_OPCODE, "INVALID OPCODE"},
+	{SPDK_CLIENT_SC_INVALID_FIELD, "INVALID FIELD"},
+	{SPDK_CLIENT_SC_COMMAND_ID_CONFLICT, "COMMAND ID CONFLICT"},
+	{SPDK_CLIENT_SC_DATA_TRANSFER_ERROR, "DATA TRANSFER ERROR"},
+	{SPDK_CLIENT_SC_ABORTED_POWER_LOSS, "ABORTED - POWER LOSS"},
+	{SPDK_CLIENT_SC_INTERNAL_DEVICE_ERROR, "INTERNAL DEVICE ERROR"},
+	{SPDK_CLIENT_SC_ABORTED_BY_REQUEST, "ABORTED - BY REQUEST"},
+	{SPDK_CLIENT_SC_ABORTED_SQ_DELETION, "ABORTED - SQ DELETION"},
+	{SPDK_CLIENT_SC_ABORTED_FAILED_FUSED, "ABORTED - FAILED FUSED"},
+	{SPDK_CLIENT_SC_ABORTED_MISSING_FUSED, "ABORTED - MISSING FUSED"},
+	{SPDK_CLIENT_SC_INVALID_NAMESPACE_OR_FORMAT, "INVALID NAMESPACE OR FORMAT"},
+	{SPDK_CLIENT_SC_COMMAND_SEQUENCE_ERROR, "COMMAND SEQUENCE ERROR"},
+	{SPDK_CLIENT_SC_INVALID_SGL_SEG_DESCRIPTOR, "INVALID SGL SEGMENT DESCRIPTOR"},
+	{SPDK_CLIENT_SC_INVALID_NUM_SGL_DESCIRPTORS, "INVALID NUMBER OF SGL DESCRIPTORS"},
+	{SPDK_CLIENT_SC_DATA_SGL_LENGTH_INVALID, "DATA SGL LENGTH INVALID"},
+	{SPDK_CLIENT_SC_METADATA_SGL_LENGTH_INVALID, "METADATA SGL LENGTH INVALID"},
+	{SPDK_CLIENT_SC_SGL_DESCRIPTOR_TYPE_INVALID, "SGL DESCRIPTOR TYPE INVALID"},
+	{SPDK_CLIENT_SC_INVALID_CONTROLLER_MEM_BUF, "INVALID CONTROLLER MEMORY BUFFER"},
+	{SPDK_CLIENT_SC_INVALID_PRP_OFFSET, "INVALID PRP OFFSET"},
+	{SPDK_CLIENT_SC_ATOMIC_WRITE_UNIT_EXCEEDED, "ATOMIC WRITE UNIT EXCEEDED"},
+	{SPDK_CLIENT_SC_OPERATION_DENIED, "OPERATION DENIED"},
+	{SPDK_CLIENT_SC_INVALID_SGL_OFFSET, "INVALID SGL OFFSET"},
+	{SPDK_CLIENT_SC_HOSTID_INCONSISTENT_FORMAT, "HOSTID INCONSISTENT FORMAT"},
+	{SPDK_CLIENT_SC_KEEP_ALIVE_EXPIRED, "KEEP ALIVE EXPIRED"},
+	{SPDK_CLIENT_SC_KEEP_ALIVE_INVALID, "KEEP ALIVE INVALID"},
+	{SPDK_CLIENT_SC_ABORTED_PREEMPT, "ABORTED - PREEMPT AND ABORT"},
+	{SPDK_CLIENT_SC_SANITIZE_FAILED, "SANITIZE FAILED"},
+	{SPDK_CLIENT_SC_SANITIZE_IN_PROGRESS, "SANITIZE IN PROGRESS"},
+	{SPDK_CLIENT_SC_SGL_DATA_BLOCK_GRANULARITY_INVALID, "DATA BLOCK GRANULARITY INVALID"},
+	{SPDK_CLIENT_SC_COMMAND_INVALID_IN_CMB, "COMMAND NOT SUPPORTED FOR QUEUE IN CMB"},
+	{SPDK_CLIENT_SC_LBA_OUT_OF_RANGE, "LBA OUT OF RANGE"},
+	{SPDK_CLIENT_SC_CAPACITY_EXCEEDED, "CAPACITY EXCEEDED"},
+	{SPDK_CLIENT_SC_NAMESPACE_NOT_READY, "NAMESPACE NOT READY"},
+	{SPDK_CLIENT_SC_RESERVATION_CONFLICT, "RESERVATION CONFLICT"},
+	{SPDK_CLIENT_SC_FORMAT_IN_PROGRESS, "FORMAT IN PROGRESS"},
+	{0xFFFF, "GENERIC"}};
+
+static const struct client_string command_specific_status[] = {
+	{SPDK_CLIENT_SC_COMPLETION_QUEUE_INVALID, "INVALID COMPLETION QUEUE"},
+	{SPDK_CLIENT_SC_INVALID_QUEUE_IDENTIFIER, "INVALID QUEUE IDENTIFIER"},
+	{SPDK_CLIENT_SC_INVALID_QUEUE_SIZE, "INVALID QUEUE SIZE"},
+	{SPDK_CLIENT_SC_ABORT_COMMAND_LIMIT_EXCEEDED, "ABORT CMD LIMIT EXCEEDED"},
+	{SPDK_CLIENT_SC_ASYNC_EVENT_REQUEST_LIMIT_EXCEEDED, "ASYNC LIMIT EXCEEDED"},
+	{SPDK_CLIENT_SC_INVALID_FIRMWARE_SLOT, "INVALID FIRMWARE SLOT"},
+	{SPDK_CLIENT_SC_INVALID_FIRMWARE_IMAGE, "INVALID FIRMWARE IMAGE"},
+	{SPDK_CLIENT_SC_INVALID_INTERRUPT_VECTOR, "INVALID INTERRUPT VECTOR"},
+	{SPDK_CLIENT_SC_INVALID_LOG_PAGE, "INVALID LOG PAGE"},
+	{SPDK_CLIENT_SC_INVALID_FORMAT, "INVALID FORMAT"},
+	{SPDK_CLIENT_SC_FIRMWARE_REQ_CONVENTIONAL_RESET, "FIRMWARE REQUIRES CONVENTIONAL RESET"},
+	{SPDK_CLIENT_SC_INVALID_QUEUE_DELETION, "INVALID QUEUE DELETION"},
+	{SPDK_CLIENT_SC_FEATURE_ID_NOT_SAVEABLE, "FEATURE ID NOT SAVEABLE"},
+	{SPDK_CLIENT_SC_FEATURE_NOT_CHANGEABLE, "FEATURE NOT CHANGEABLE"},
+	{SPDK_CLIENT_SC_FEATURE_NOT_NAMESPACE_SPECIFIC, "FEATURE NOT NAMESPACE SPECIFIC"},
+	{SPDK_CLIENT_SC_FIRMWARE_REQ_NVM_RESET, "FIRMWARE REQUIRES NVM RESET"},
+	{SPDK_CLIENT_SC_FIRMWARE_REQ_RESET, "FIRMWARE REQUIRES RESET"},
+	{SPDK_CLIENT_SC_FIRMWARE_REQ_MAX_TIME_VIOLATION, "FIRMWARE REQUIRES MAX TIME VIOLATION"},
+	{SPDK_CLIENT_SC_FIRMWARE_ACTIVATION_PROHIBITED, "FIRMWARE ACTIVATION PROHIBITED"},
+	{SPDK_CLIENT_SC_OVERLAPPING_RANGE, "OVERLAPPING RANGE"},
+	{SPDK_CLIENT_SC_NAMESPACE_INSUFFICIENT_CAPACITY, "NAMESPACE INSUFFICIENT CAPACITY"},
+	{SPDK_CLIENT_SC_NAMESPACE_ID_UNAVAILABLE, "NAMESPACE ID UNAVAILABLE"},
+	{SPDK_CLIENT_SC_NAMESPACE_ALREADY_ATTACHED, "NAMESPACE ALREADY ATTACHED"},
+	{SPDK_CLIENT_SC_NAMESPACE_IS_PRIVATE, "NAMESPACE IS PRIVATE"},
+	{SPDK_CLIENT_SC_NAMESPACE_NOT_ATTACHED, "NAMESPACE NOT ATTACHED"},
+	{SPDK_CLIENT_SC_THINPROVISIONING_NOT_SUPPORTED, "THINPROVISIONING NOT SUPPORTED"},
+	{SPDK_CLIENT_SC_CONTROLLER_LIST_INVALID, "CONTROLLER LIST INVALID"},
+	{SPDK_CLIENT_SC_DEVICE_SELF_TEST_IN_PROGRESS, "DEVICE SELF-TEST IN PROGRESS"},
+	{SPDK_CLIENT_SC_BOOT_PARTITION_WRITE_PROHIBITED, "BOOT PARTITION WRITE PROHIBITED"},
+	{SPDK_CLIENT_SC_INVALID_CTRLR_ID, "INVALID CONTROLLER ID"},
+	{SPDK_CLIENT_SC_INVALID_SECONDARY_CTRLR_STATE, "INVALID SECONDARY CONTROLLER STATE"},
+	{SPDK_CLIENT_SC_INVALID_NUM_CTRLR_RESOURCES, "INVALID NUMBER OF CONTROLLER RESOURCES"},
+	{SPDK_CLIENT_SC_INVALID_RESOURCE_ID, "INVALID RESOURCE IDENTIFIER"},
+	{SPDK_CLIENT_SC_STREAM_RESOURCE_ALLOCATION_FAILED, "STREAM RESOURCE ALLOCATION FAILED"},
+	{SPDK_CLIENT_SC_CONFLICTING_ATTRIBUTES, "CONFLICTING ATTRIBUTES"},
+	{SPDK_CLIENT_SC_INVALID_PROTECTION_INFO, "INVALID PROTECTION INFO"},
+	{SPDK_CLIENT_SC_ATTEMPTED_WRITE_TO_RO_RANGE, "WRITE TO RO RANGE"},
+	{0xFFFF, "COMMAND SPECIFIC"}};
+
+static const struct client_string media_error_status[] = {
+	{SPDK_CLIENT_SC_WRITE_FAULTS, "WRITE FAULTS"},
+	{SPDK_CLIENT_SC_UNRECOVERED_READ_ERROR, "UNRECOVERED READ ERROR"},
+	{SPDK_CLIENT_SC_GUARD_CHECK_ERROR, "GUARD CHECK ERROR"},
+	{SPDK_CLIENT_SC_APPLICATION_TAG_CHECK_ERROR, "APPLICATION TAG CHECK ERROR"},
+	{SPDK_CLIENT_SC_REFERENCE_TAG_CHECK_ERROR, "REFERENCE TAG CHECK ERROR"},
+	{SPDK_CLIENT_SC_COMPARE_FAILURE, "COMPARE FAILURE"},
+	{SPDK_CLIENT_SC_ACCESS_DENIED, "ACCESS DENIED"},
+	{SPDK_CLIENT_SC_DEALLOCATED_OR_UNWRITTEN_BLOCK, "DEALLOCATED OR UNWRITTEN BLOCK"},
+
+	{0xFFFF, "MEDIA ERROR"}};
+
+static const struct client_string path_status[] = {
+	{SPDK_CLIENT_SC_INTERNAL_PATH_ERROR, "INTERNAL PATH ERROR"},
+	{SPDK_CLIENT_SC_CONTROLLER_PATH_ERROR, "CONTROLLER PATH ERROR"},
+	{SPDK_CLIENT_SC_HOST_PATH_ERROR, "HOST PATH ERROR"},
+	{SPDK_CLIENT_SC_ABORTED_BY_HOST, "ABORTED BY HOST"},
+	{0xFFFF, "PATH ERROR"}};
+
+const char *
+spdk_req_cpl_get_status_string(const struct spdk_req_status *status)
+{
+	const struct client_string *entry;
+
+	switch (status->sct)
+	{
+	case SPDK_CLIENT_SCT_GENERIC:
+		entry = generic_status;
+		break;
+	case SPDK_CLIENT_SCT_COMMAND_SPECIFIC:
+		entry = command_specific_status;
+		break;
+	case SPDK_CLIENT_SCT_MEDIA_ERROR:
+		entry = media_error_status;
+		break;
+	case SPDK_CLIENT_SCT_PATH:
+		entry = path_status;
+		break;
+	case SPDK_CLIENT_SCT_VENDOR_SPECIFIC:
+		return "VENDOR SPECIFIC";
+	default:
+		return "RESERVED";
+	}
+
+	return client_get_string(entry, status->sc);
+}
+
+void spdk_client_print_completion(uint16_t qid, struct spdk_req_cpl *cpl)
+{
+	assert(cpl != NULL);
+
+	/* Check that sqid matches qid. Note that sqid is reserved
+	 * for fabrics so don't print an error when sqid is 0. */
+	if (cpl->sqid != qid && cpl->sqid != 0)
+	{
+		SPDK_ERRLOG("sqid %u doesn't match qid\n", cpl->sqid);
+	}
+
+	SPDK_NOTICELOG("%s (%02x/%02x) qid:%d cid:%d cdw0:%x sqhd:%04x p:%x m:%x dnr:%x\n",
+				   spdk_req_cpl_get_status_string(&cpl->status),
+				   cpl->status.sct, cpl->status.sc, qid, cpl->cid, cpl->cdw0,
+				   cpl->sqhd, cpl->status.p, cpl->status.m, cpl->status.dnr);
+}
+
+void spdk_client_qpair_print_completion(struct spdk_client_qpair *qpair, struct spdk_req_cpl *cpl)
+{
+	spdk_client_print_completion(qpair->id, cpl);
+}
+
+bool client_completion_is_retry(const struct spdk_req_cpl *cpl)
+{
+	/*
+	 * TODO: spec is not clear how commands that are aborted due
+	 *  to TLER will be marked.  So for now, it seems
+	 *  NAMESPACE_NOT_READY is the only case where we should
+	 *  look at the DNR bit.
+	 */
+	switch ((int)cpl->status.sct)
+	{
+	case SPDK_CLIENT_SCT_GENERIC:
+		switch ((int)cpl->status.sc)
+		{
+		case SPDK_CLIENT_SC_NAMESPACE_NOT_READY:
+		case SPDK_CLIENT_SC_FORMAT_IN_PROGRESS:
+			if (cpl->status.dnr)
+			{
+				return false;
+			}
+			else
+			{
+				return true;
+			}
+		case SPDK_CLIENT_SC_INVALID_OPCODE:
+		case SPDK_CLIENT_SC_INVALID_FIELD:
+		case SPDK_CLIENT_SC_COMMAND_ID_CONFLICT:
+		case SPDK_CLIENT_SC_DATA_TRANSFER_ERROR:
+		case SPDK_CLIENT_SC_ABORTED_POWER_LOSS:
+		case SPDK_CLIENT_SC_INTERNAL_DEVICE_ERROR:
+		case SPDK_CLIENT_SC_ABORTED_BY_REQUEST:
+		case SPDK_CLIENT_SC_ABORTED_SQ_DELETION:
+		case SPDK_CLIENT_SC_ABORTED_FAILED_FUSED:
+		case SPDK_CLIENT_SC_ABORTED_MISSING_FUSED:
+		case SPDK_CLIENT_SC_INVALID_NAMESPACE_OR_FORMAT:
+		case SPDK_CLIENT_SC_COMMAND_SEQUENCE_ERROR:
+		case SPDK_CLIENT_SC_LBA_OUT_OF_RANGE:
+		case SPDK_CLIENT_SC_CAPACITY_EXCEEDED:
+		default:
+			return false;
+		}
+	case SPDK_CLIENT_SCT_PATH:
+		/*
+		 * Per Client TP 4028 (Path and Transport Error Enhancements), retries should be
+		 * based on the setting of the DNR bit for Internal Path Error
+		 */
+		switch ((int)cpl->status.sc)
+		{
+		case SPDK_CLIENT_SC_INTERNAL_PATH_ERROR:
+			return !cpl->status.dnr;
+		default:
+			return false;
+		}
+	case SPDK_CLIENT_SCT_COMMAND_SPECIFIC:
+	case SPDK_CLIENT_SCT_MEDIA_ERROR:
+	case SPDK_CLIENT_SCT_VENDOR_SPECIFIC:
+	default:
+		return false;
+	}
+}
+
+static void
+client_qpair_manual_complete_request(struct spdk_client_qpair *qpair,
+									 struct client_request *req, uint32_t sct, uint32_t sc,
+									 uint32_t dnr, bool print_on_error)
+{
+	struct spdk_req_cpl cpl;
+	bool error;
+
+	memset(&cpl, 0, sizeof(cpl));
+	cpl.sqid = qpair->id;
+	cpl.status.sct = sct;
+	cpl.status.sc = sc;
+	cpl.status.dnr = dnr;
+
+	error = spdk_req_cpl_is_error(&cpl);
+
+	if (error && print_on_error && !qpair->ctrlr->opts.disable_error_logging)
+	{
+		SPDK_NOTICELOG("Command completed manually:\n");
+		spdk_client_qpair_print_command(qpair, &req->cmd);
+		spdk_client_qpair_print_completion(qpair, &cpl);
+	}
+
+	client_complete_request(req->cb_fn, req->cb_arg, qpair, req, &cpl);
+	client_free_request(req);
+}
+
+void client_qpair_abort_queued_reqs(struct spdk_client_qpair *qpair, uint32_t dnr)
+{
+	struct client_request *req;
+	STAILQ_HEAD(, client_request)
+	tmp;
+
+	STAILQ_INIT(&tmp);
+	STAILQ_SWAP(&tmp, &qpair->queued_req, client_request);
+
+	while (!STAILQ_EMPTY(&tmp))
+	{
+		req = STAILQ_FIRST(&tmp);
+		STAILQ_REMOVE_HEAD(&tmp, stailq);
+		if (!qpair->ctrlr->opts.disable_error_logging)
+		{
+			SPDK_ERRLOG("aborting queued i/o\n");
+		}
+		client_qpair_manual_complete_request(qpair, req, SPDK_CLIENT_SCT_GENERIC,
+											 SPDK_CLIENT_SC_ABORTED_SQ_DELETION, dnr, true);
+	}
+}
+
+/* The callback to a request may submit the next request which is queued and
+ * then the same callback may abort it immediately. This repetition may cause
+ * infinite recursive calls. Hence move aborting requests to another list here
+ * and abort them later at resubmission.
+ */
+static void
+_client_qpair_complete_abort_queued_reqs(struct spdk_client_qpair *qpair)
+{
+	struct client_request *req;
+	STAILQ_HEAD(, client_request)
+	tmp;
+
+	if (spdk_likely(STAILQ_EMPTY(&qpair->aborting_queued_req)))
+	{
+		return;
+	}
+
+	STAILQ_INIT(&tmp);
+	STAILQ_SWAP(&tmp, &qpair->aborting_queued_req, client_request);
+
+	while (!STAILQ_EMPTY(&tmp))
+	{
+		req = STAILQ_FIRST(&tmp);
+		STAILQ_REMOVE_HEAD(&tmp, stailq);
+		client_qpair_manual_complete_request(qpair, req, SPDK_CLIENT_SCT_GENERIC,
+											 SPDK_CLIENT_SC_ABORTED_BY_REQUEST, 1, true);
+	}
+}
+
+uint32_t
+client_qpair_abort_queued_reqs_with_cbarg(struct spdk_client_qpair *qpair, void *cmd_cb_arg)
+{
+	struct client_request *req, *tmp;
+	uint32_t aborting = 0;
+
+	STAILQ_FOREACH_SAFE(req, &qpair->queued_req, stailq, tmp)
+	{
+		if (req->cb_arg == cmd_cb_arg)
+		{
+			STAILQ_REMOVE(&qpair->queued_req, req, client_request, stailq);
+			STAILQ_INSERT_TAIL(&qpair->aborting_queued_req, req, stailq);
+			if (!qpair->ctrlr->opts.disable_error_logging)
+			{
+				SPDK_ERRLOG("aborting queued i/o\n");
+			}
+			aborting++;
+		}
+	}
+
+	return aborting;
+}
+
+static inline bool
+client_qpair_check_enabled(struct spdk_client_qpair *qpair)
+{
+	struct client_request *req;
+
+	/*
+	 * Either during initial connect or reset, the qpair should follow the given state machine.
+	 * QPAIR_DISABLED->QPAIR_CONNECTING->QPAIR_CONNECTED->QPAIR_ENABLING->QPAIR_ENABLED. In the
+	 * reset case, once the qpair is properly connected, we need to abort any outstanding requests
+	 * from the old transport connection and encourage the application to retry them. We also need
+	 * to submit any queued requests that built up while we were in the connected or enabling state.
+	 */
+	if (client_qpair_get_state(qpair) == CLIENT_QPAIR_CONNECTED && !qpair->ctrlr->is_resetting)
+	{
+		client_qpair_set_state(qpair, CLIENT_QPAIR_ENABLING);
+		/*
+		 * PCIe is special, for fabrics transports, we can abort requests before disconnect during reset
+		 * but we have historically not disconnected pcie qpairs during reset so we have to abort requests
+		 * here.
+		 */
+		if (qpair->ctrlr->trtype == SPDK_CLIENT_TRANSPORT_PCIE &&
+			!qpair->is_new_qpair)
+		{
+			client_qpair_abort_all_queued_reqs(qpair, 0);
+			client_transport_qpair_abort_reqs(qpair, 0);
+		}
+
+		client_qpair_set_state(qpair, CLIENT_QPAIR_ENABLED);
+		while (!STAILQ_EMPTY(&qpair->queued_req))
+		{
+			req = STAILQ_FIRST(&qpair->queued_req);
+			STAILQ_REMOVE_HEAD(&qpair->queued_req, stailq);
+			if (client_qpair_resubmit_request(qpair, req))
+			{
+				break;
+			}
+		}
+	}
+
+	/*
+	 * When doing a reset, we must disconnect the qpair on the proper core.
+	 * Note, reset is the only case where we set the failure reason without
+	 * setting the qpair state since reset is done at the generic layer on the
+	 * controller thread and we can't disconnect I/O qpairs from the controller
+	 * thread.
+	 */
+	if (qpair->transport_failure_reason != SPDK_CLIENT_QPAIR_FAILURE_NONE &&
+		client_qpair_get_state(qpair) == CLIENT_QPAIR_ENABLED)
+	{
+		/* Don't disconnect PCIe qpairs. They are a special case for reset. */
+		if (qpair->ctrlr->trtype != SPDK_CLIENT_TRANSPORT_PCIE)
+		{
+			client_ctrlr_disconnect_qpair(qpair);
+		}
+		return false;
+	}
+
+	return client_qpair_get_state(qpair) == CLIENT_QPAIR_ENABLED;
+}
+
+void client_qpair_resubmit_requests(struct spdk_client_qpair *qpair, uint32_t num_requests)
+{
+	uint32_t i;
+	int resubmit_rc;
+	struct client_request *req;
+
+	assert(num_requests > 0);
+
+	for (i = 0; i < num_requests; i++)
+	{
+		if (qpair->ctrlr->is_resetting)
+		{
+			break;
+		}
+		if ((req = STAILQ_FIRST(&qpair->queued_req)) == NULL)
+		{
+			break;
+		}
+		STAILQ_REMOVE_HEAD(&qpair->queued_req, stailq);
+		resubmit_rc = client_qpair_resubmit_request(qpair, req);
+		if (spdk_unlikely(resubmit_rc != 0))
+		{
+			SPDK_DEBUGLOG(client, "Unable to resubmit as many requests as we completed.\n");
+			break;
+		}
+	}
+
+	_client_qpair_complete_abort_queued_reqs(qpair);
+}
+
+static void
+client_complete_register_operations(struct spdk_client_qpair *qpair)
+{
+	struct client_register_completion *ctx;
+	struct spdk_client_ctrlr *ctrlr = qpair->ctrlr;
+	STAILQ_HEAD(, client_register_completion)
+	operations;
+
+	STAILQ_INIT(&operations);
+	client_robust_mutex_lock(&ctrlr->ctrlr_lock);
+	STAILQ_SWAP(&ctrlr->register_operations, &operations, client_register_completion);
+	client_robust_mutex_unlock(&ctrlr->ctrlr_lock);
+
+	while (!STAILQ_EMPTY(&operations))
+	{
+		ctx = STAILQ_FIRST(&operations);
+		STAILQ_REMOVE_HEAD(&operations, stailq);
+		if (ctx->cb_fn != NULL)
+		{
+			ctx->cb_fn(ctx->cb_ctx, ctx->value, &ctx->cpl);
+		}
+		free(ctx);
+	}
+}
+
+int32_t
+spdk_client_qpair_process_completions(struct spdk_client_qpair *qpair, uint32_t max_completions)
+{
+	int32_t ret;
+	struct client_request *req, *tmp;
+
+	if (spdk_unlikely(qpair->ctrlr->is_failed))
+	{
+		if (qpair->ctrlr->is_removed)
+		{
+			client_qpair_set_state(qpair, CLIENT_QPAIR_DESTROYING);
+			client_qpair_abort_all_queued_reqs(qpair, 0);
+			client_transport_qpair_abort_reqs(qpair, 0);
+		}
+		return -ENXIO;
+	}
+
+	if (spdk_unlikely(!client_qpair_check_enabled(qpair) &&
+					  !(client_qpair_get_state(qpair) == CLIENT_QPAIR_CONNECTING)))
+	{
+		/*
+		 * qpair is not enabled, likely because a controller reset is
+		 *  in progress.
+		 */
+		return -ENXIO;
+	}
+
+	/* error injection for those queued error requests */
+	if (spdk_unlikely(!STAILQ_EMPTY(&qpair->err_req_head)))
+	{
+		STAILQ_FOREACH_SAFE(req, &qpair->err_req_head, stailq, tmp)
+		{
+			if (spdk_get_ticks() - req->submit_tick > req->timeout_tsc)
+			{
+				STAILQ_REMOVE(&qpair->err_req_head, req, client_request, stailq);
+				client_qpair_manual_complete_request(qpair, req,
+													 req->cpl.status.sct,
+													 req->cpl.status.sc, 0, true);
+			}
+		}
+	}
+
+	qpair->in_completion_context = 1;
+	ret = client_transport_qpair_process_completions(qpair, max_completions);
+	if (ret < 0)
+	{
+		SPDK_ERRLOG("CQ transport error %d (%s) on qpair id %hu\n", ret, spdk_strerror(-ret), qpair->id);
+		if (client_qpair_is_admin_queue(qpair))
+		{
+			client_ctrlr_fail(qpair->ctrlr, false);
+		}
+	}
+	qpair->in_completion_context = 0;
+	if (qpair->delete_after_completion_context)
+	{
+		/*
+		 * A request to delete this qpair was made in the context of this completion
+		 *  routine - so it is safe to delete it now.
+		 */
+		spdk_client_ctrlr_free_io_qpair(qpair);
+		return ret;
+	}
+
+	/*
+	 * At this point, ret must represent the number of completions we reaped.
+	 * submit as many queued requests as we completed.
+	 */
+	if (ret > 0)
+	{
+		client_qpair_resubmit_requests(qpair, ret);
+	}
+
+	/* Complete any pending register operations */
+	if (client_qpair_is_admin_queue(qpair))
+	{
+		client_complete_register_operations(qpair);
+	}
+
+	return ret;
+}
+
+spdk_client_qp_failure_reason
+spdk_client_qpair_get_failure_reason(struct spdk_client_qpair *qpair)
+{
+	return qpair->transport_failure_reason;
+}
+
+int client_qpair_init(struct spdk_client_qpair *qpair, uint16_t id,
+					  struct spdk_client_ctrlr *ctrlr,
+					  enum spdk_client_qprio qprio,
+					  uint32_t num_requests, bool async)
+{
+	size_t req_size_padded;
+	uint32_t i;
+
+	qpair->id = id;
+	qpair->qprio = qprio;
+
+	qpair->in_completion_context = 0;
+	qpair->delete_after_completion_context = 0;
+	qpair->no_deletion_notification_needed = 0;
+
+	qpair->ctrlr = ctrlr;
+	qpair->trtype = ctrlr->trtype;
+	qpair->is_new_qpair = true;
+	qpair->async = async;
+	qpair->poll_status = NULL;
+
+	STAILQ_INIT(&qpair->free_req);
+	STAILQ_INIT(&qpair->queued_req);
+	STAILQ_INIT(&qpair->aborting_queued_req);
+	TAILQ_INIT(&qpair->err_cmd_head);
+	STAILQ_INIT(&qpair->err_req_head);
+
+	STAILQ_INIT(&qpair->free_rpc_req);
+
+	req_size_padded = (sizeof(struct client_request) + 63) & ~(size_t)63;
+
+	/* Add one for the reserved_req */
+	num_requests++;
+
+	qpair->req_buf = spdk_zmalloc(req_size_padded * num_requests, 64, NULL,
+								  SPDK_ENV_SOCKET_ID_ANY, SPDK_MALLOC_SHARE);
+	if (qpair->req_buf == NULL)
+	{
+		SPDK_ERRLOG("no memory to allocate qpair(cntlid:0x%x sqid:%d) req_buf with %d request\n",
+					ctrlr->cntlid, qpair->id, num_requests);
+		return -ENOMEM;
+	}
+
+	for (i = 0; i < num_requests; i++)
+	{
+		struct client_request *req = qpair->req_buf + i * req_size_padded;
+
+		req->qpair = qpair;
+		if (i == 0)
+		{
+			qpair->reserved_req = req;
+		}
+		else
+		{
+			STAILQ_INSERT_HEAD(&qpair->free_req, req, stailq);
+		}
+	}
+
+	// 暂时写死4096的rpc request队列深度
+	req_size_padded = (sizeof(struct rpc_request) + 63) & ~(size_t)63;
+	qpair->rpc_req_buf = spdk_zmalloc(req_size_padded * 4096, 64, NULL,
+									  SPDK_ENV_SOCKET_ID_ANY, SPDK_MALLOC_SHARE);
+	if (qpair->rpc_req_buf == NULL)
+	{
+		SPDK_ERRLOG("no memory to allocate qpair(cntlid:0x%x sqid:%d) rpc_req_buf with %d request\n",
+					ctrlr->cntlid, qpair->id, 4096);
+		spdk_free(qpair->req_buf);
+		return -ENOMEM;
+	}
+
+	for (i = 0; i < 4096; i++)
+	{
+		struct rpc_request *req = qpair->rpc_req_buf + i * req_size_padded;
+		req->qpair = qpair;
+		req->request_id = i;
+		STAILQ_INSERT_TAIL(&qpair->free_rpc_req, req, stailq);
+	}
+
+	return 0;
+}
+
+void client_qpair_complete_error_reqs(struct spdk_client_qpair *qpair)
+{
+	struct client_request *req;
+
+	while (!STAILQ_EMPTY(&qpair->err_req_head))
+	{
+		req = STAILQ_FIRST(&qpair->err_req_head);
+		STAILQ_REMOVE_HEAD(&qpair->err_req_head, stailq);
+		client_qpair_manual_complete_request(qpair, req,
+											 req->cpl.status.sct,
+											 req->cpl.status.sc, 0, true);
+	}
+}
+
+void client_qpair_deinit(struct spdk_client_qpair *qpair)
+{
+	struct client_error_cmd *cmd, *entry;
+
+	client_qpair_abort_queued_reqs(qpair, 0);
+	_client_qpair_complete_abort_queued_reqs(qpair);
+	client_qpair_complete_error_reqs(qpair);
+
+	TAILQ_FOREACH_SAFE(cmd, &qpair->err_cmd_head, link, entry)
+	{
+		TAILQ_REMOVE(&qpair->err_cmd_head, cmd, link);
+		spdk_free(cmd);
+	}
+
+	spdk_free(qpair->req_buf);
+}
+
+static inline int
+_client_qpair_submit_request(struct spdk_client_qpair *qpair, struct client_request *req)
+{
+	int rc = 0;
+	struct client_request *child_req, *tmp;
+	struct client_error_cmd *cmd;
+	struct spdk_client_ctrlr *ctrlr = qpair->ctrlr;
+	bool child_req_failed = false;
+
+	client_qpair_check_enabled(qpair);
+
+	if (spdk_unlikely(client_qpair_get_state(qpair) == CLIENT_QPAIR_DISCONNECTED ||
+					  client_qpair_get_state(qpair) == CLIENT_QPAIR_DISCONNECTING ||
+					  client_qpair_get_state(qpair) == CLIENT_QPAIR_DESTROYING))
+	{
+		TAILQ_FOREACH_SAFE(child_req, &req->children, child_tailq, tmp)
+		{
+			client_request_remove_child(req, child_req);
+			client_request_free_children(child_req);
+			client_free_request(child_req);
+		}
+		if (req->parent != NULL)
+		{
+			client_request_remove_child(req->parent, req);
+		}
+		client_free_request(req);
+		return -ENXIO;
+	}
+
+	if (req->num_children)
+	{
+		/*
+		 * This is a split (parent) request. Submit all of the children but not the parent
+		 * request itself, since the parent is the original unsplit request.
+		 */
+		TAILQ_FOREACH_SAFE(child_req, &req->children, child_tailq, tmp)
+		{
+			if (spdk_likely(!child_req_failed))
+			{
+				rc = client_qpair_submit_request(qpair, child_req);
+				if (spdk_unlikely(rc != 0))
+				{
+					child_req_failed = true;
+				}
+			}
+			else
+			{ /* free remaining child_reqs since one child_req fails */
+				client_request_remove_child(req, child_req);
+				client_request_free_children(child_req);
+				client_free_request(child_req);
+			}
+		}
+
+		if (spdk_unlikely(child_req_failed))
+		{
+			/* part of children requests have been submitted,
+			 * return success since we must wait for those children to complete,
+			 * but set the parent request to failure.
+			 */
+			if (req->num_children)
+			{
+				req->cpl.status.sct = SPDK_CLIENT_SCT_GENERIC;
+				req->cpl.status.sc = SPDK_CLIENT_SC_INTERNAL_DEVICE_ERROR;
+				return 0;
+			}
+			goto error;
+		}
+
+		return rc;
+	}
+
+	/* queue those requests which matches with opcode in err_cmd list */
+	if (spdk_unlikely(!TAILQ_EMPTY(&qpair->err_cmd_head)))
+	{
+		TAILQ_FOREACH(cmd, &qpair->err_cmd_head, link)
+		{
+			if (!cmd->do_not_submit)
+			{
+				continue;
+			}
+
+			if ((cmd->opc == req->cmd.opc) && cmd->err_count)
+			{
+				/* add to error request list and set cpl */
+				req->timeout_tsc = cmd->timeout_tsc;
+				req->submit_tick = spdk_get_ticks();
+				req->cpl.status.sct = cmd->status.sct;
+				req->cpl.status.sc = cmd->status.sc;
+				STAILQ_INSERT_TAIL(&qpair->err_req_head, req, stailq);
+				cmd->err_count--;
+				return 0;
+			}
+		}
+	}
+
+	if (spdk_unlikely(ctrlr->is_failed))
+	{
+		rc = -ENXIO;
+		goto error;
+	}
+
+	/* assign submit_tick before submitting req to specific transport */
+	if (spdk_unlikely(ctrlr->timeout_enabled))
+	{
+		if (req->submit_tick == 0)
+		{ /* req submitted for the first time */
+			req->submit_tick = spdk_get_ticks();
+			req->timed_out = false;
+		}
+	}
+	else
+	{
+		req->submit_tick = 0;
+	}
+
+	if (spdk_likely(client_qpair_get_state(qpair) == CLIENT_QPAIR_ENABLED))
+	{
+		rc = client_transport_qpair_submit_request(qpair, req);
+	}
+	else
+	{
+		/* The controller is being reset - queue this request and
+		 *  submit it later when the reset is completed.
+		 */
+		return -EAGAIN;
+	}
+
+	if (spdk_likely(rc == 0))
+	{
+		if (SPDK_DEBUGLOG_FLAG_ENABLED("client"))
+		{
+			spdk_client_print_command(qpair->id, &req->cmd);
+		}
+		req->queued = false;
+		return 0;
+	}
+
+	if (rc == -EAGAIN)
+	{
+		return -EAGAIN;
+	}
+
+error:
+	if (req->parent != NULL)
+	{
+		client_request_remove_child(req->parent, req);
+	}
+
+	/* The request is from queued_req list we should trigger the callback from caller */
+	if (spdk_unlikely(req->queued))
+	{
+		client_qpair_manual_complete_request(qpair, req, SPDK_CLIENT_SCT_GENERIC,
+											 SPDK_CLIENT_SC_INTERNAL_DEVICE_ERROR, true, true);
+		return rc;
+	}
+
+	client_free_request(req);
+
+	return rc;
+}
+
+int client_qpair_submit_request(struct spdk_client_qpair *qpair, struct client_request *req)
+{
+	int rc;
+
+	if (spdk_unlikely(!STAILQ_EMPTY(&qpair->queued_req) && req->num_children == 0))
+	{
+
+		if (client_qpair_get_state(qpair) != CLIENT_QPAIR_CONNECTING)
+		{
+			STAILQ_INSERT_TAIL(&qpair->queued_req, req, stailq);
+			req->queued = true;
+			return 0;
+		}
+	}
+
+	rc = _client_qpair_submit_request(qpair, req);
+	if (rc == -EAGAIN)
+	{
+		STAILQ_INSERT_TAIL(&qpair->queued_req, req, stailq);
+		req->queued = true;
+		rc = 0;
+	}
+
+	return rc;
+}
+
+static int
+client_qpair_resubmit_request(struct spdk_client_qpair *qpair, struct client_request *req)
+{
+	int rc;
+
+	/*
+	 * We should never have a request with children on the queue.
+	 * This is necessary to preserve the 1:1 relationship between
+	 * completions and resubmissions.
+	 */
+	assert(req->num_children == 0);
+	assert(req->queued);
+	rc = _client_qpair_submit_request(qpair, req);
+	if (spdk_unlikely(rc == -EAGAIN))
+	{
+		STAILQ_INSERT_HEAD(&qpair->queued_req, req, stailq);
+	}
+
+	return rc;
+}
+
+void client_qpair_abort_all_queued_reqs(struct spdk_client_qpair *qpair, uint32_t dnr)
+{
+	client_qpair_complete_error_reqs(qpair);
+	client_qpair_abort_queued_reqs(qpair, dnr);
+	_client_qpair_complete_abort_queued_reqs(qpair);
+}
+
+int spdk_client_qpair_add_cmd_error_injection(struct spdk_client_ctrlr *ctrlr,
+											  struct spdk_client_qpair *qpair,
+											  uint8_t opc, bool do_not_submit,
+											  uint64_t timeout_in_us,
+											  uint32_t err_count,
+											  uint8_t sct, uint8_t sc)
+{
+	struct client_error_cmd *entry, *cmd = NULL;
+	int rc = 0;
+
+	if (qpair == NULL)
+	{
+		qpair = ctrlr->adminq;
+		client_robust_mutex_lock(&ctrlr->ctrlr_lock);
+	}
+
+	TAILQ_FOREACH(entry, &qpair->err_cmd_head, link)
+	{
+		if (entry->opc == opc)
+		{
+			cmd = entry;
+			break;
+		}
+	}
+
+	if (cmd == NULL)
+	{
+		cmd = spdk_zmalloc(sizeof(*cmd), 64, NULL, SPDK_ENV_LCORE_ID_ANY, SPDK_MALLOC_DMA);
+		if (!cmd)
+		{
+			rc = -ENOMEM;
+			goto out;
+		}
+		TAILQ_INSERT_TAIL(&qpair->err_cmd_head, cmd, link);
+	}
+
+	cmd->do_not_submit = do_not_submit;
+	cmd->err_count = err_count;
+	cmd->timeout_tsc = timeout_in_us * spdk_get_ticks_hz() / 1000000ULL;
+	cmd->opc = opc;
+	cmd->status.sct = sct;
+	cmd->status.sc = sc;
+out:
+	if (client_qpair_is_admin_queue(qpair))
+	{
+		client_robust_mutex_unlock(&ctrlr->ctrlr_lock);
+	}
+
+	return rc;
+}
+
+void spdk_client_qpair_remove_cmd_error_injection(struct spdk_client_ctrlr *ctrlr,
+												  struct spdk_client_qpair *qpair,
+												  uint8_t opc)
+{
+	struct client_error_cmd *cmd, *entry;
+
+	if (qpair == NULL)
+	{
+		qpair = ctrlr->adminq;
+		client_robust_mutex_lock(&ctrlr->ctrlr_lock);
+	}
+
+	TAILQ_FOREACH_SAFE(cmd, &qpair->err_cmd_head, link, entry)
+	{
+		if (cmd->opc == opc)
+		{
+			TAILQ_REMOVE(&qpair->err_cmd_head, cmd, link);
+			spdk_free(cmd);
+			break;
+		}
+	}
+
+	if (client_qpair_is_admin_queue(qpair))
+	{
+		client_robust_mutex_unlock(&ctrlr->ctrlr_lock);
+	}
+}
+
+uint16_t
+spdk_client_qpair_get_id(struct spdk_client_qpair *qpair)
+{
+	return qpair->id;
+}
diff --git a/lib/rdma_server/rdma_c.c b/lib/rdma_server/rdma_c.c
new file mode 100644
index 000000000..59dc955d2
--- /dev/null
+++ b/lib/rdma_server/rdma_c.c
@@ -0,0 +1,3631 @@
+/*-
+ *   BSD LICENSE
+ *
+ *   Copyright (c) Intel Corporation. All rights reserved.
+ *   Copyright (c) 2019-2021 Mellanox Technologies LTD. All rights reserved.
+ *   Copyright (c) 2021, 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ *
+ *   Redistribution and use in source and binary forms, with or without
+ *   modification, are permitted provided that the following conditions
+ *   are met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in
+ *       the documentation and/or other materials provided with the
+ *       distribution.
+ *     * Neither the name of Intel Corporation nor the names of its
+ *       contributors may be used to endorse or promote products derived
+ *       from this software without specific prior written permission.
+ *
+ *   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ *   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ *   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ *   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ *   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ *   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ *   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ *   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ *   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ *   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ *   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/*
+ * Client over RDMA transport
+ */
+
+#include "spdk/stdinc.h"
+
+#include "spdk/assert.h"
+#include "spdk/dma.h"
+#include "spdk/log.h"
+#include "spdk/trace.h"
+#include "spdk/queue.h"
+#include "spdk/string.h"
+#include "spdk/endian.h"
+#include "spdk/likely.h"
+#include "spdk/config.h"
+
+#include "spdk/rdma_client.h"
+#include "spdk_internal/rdma_client.h"
+#include "spdk_internal/rdma.h"
+
+#define CLIENT_RDMA_TIME_OUT_IN_MS 2000
+#define CLIENT_RDMA_RW_BUFFER_SIZE 131072
+
+/*
+ * CLIENT RDMA qpair Resource Defaults
+ */
+#define CLIENT_RDMA_DEFAULT_TX_SGE 2
+#define CLIENT_RDMA_DEFAULT_RX_SGE 1
+
+/* Max number of Client-oF SGL descriptors supported by the host */
+#define CLIENT_RDMA_MAX_SGL_DESCRIPTORS 16
+
+/* number of STAILQ entries for holding pending RDMA CM events. */
+#define CLIENT_RDMA_NUM_CM_EVENTS 256
+
+/* CM event processing timeout */
+#define CLIENT_RDMA_QPAIR_CM_EVENT_TIMEOUT_US 1000000
+
+/* The default size for a shared rdma completion queue. */
+#define DEFAULT_CLIENT_RDMA_CQ_SIZE 4096
+
+/*
+ * In the special case of a stale connection we don't expose a mechanism
+ * for the user to retry the connection so we need to handle it internally.
+ */
+#define CLIENT_RDMA_STALE_CONN_RETRY_MAX 5
+#define CLIENT_RDMA_STALE_CONN_RETRY_DELAY_US 10000
+
+/*
+ * Maximum value of transport_retry_count used by RDMA controller
+ */
+#define CLIENT_RDMA_CTRLR_MAX_TRANSPORT_RETRY_COUNT 7
+
+/*
+ * Maximum value of transport_ack_timeout used by RDMA controller
+ */
+#define CLIENT_RDMA_CTRLR_MAX_TRANSPORT_ACK_TIMEOUT 31
+
+/*
+ * Number of microseconds to keep a pointer to destroyed qpairs
+ * in the poll group.
+ */
+#define CLIENT_RDMA_DESTROYED_QPAIR_EXPIRATION_TIMEOUT_US 1000000ull
+
+/*
+ * Number of microseconds to wait until the lingering qpair becomes quiet.
+ */
+#define CLIENT_RDMA_DISCONNECTED_QPAIR_TIMEOUT_US 1000000ull
+
+/*
+ * The max length of keyed SGL data block (3 bytes)
+ */
+#define CLIENT_RDMA_MAX_KEYED_SGL_LENGTH ((1u << 24u) - 1)
+
+#define WC_PER_QPAIR(queue_depth) (queue_depth * 2)
+
+#define CLIENT_RDMA_POLL_GROUP_CHECK_QPN(_rqpair, qpn) \
+	((_rqpair)->rdma_qp && (_rqpair)->rdma_qp->qp->qp_num == (qpn))
+
+struct client_rdma_memory_domain
+{
+	TAILQ_ENTRY(client_rdma_memory_domain)
+	link;
+	uint32_t ref;
+	struct ibv_pd *pd;
+	struct spdk_memory_domain *domain;
+	struct spdk_memory_domain_rdma_ctx rdma_ctx;
+};
+
+enum client_rdma_wr_type
+{
+	RDMA_WR_TYPE_RECV,
+	RDMA_WR_TYPE_SEND,
+};
+
+struct client_rdma_wr
+{
+	/* Using this instead of the enum allows this struct to only occupy one byte. */
+	uint8_t type;
+};
+
+struct spdk_client_cmd
+{
+	struct spdk_req_cmd cmd;
+	struct spdk_req_sgl_descriptor sgl[CLIENT_RDMA_MAX_SGL_DESCRIPTORS];
+};
+
+struct spdk_client_rdma_hooks g_client_hooks = {};
+
+/* STAILQ wrapper for cm events. */
+struct client_rdma_cm_event_entry
+{
+	struct rdma_cm_event *evt;
+	STAILQ_ENTRY(client_rdma_cm_event_entry)
+	link;
+};
+
+/* Client RDMA transport extensions for spdk_client_ctrlr */
+struct client_rdma_ctrlr
+{
+	struct spdk_client_ctrlr ctrlr;
+
+	struct ibv_pd *pd;
+
+	uint16_t max_sge;
+
+	struct rdma_event_channel *cm_channel;
+
+	STAILQ_HEAD(, client_rdma_cm_event_entry)
+	pending_cm_events;
+
+	STAILQ_HEAD(, client_rdma_cm_event_entry)
+	free_cm_events;
+
+	struct client_rdma_cm_event_entry *cm_events;
+};
+
+struct client_rdma_destroyed_qpair
+{
+	struct client_rdma_qpair *destroyed_qpair_tracker;
+	uint64_t timeout_ticks;
+	STAILQ_ENTRY(client_rdma_destroyed_qpair)
+	link;
+};
+
+struct client_rdma_poller_stats
+{
+	uint64_t polls;
+	uint64_t idle_polls;
+	uint64_t queued_requests;
+	uint64_t completions;
+	struct spdk_rdma_qp_stats rdma_stats;
+};
+
+struct client_rdma_poller
+{
+	struct ibv_context *device;
+	struct ibv_cq *cq;
+	int required_num_wc;
+	int current_num_wc;
+	struct client_rdma_poller_stats stats;
+	STAILQ_ENTRY(client_rdma_poller)
+	link;
+};
+
+typedef int (*client_rdma_cm_event_cb)(struct client_rdma_qpair *rqpair, int ret);
+
+struct client_rdma_poll_group
+{
+	struct spdk_client_transport_poll_group group;
+	STAILQ_HEAD(, client_rdma_poller)
+	pollers;
+	uint32_t num_pollers;
+	STAILQ_HEAD(, client_rdma_destroyed_qpair)
+	destroyed_qpairs;
+};
+
+/* Memory regions */
+union client_rdma_mr
+{
+	struct ibv_mr *mr;
+	uint64_t key;
+};
+
+enum client_rdma_qpair_state
+{
+	CLIENT_RDMA_QPAIR_STATE_INVALID = 0,
+	CLIENT_RDMA_QPAIR_STATE_STALE_CONN,
+	CLIENT_RDMA_QPAIR_STATE_INITIALIZING,
+	CLIENT_RDMA_QPAIR_STATE_RUNNING,
+	CLIENT_RDMA_QPAIR_STATE_EXITING,
+	CLIENT_RDMA_QPAIR_STATE_LINGERING,
+	CLIENT_RDMA_QPAIR_STATE_EXITED,
+};
+
+/* Client RDMA qpair extensions for spdk_client_qpair */
+struct client_rdma_qpair
+{
+	struct spdk_client_qpair qpair;
+
+	struct spdk_rdma_qp *rdma_qp;
+	struct rdma_cm_id *cm_id;
+	struct ibv_cq *cq;
+
+	struct spdk_client_rdma_req *rdma_reqs;
+
+	uint32_t max_send_sge;
+
+	uint32_t max_recv_sge;
+
+	uint16_t num_entries;
+
+	bool delay_cmd_submit;
+
+	uint32_t num_completions;
+
+	/* Parallel arrays of response buffers + response SGLs of size num_entries */
+	struct ibv_sge *rsp_sgls;
+	struct spdk_client_rdma_rsp *rsps;
+
+	struct ibv_recv_wr *rsp_recv_wrs;
+
+	/* Memory region describing all rsps for this qpair */
+	union client_rdma_mr rsp_mr;
+
+	/*
+	 * Array of num_entries Client commands registered as RDMA message buffers.
+	 * Indexed by rdma_req->id.
+	 */
+	struct spdk_client_cmd *cmds;
+
+	/* Memory region describing all cmds for this qpair */
+	union client_rdma_mr cmd_mr;
+
+	struct spdk_rdma_mem_map *mr_map;
+
+	TAILQ_HEAD(, spdk_client_rdma_req)
+	free_reqs;
+	TAILQ_HEAD(, spdk_client_rdma_req)
+	outstanding_reqs;
+
+	struct client_rdma_memory_domain *memory_domain;
+
+	/* Counts of outstanding send and recv objects */
+	uint16_t current_num_recvs;
+	uint16_t current_num_sends;
+
+	/* Placed at the end of the struct since it is not used frequently */
+	struct rdma_cm_event *evt;
+	struct client_rdma_poller *poller;
+
+	enum client_rdma_qpair_state state;
+
+	bool in_connect_poll;
+
+	uint64_t evt_timeout_ticks;
+	client_rdma_cm_event_cb evt_cb;
+	enum rdma_cm_event_type expected_evt_type;
+
+	uint8_t stale_conn_retry_count;
+	/* Used by poll group to keep the qpair around until it is ready to remove it. */
+	bool defer_deletion_to_pg;
+};
+
+enum CLIENT_RDMA_COMPLETION_FLAGS
+{
+	CLIENT_RDMA_SEND_COMPLETED = 1u << 0,
+	CLIENT_RDMA_RECV_COMPLETED = 1u << 1,
+};
+
+struct spdk_client_rdma_req
+{
+	uint16_t id;
+	uint16_t completion_flags : 2;
+	uint16_t reserved : 14;
+	/* if completion of RDMA_RECV received before RDMA_SEND, we will complete client request
+	 * during processing of RDMA_SEND. To complete the request we must know the index
+	 * of client_cpl received in RDMA_RECV, so store it in this field */
+	uint16_t rsp_idx;
+
+	struct client_rdma_wr rdma_wr;
+
+	struct ibv_send_wr send_wr;
+
+	struct client_request *req;
+
+	struct ibv_sge send_sgl[CLIENT_RDMA_DEFAULT_TX_SGE];
+
+	TAILQ_ENTRY(spdk_client_rdma_req)
+	link;
+};
+
+struct spdk_client_rdma_rsp
+{
+	struct spdk_req_cpl cpl;
+	struct client_rdma_qpair *rqpair;
+	uint16_t idx;
+	struct client_rdma_wr rdma_wr;
+};
+
+struct client_rdma_memory_translation_ctx
+{
+	void *addr;
+	size_t length;
+	uint32_t lkey;
+	uint32_t rkey;
+};
+
+static const char *rdma_cm_event_str[] = {
+	"RDMA_CM_EVENT_ADDR_RESOLVED",
+	"RDMA_CM_EVENT_ADDR_ERROR",
+	"RDMA_CM_EVENT_ROUTE_RESOLVED",
+	"RDMA_CM_EVENT_ROUTE_ERROR",
+	"RDMA_CM_EVENT_CONNECT_REQUEST",
+	"RDMA_CM_EVENT_CONNECT_RESPONSE",
+	"RDMA_CM_EVENT_CONNECT_ERROR",
+	"RDMA_CM_EVENT_UNREACHABLE",
+	"RDMA_CM_EVENT_REJECTED",
+	"RDMA_CM_EVENT_ESTABLISHED",
+	"RDMA_CM_EVENT_DISCONNECTED",
+	"RDMA_CM_EVENT_DEVICE_REMOVAL",
+	"RDMA_CM_EVENT_MULTICAST_JOIN",
+	"RDMA_CM_EVENT_MULTICAST_ERROR",
+	"RDMA_CM_EVENT_ADDR_CHANGE",
+	"RDMA_CM_EVENT_TIMEWAIT_EXIT"};
+
+struct client_rdma_qpair *client_rdma_poll_group_get_qpair_by_id(struct client_rdma_poll_group *group,
+																 uint32_t qp_num);
+
+static TAILQ_HEAD(, client_rdma_memory_domain) g_memory_domains = TAILQ_HEAD_INITIALIZER(
+	g_memory_domains);
+static pthread_mutex_t g_memory_domains_lock = PTHREAD_MUTEX_INITIALIZER;
+
+static struct client_rdma_memory_domain *
+client_rdma_get_memory_domain(struct ibv_pd *pd)
+{
+	struct client_rdma_memory_domain *domain = NULL;
+	struct spdk_memory_domain_ctx ctx;
+	int rc;
+
+	pthread_mutex_lock(&g_memory_domains_lock);
+
+	TAILQ_FOREACH(domain, &g_memory_domains, link)
+	{
+		if (domain->pd == pd)
+		{
+			domain->ref++;
+			pthread_mutex_unlock(&g_memory_domains_lock);
+			return domain;
+		}
+	}
+
+	domain = calloc(1, sizeof(*domain));
+	if (!domain)
+	{
+		SPDK_ERRLOG("Memory allocation failed\n");
+		pthread_mutex_unlock(&g_memory_domains_lock);
+		return NULL;
+	}
+
+	domain->rdma_ctx.size = sizeof(domain->rdma_ctx);
+	domain->rdma_ctx.ibv_pd = pd;
+	ctx.size = sizeof(ctx);
+	ctx.user_ctx = &domain->rdma_ctx;
+
+	rc = spdk_memory_domain_create(&domain->domain, SPDK_DMA_DEVICE_TYPE_RDMA, &ctx,
+								   SPDK_RDMA_DMA_DEVICE);
+	if (rc)
+	{
+		SPDK_ERRLOG("Failed to create memory domain\n");
+		free(domain);
+		pthread_mutex_unlock(&g_memory_domains_lock);
+		return NULL;
+	}
+
+	domain->pd = pd;
+	domain->ref = 1;
+	TAILQ_INSERT_TAIL(&g_memory_domains, domain, link);
+
+	pthread_mutex_unlock(&g_memory_domains_lock);
+
+	return domain;
+}
+
+static void
+client_rdma_put_memory_domain(struct client_rdma_memory_domain *device)
+{
+	if (!device)
+	{
+		return;
+	}
+
+	pthread_mutex_lock(&g_memory_domains_lock);
+
+	assert(device->ref > 0);
+
+	device->ref--;
+
+	if (device->ref == 0)
+	{
+		spdk_memory_domain_destroy(device->domain);
+		TAILQ_REMOVE(&g_memory_domains, device, link);
+		free(device);
+	}
+
+	pthread_mutex_unlock(&g_memory_domains_lock);
+}
+
+static inline void *
+client_rdma_calloc(size_t nmemb, size_t size)
+{
+	if (!nmemb || !size)
+	{
+		return NULL;
+	}
+
+	if (!g_client_hooks.get_rkey)
+	{
+		return calloc(nmemb, size);
+	}
+	else
+	{
+		return spdk_zmalloc(nmemb * size, 0, NULL, SPDK_ENV_SOCKET_ID_ANY, SPDK_MALLOC_DMA);
+	}
+}
+
+static inline void
+client_rdma_free(void *buf)
+{
+	if (!g_client_hooks.get_rkey)
+	{
+		free(buf);
+	}
+	else
+	{
+		spdk_free(buf);
+	}
+}
+
+static int client_rdma_ctrlr_delete_io_qpair(struct spdk_client_ctrlr *ctrlr,
+											 struct spdk_client_qpair *qpair);
+
+static inline struct client_rdma_qpair *
+client_rdma_qpair(struct spdk_client_qpair *qpair)
+{
+	assert(qpair->trtype == SPDK_CLIENT_TRANSPORT_RDMA);
+	return SPDK_CONTAINEROF(qpair, struct client_rdma_qpair, qpair);
+}
+
+static inline struct client_rdma_poll_group *
+client_rdma_poll_group(struct spdk_client_transport_poll_group *group)
+{
+	return (SPDK_CONTAINEROF(group, struct client_rdma_poll_group, group));
+}
+
+static inline struct client_rdma_ctrlr *
+client_rdma_ctrlr(struct spdk_client_ctrlr *ctrlr)
+{
+	return SPDK_CONTAINEROF(ctrlr, struct client_rdma_ctrlr, ctrlr);
+}
+
+static struct spdk_client_rdma_req *
+client_rdma_req_get(struct client_rdma_qpair *rqpair)
+{
+	struct spdk_client_rdma_req *rdma_req;
+
+	rdma_req = TAILQ_FIRST(&rqpair->free_reqs);
+	if (rdma_req)
+	{
+		TAILQ_REMOVE(&rqpair->free_reqs, rdma_req, link);
+		TAILQ_INSERT_TAIL(&rqpair->outstanding_reqs, rdma_req, link);
+	}
+
+	return rdma_req;
+}
+
+static void
+client_rdma_req_put(struct client_rdma_qpair *rqpair, struct spdk_client_rdma_req *rdma_req)
+{
+	rdma_req->completion_flags = 0;
+	rdma_req->req = NULL;
+	TAILQ_INSERT_HEAD(&rqpair->free_reqs, rdma_req, link);
+}
+
+static void
+client_rdma_req_complete(struct spdk_client_rdma_req *rdma_req,
+						 struct spdk_req_cpl *rsp)
+{
+	struct client_request *req = rdma_req->req;
+	struct client_rdma_qpair *rqpair;
+
+	assert(req != NULL);
+
+	rqpair = client_rdma_qpair(req->qpair);
+	TAILQ_REMOVE(&rqpair->outstanding_reqs, rdma_req, link);
+
+	client_complete_request(req->cb_fn, req->cb_arg, req->qpair, req, rsp);
+	client_free_request(req);
+}
+
+static const char *
+client_rdma_cm_event_str_get(uint32_t event)
+{
+	if (event < SPDK_COUNTOF(rdma_cm_event_str))
+	{
+		return rdma_cm_event_str[event];
+	}
+	else
+	{
+		return "Undefined";
+	}
+}
+
+void client_transport_ctrlr_disconnect_qpair_done(struct spdk_client_qpair *qpair)
+{
+	client_qpair_abort_all_queued_reqs(qpair, 0);
+	client_transport_qpair_abort_reqs(qpair, 0);
+	client_qpair_set_state(qpair, CLIENT_QPAIR_DISCONNECTED);
+}
+
+static int
+client_rdma_qpair_process_cm_event(struct client_rdma_qpair *rqpair)
+{
+	struct rdma_cm_event *event = rqpair->evt;
+	struct spdk_srv_rdma_accept_private_data *accept_data;
+	int rc = 0;
+
+	if (event)
+	{
+		switch (event->event)
+		{
+		case RDMA_CM_EVENT_ADDR_RESOLVED:
+		case RDMA_CM_EVENT_ADDR_ERROR:
+		case RDMA_CM_EVENT_ROUTE_RESOLVED:
+		case RDMA_CM_EVENT_ROUTE_ERROR:
+			break;
+		case RDMA_CM_EVENT_CONNECT_REQUEST:
+			break;
+		case RDMA_CM_EVENT_CONNECT_ERROR:
+			break;
+		case RDMA_CM_EVENT_UNREACHABLE:
+		case RDMA_CM_EVENT_REJECTED:
+			break;
+		case RDMA_CM_EVENT_CONNECT_RESPONSE:
+			rc = spdk_rdma_qp_complete_connect(rqpair->rdma_qp);
+		/* fall through */
+		case RDMA_CM_EVENT_ESTABLISHED:
+			accept_data = (struct spdk_srv_rdma_accept_private_data *)event->param.conn.private_data;
+			if (accept_data == NULL)
+			{
+				rc = -1;
+			}
+			else
+			{
+				SPDK_NOTICELOG("client_rdma_qpair_process_cm_event num_entries before: %d\n", rqpair->num_entries);
+				SPDK_DEBUGLOG(client, "Requested queue depth %d. Actually got queue depth %d.\n",
+							  rqpair->num_entries, accept_data->crqsize);
+				rqpair->num_entries = spdk_min(rqpair->num_entries, accept_data->crqsize);
+				SPDK_NOTICELOG("client_rdma_qpair_process_cm_event num_entries: %d %d\n", rqpair->num_entries, accept_data->crqsize);
+			}
+			break;
+		case RDMA_CM_EVENT_DISCONNECTED:
+			rqpair->qpair.transport_failure_reason = SPDK_CLIENT_QPAIR_FAILURE_REMOTE;
+			break;
+		case RDMA_CM_EVENT_DEVICE_REMOVAL:
+			rqpair->qpair.transport_failure_reason = SPDK_CLIENT_QPAIR_FAILURE_LOCAL;
+			break;
+		case RDMA_CM_EVENT_MULTICAST_JOIN:
+		case RDMA_CM_EVENT_MULTICAST_ERROR:
+			break;
+		case RDMA_CM_EVENT_ADDR_CHANGE:
+			rqpair->qpair.transport_failure_reason = SPDK_CLIENT_QPAIR_FAILURE_LOCAL;
+			break;
+		case RDMA_CM_EVENT_TIMEWAIT_EXIT:
+			break;
+		default:
+			SPDK_ERRLOG("Unexpected Acceptor Event [%d]\n", event->event);
+			break;
+		}
+		rqpair->evt = NULL;
+		rdma_ack_cm_event(event);
+	}
+
+	return rc;
+}
+
+/*
+ * This function must be called under the client controller's lock
+ * because it touches global controller variables. The lock is taken
+ * by the generic transport code before invoking a few of the functions
+ * in this file: client_rdma_ctrlr_connect_qpair, client_rdma_ctrlr_delete_io_qpair,
+ * and conditionally client_rdma_qpair_process_completions when it is calling
+ * completions on the admin qpair. When adding a new call to this function, please
+ * verify that it is in a situation where it falls under the lock.
+ */
+static int
+client_rdma_poll_events(struct client_rdma_ctrlr *rctrlr)
+{
+	struct client_rdma_cm_event_entry *entry, *tmp;
+	struct client_rdma_qpair *event_qpair;
+	struct rdma_cm_event *event;
+	struct rdma_event_channel *channel = rctrlr->cm_channel;
+
+	STAILQ_FOREACH_SAFE(entry, &rctrlr->pending_cm_events, link, tmp)
+	{
+		event_qpair = entry->evt->id->context;
+		if (event_qpair->evt == NULL)
+		{
+			event_qpair->evt = entry->evt;
+			STAILQ_REMOVE(&rctrlr->pending_cm_events, entry, client_rdma_cm_event_entry, link);
+			STAILQ_INSERT_HEAD(&rctrlr->free_cm_events, entry, link);
+		}
+	}
+
+	while (rdma_get_cm_event(channel, &event) == 0)
+	{
+		event_qpair = event->id->context;
+		if (event_qpair->evt == NULL)
+		{
+			event_qpair->evt = event;
+		}
+		else
+		{
+			assert(rctrlr == client_rdma_ctrlr(event_qpair->qpair.ctrlr));
+			entry = STAILQ_FIRST(&rctrlr->free_cm_events);
+			if (entry == NULL)
+			{
+				rdma_ack_cm_event(event);
+				return -ENOMEM;
+			}
+			STAILQ_REMOVE(&rctrlr->free_cm_events, entry, client_rdma_cm_event_entry, link);
+			entry->evt = event;
+			STAILQ_INSERT_TAIL(&rctrlr->pending_cm_events, entry, link);
+		}
+	}
+
+	/* rdma_get_cm_event() returns -1 on error. If an error occurs, errno
+	 * will be set to indicate the failure reason. So return negated errno here.
+	 */
+	return -errno;
+}
+
+static int
+client_rdma_validate_cm_event(enum rdma_cm_event_type expected_evt_type,
+							  struct rdma_cm_event *reaped_evt)
+{
+	int rc = -EBADMSG;
+
+	if (expected_evt_type == reaped_evt->event)
+	{
+		return 0;
+	}
+
+	switch (expected_evt_type)
+	{
+	case RDMA_CM_EVENT_ESTABLISHED:
+		/*
+		 * There is an enum ib_cm_rej_reason in the kernel headers that sets 10 as
+		 * IB_CM_REJ_STALE_CONN. I can't find the corresponding userspace but we get
+		 * the same values here.
+		 */
+		if (reaped_evt->event == RDMA_CM_EVENT_REJECTED && reaped_evt->status == 10)
+		{
+			rc = -ESTALE;
+		}
+		else if (reaped_evt->event == RDMA_CM_EVENT_CONNECT_RESPONSE)
+		{
+			/*
+			 *  If we are using a qpair which is not created using rdma cm API
+			 *  then we will receive RDMA_CM_EVENT_CONNECT_RESPONSE instead of
+			 *  RDMA_CM_EVENT_ESTABLISHED.
+			 */
+			return 0;
+		}
+		break;
+	default:
+		break;
+	}
+
+	SPDK_ERRLOG("Expected %s but received %s (%d) from CM event channel (status = %d)\n",
+				client_rdma_cm_event_str_get(expected_evt_type),
+				client_rdma_cm_event_str_get(reaped_evt->event), reaped_evt->event,
+				reaped_evt->status);
+	return rc;
+}
+static int
+client_rdma_process_event_start(struct client_rdma_qpair *rqpair,
+								enum rdma_cm_event_type evt,
+								client_rdma_cm_event_cb evt_cb)
+{
+	int rc;
+
+	assert(evt_cb != NULL);
+
+	if (rqpair->evt != NULL)
+	{
+		rc = client_rdma_qpair_process_cm_event(rqpair);
+		if (rc)
+		{
+			return rc;
+		}
+	}
+
+	rqpair->expected_evt_type = evt;
+	rqpair->evt_cb = evt_cb;
+	rqpair->evt_timeout_ticks = (CLIENT_RDMA_QPAIR_CM_EVENT_TIMEOUT_US * spdk_get_ticks_hz()) /
+									SPDK_SEC_TO_USEC +
+								spdk_get_ticks();
+
+	return 0;
+}
+
+static int
+client_rdma_process_event_poll(struct client_rdma_qpair *rqpair)
+{
+	struct client_rdma_ctrlr *rctrlr;
+	int rc = 0, rc2;
+
+	rctrlr = client_rdma_ctrlr(rqpair->qpair.ctrlr);
+	assert(rctrlr != NULL);
+
+	if (!rqpair->evt && spdk_get_ticks() < rqpair->evt_timeout_ticks)
+	{
+		rc = client_rdma_poll_events(rctrlr);
+		if (rc == -EAGAIN || rc == -EWOULDBLOCK)
+		{
+			return rc;
+		}
+	}
+
+	if (rqpair->evt == NULL)
+	{
+		rc = -EADDRNOTAVAIL;
+		goto exit;
+	}
+
+	rc = client_rdma_validate_cm_event(rqpair->expected_evt_type, rqpair->evt);
+
+	rc2 = client_rdma_qpair_process_cm_event(rqpair);
+	/* bad message takes precedence over the other error codes from processing the event. */
+	rc = rc == 0 ? rc2 : rc;
+
+exit:
+	assert(rqpair->evt_cb != NULL);
+	return rqpair->evt_cb(rqpair, rc);
+}
+
+static int
+client_rdma_resize_cq(struct client_rdma_qpair *rqpair, struct client_rdma_poller *poller)
+{
+	int current_num_wc, required_num_wc;
+
+	required_num_wc = poller->required_num_wc + WC_PER_QPAIR(rqpair->num_entries);
+	current_num_wc = poller->current_num_wc;
+	if (current_num_wc < required_num_wc)
+	{
+		current_num_wc = spdk_max(current_num_wc * 2, required_num_wc);
+	}
+
+	if (poller->current_num_wc != current_num_wc)
+	{
+		SPDK_DEBUGLOG(client, "Resize RDMA CQ from %d to %d\n", poller->current_num_wc,
+					  current_num_wc);
+		if (ibv_resize_cq(poller->cq, current_num_wc))
+		{
+			SPDK_ERRLOG("RDMA CQ resize failed: errno %d: %s\n", errno, spdk_strerror(errno));
+			return -1;
+		}
+
+		poller->current_num_wc = current_num_wc;
+	}
+
+	poller->required_num_wc = required_num_wc;
+	return 0;
+}
+
+static int
+client_rdma_poll_group_set_cq(struct spdk_client_qpair *qpair)
+{
+	struct client_rdma_qpair *rqpair = client_rdma_qpair(qpair);
+	struct client_rdma_poll_group *group = client_rdma_poll_group(qpair->poll_group);
+	struct client_rdma_poller *poller;
+
+	assert(rqpair->cq == NULL);
+
+	STAILQ_FOREACH(poller, &group->pollers, link)
+	{
+		if (poller->device == rqpair->cm_id->verbs)
+		{
+			if (client_rdma_resize_cq(rqpair, poller))
+			{
+				return -EPROTO;
+			}
+			rqpair->cq = poller->cq;
+			rqpair->poller = poller;
+			break;
+		}
+	}
+
+	if (rqpair->cq == NULL)
+	{
+		SPDK_ERRLOG("Unable to find a cq for qpair %p on poll group %p\n", qpair, qpair->poll_group);
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static int
+client_rdma_qpair_init(struct client_rdma_qpair *rqpair)
+{
+	int rc;
+	struct spdk_rdma_qp_init_attr attr = {};
+	struct ibv_device_attr dev_attr;
+	struct client_rdma_ctrlr *rctrlr;
+
+	rc = ibv_query_device(rqpair->cm_id->verbs, &dev_attr);
+	if (rc != 0)
+	{
+		SPDK_ERRLOG("Failed to query RDMA device attributes.\n");
+		return -1;
+	}
+
+	if (rqpair->qpair.poll_group)
+	{
+		assert(!rqpair->cq);
+		rc = client_rdma_poll_group_set_cq(&rqpair->qpair);
+		if (rc)
+		{
+			SPDK_ERRLOG("Unable to activate the rdmaqpair.\n");
+			return -1;
+		}
+		assert(rqpair->cq);
+	}
+	else
+	{
+		rqpair->cq = ibv_create_cq(rqpair->cm_id->verbs, rqpair->num_entries * 2, rqpair, NULL, 0);
+		if (!rqpair->cq)
+		{
+			SPDK_ERRLOG("Unable to create completion queue: errno %d: %s\n", errno, spdk_strerror(errno));
+			return -1;
+		}
+	}
+
+	rctrlr = client_rdma_ctrlr(rqpair->qpair.ctrlr);
+	if (g_client_hooks.get_ibv_pd)
+	{
+		rctrlr->pd = g_client_hooks.get_ibv_pd(rqpair->qpair.trid, rqpair->cm_id->verbs);
+	}
+	else
+	{
+		rctrlr->pd = NULL;
+	}
+
+	attr.pd = rctrlr->pd;
+	attr.stats = rqpair->poller ? &rqpair->poller->stats.rdma_stats : NULL;
+	attr.send_cq = rqpair->cq;
+	attr.recv_cq = rqpair->cq;
+	attr.cap.max_send_wr = rqpair->num_entries; /* SEND operations */
+	attr.cap.max_recv_wr = rqpair->num_entries; /* RECV operations */
+	attr.cap.max_send_sge = spdk_min(CLIENT_RDMA_DEFAULT_TX_SGE, dev_attr.max_sge);
+	attr.cap.max_recv_sge = spdk_min(CLIENT_RDMA_DEFAULT_RX_SGE, dev_attr.max_sge);
+
+	rqpair->rdma_qp = spdk_rdma_qp_create(rqpair->cm_id, &attr);
+
+	if (!rqpair->rdma_qp)
+	{
+		return -1;
+	}
+
+	rqpair->memory_domain = client_rdma_get_memory_domain(rqpair->rdma_qp->qp->pd);
+	if (!rqpair->memory_domain)
+	{
+		SPDK_ERRLOG("Failed to get memory domain\n");
+		return -1;
+	}
+
+	/* ibv_create_qp will change the values in attr.cap. Make sure we store the proper value. */
+	rqpair->max_send_sge = spdk_min(CLIENT_RDMA_DEFAULT_TX_SGE, attr.cap.max_send_sge);
+	rqpair->max_recv_sge = spdk_min(CLIENT_RDMA_DEFAULT_RX_SGE, attr.cap.max_recv_sge);
+	rqpair->current_num_recvs = 0;
+	rqpair->current_num_sends = 0;
+
+	rctrlr->pd = rqpair->rdma_qp->qp->pd;
+
+	rqpair->cm_id->context = rqpair;
+
+	return 0;
+}
+
+static inline int
+client_rdma_qpair_submit_sends(struct client_rdma_qpair *rqpair)
+{
+	struct ibv_send_wr *bad_send_wr = NULL;
+	int rc;
+
+	rc = spdk_rdma_qp_flush_send_wrs(rqpair->rdma_qp, &bad_send_wr);
+
+	if (spdk_unlikely(rc))
+	{
+		SPDK_ERRLOG("Failed to post WRs on send queue, errno %d (%s), bad_wr %p\n",
+					rc, spdk_strerror(rc), bad_send_wr);
+		while (bad_send_wr != NULL)
+		{
+			assert(rqpair->current_num_sends > 0);
+			rqpair->current_num_sends--;
+			bad_send_wr = bad_send_wr->next;
+		}
+		return rc;
+	}
+
+	return 0;
+}
+
+static inline int
+client_rdma_qpair_submit_recvs(struct client_rdma_qpair *rqpair)
+{
+	struct ibv_recv_wr *bad_recv_wr;
+	int rc = 0;
+
+	rc = spdk_rdma_qp_flush_recv_wrs(rqpair->rdma_qp, &bad_recv_wr);
+	if (spdk_unlikely(rc))
+	{
+		SPDK_ERRLOG("Failed to post WRs on receive queue, errno %d (%s), bad_wr %p\n",
+					rc, spdk_strerror(rc), bad_recv_wr);
+		while (bad_recv_wr != NULL)
+		{
+			assert(rqpair->current_num_sends > 0);
+			rqpair->current_num_recvs--;
+			bad_recv_wr = bad_recv_wr->next;
+		}
+	}
+
+	return rc;
+}
+
+/* Append the given send wr structure to the qpair's outstanding sends list. */
+/* This function accepts only a single wr. */
+static inline int
+client_rdma_qpair_queue_send_wr(struct client_rdma_qpair *rqpair, struct ibv_send_wr *wr)
+{
+	assert(wr->next == NULL);
+
+	assert(rqpair->current_num_sends < rqpair->num_entries);
+
+	rqpair->current_num_sends++;
+	spdk_rdma_qp_queue_send_wrs(rqpair->rdma_qp, wr);
+
+	if (!rqpair->delay_cmd_submit)
+	{
+		return client_rdma_qpair_submit_sends(rqpair);
+	}
+
+	return 0;
+}
+
+/* Append the given recv wr structure to the qpair's outstanding recvs list. */
+/* This function accepts only a single wr. */
+static inline int
+client_rdma_qpair_queue_recv_wr(struct client_rdma_qpair *rqpair, struct ibv_recv_wr *wr)
+{
+
+	assert(wr->next == NULL);
+	assert(rqpair->current_num_recvs < rqpair->num_entries);
+
+	rqpair->current_num_recvs++;
+	spdk_rdma_qp_queue_recv_wrs(rqpair->rdma_qp, wr);
+
+	if (!rqpair->delay_cmd_submit)
+	{
+		return client_rdma_qpair_submit_recvs(rqpair);
+	}
+
+	return 0;
+}
+
+#define client_rdma_trace_ibv_sge(sg_list)                                          \
+	if (sg_list)                                                                    \
+	{                                                                               \
+		SPDK_DEBUGLOG(client, "local addr %p length 0x%x lkey 0x%x\n",              \
+					  (void *)(sg_list)->addr, (sg_list)->length, (sg_list)->lkey); \
+	}
+
+static int
+client_rdma_post_recv(struct client_rdma_qpair *rqpair, uint16_t rsp_idx)
+{
+	struct ibv_recv_wr *wr;
+
+	wr = &rqpair->rsp_recv_wrs[rsp_idx];
+	wr->next = NULL;
+	client_rdma_trace_ibv_sge(wr->sg_list);
+	return client_rdma_qpair_queue_recv_wr(rqpair, wr);
+}
+
+static int
+client_rdma_reg_mr(struct rdma_cm_id *cm_id, union client_rdma_mr *mr, void *mem, size_t length)
+{
+	if (!g_client_hooks.get_rkey)
+	{
+		mr->mr = rdma_reg_msgs(cm_id, mem, length);
+		if (mr->mr == NULL)
+		{
+			SPDK_ERRLOG("Unable to register mr: %s (%d)\n",
+						spdk_strerror(errno), errno);
+			return -1;
+		}
+	}
+	else
+	{
+		mr->key = g_client_hooks.get_rkey(cm_id->pd, mem, length);
+	}
+
+	return 0;
+}
+
+static void
+client_rdma_dereg_mr(union client_rdma_mr *mr)
+{
+	if (!g_client_hooks.get_rkey)
+	{
+		if (mr->mr && rdma_dereg_mr(mr->mr))
+		{
+			SPDK_ERRLOG("Unable to de-register mr\n");
+		}
+	}
+	else
+	{
+		if (mr->key)
+		{
+			g_client_hooks.put_rkey(mr->key);
+		}
+	}
+	memset(mr, 0, sizeof(*mr));
+}
+
+static uint32_t
+client_rdma_mr_get_lkey(union client_rdma_mr *mr)
+{
+	uint32_t lkey;
+
+	if (!g_client_hooks.get_rkey)
+	{
+		lkey = mr->mr->lkey;
+	}
+	else
+	{
+		lkey = *((uint64_t *)mr->key);
+	}
+
+	return lkey;
+}
+
+static void
+client_rdma_unregister_rsps(struct client_rdma_qpair *rqpair)
+{
+	client_rdma_dereg_mr(&rqpair->rsp_mr);
+}
+
+static void
+client_rdma_free_rsps(struct client_rdma_qpair *rqpair)
+{
+	client_rdma_free(rqpair->rsps);
+	rqpair->rsps = NULL;
+	client_rdma_free(rqpair->rsp_sgls);
+	rqpair->rsp_sgls = NULL;
+	client_rdma_free(rqpair->rsp_recv_wrs);
+	rqpair->rsp_recv_wrs = NULL;
+}
+
+static int
+client_rdma_alloc_rsps(struct client_rdma_qpair *rqpair)
+{
+	rqpair->rsps = NULL;
+	rqpair->rsp_recv_wrs = NULL;
+
+	rqpair->rsp_sgls = client_rdma_calloc(rqpair->num_entries, sizeof(*rqpair->rsp_sgls));
+	if (!rqpair->rsp_sgls)
+	{
+		SPDK_ERRLOG("Failed to allocate rsp_sgls\n");
+		goto fail;
+	}
+
+	rqpair->rsp_recv_wrs = client_rdma_calloc(rqpair->num_entries, sizeof(*rqpair->rsp_recv_wrs));
+	if (!rqpair->rsp_recv_wrs)
+	{
+		SPDK_ERRLOG("Failed to allocate rsp_recv_wrs\n");
+		goto fail;
+	}
+
+	rqpair->rsps = client_rdma_calloc(rqpair->num_entries, sizeof(*rqpair->rsps));
+	if (!rqpair->rsps)
+	{
+		SPDK_ERRLOG("can not allocate rdma rsps\n");
+		goto fail;
+	}
+
+	return 0;
+fail:
+	client_rdma_free_rsps(rqpair);
+	return -ENOMEM;
+}
+
+static int
+client_rdma_register_rsps(struct client_rdma_qpair *rqpair)
+{
+	uint16_t i;
+	int rc;
+	uint32_t lkey;
+
+	rc = client_rdma_reg_mr(rqpair->cm_id, &rqpair->rsp_mr,
+							rqpair->rsps, rqpair->num_entries * sizeof(*rqpair->rsps));
+
+	if (rc < 0)
+	{
+		goto fail;
+	}
+
+	lkey = client_rdma_mr_get_lkey(&rqpair->rsp_mr);
+
+	for (i = 0; i < rqpair->num_entries; i++)
+	{
+		struct ibv_sge *rsp_sgl = &rqpair->rsp_sgls[i];
+		struct spdk_client_rdma_rsp *rsp = &rqpair->rsps[i];
+
+		rsp->rqpair = rqpair;
+		rsp->rdma_wr.type = RDMA_WR_TYPE_RECV;
+		rsp->idx = i;
+		rsp_sgl->addr = (uint64_t)&rqpair->rsps[i];
+		rsp_sgl->length = sizeof(struct spdk_req_cpl);
+		rsp_sgl->lkey = lkey;
+
+		rqpair->rsp_recv_wrs[i].wr_id = (uint64_t)&rsp->rdma_wr;
+		rqpair->rsp_recv_wrs[i].next = NULL;
+		rqpair->rsp_recv_wrs[i].sg_list = rsp_sgl;
+		rqpair->rsp_recv_wrs[i].num_sge = 1;
+
+		rc = client_rdma_post_recv(rqpair, i);
+		if (rc)
+		{
+			goto fail;
+		}
+	}
+
+	rc = client_rdma_qpair_submit_recvs(rqpair);
+	if (rc)
+	{
+		goto fail;
+	}
+
+	return 0;
+
+fail:
+	client_rdma_unregister_rsps(rqpair);
+	return rc;
+}
+
+static void
+client_rdma_unregister_reqs(struct client_rdma_qpair *rqpair)
+{
+	client_rdma_dereg_mr(&rqpair->cmd_mr);
+}
+
+static void
+client_rdma_free_reqs(struct client_rdma_qpair *rqpair)
+{
+	if (!rqpair->rdma_reqs)
+	{
+		return;
+	}
+
+	client_rdma_free(rqpair->cmds);
+	rqpair->cmds = NULL;
+
+	client_rdma_free(rqpair->rdma_reqs);
+	rqpair->rdma_reqs = NULL;
+}
+
+static int
+client_rdma_alloc_reqs(struct client_rdma_qpair *rqpair)
+{
+	uint16_t i;
+
+	rqpair->rdma_reqs = client_rdma_calloc(rqpair->num_entries, sizeof(struct spdk_client_rdma_req));
+	if (rqpair->rdma_reqs == NULL)
+	{
+		SPDK_ERRLOG("Failed to allocate rdma_reqs\n");
+		goto fail;
+	}
+
+	rqpair->cmds = client_rdma_calloc(rqpair->num_entries, sizeof(*rqpair->cmds));
+	if (!rqpair->cmds)
+	{
+		SPDK_ERRLOG("Failed to allocate RDMA cmds\n");
+		goto fail;
+	}
+
+	TAILQ_INIT(&rqpair->free_reqs);
+	TAILQ_INIT(&rqpair->outstanding_reqs);
+	for (i = 0; i < rqpair->num_entries; i++)
+	{
+		struct spdk_client_rdma_req *rdma_req;
+		struct spdk_client_cmd *cmd;
+
+		rdma_req = &rqpair->rdma_reqs[i];
+		rdma_req->rdma_wr.type = RDMA_WR_TYPE_SEND;
+		cmd = &rqpair->cmds[i];
+
+		rdma_req->id = i;
+
+		/* The first RDMA sgl element will always point
+		 * at this data structure. Depending on whether
+		 * an Client-oF SGL is required, the length of
+		 * this element may change. */
+		rdma_req->send_sgl[0].addr = (uint64_t)cmd;
+		rdma_req->send_wr.wr_id = (uint64_t)&rdma_req->rdma_wr;
+		rdma_req->send_wr.next = NULL;
+		rdma_req->send_wr.opcode = IBV_WR_SEND;
+		rdma_req->send_wr.send_flags = IBV_SEND_SIGNALED;
+		rdma_req->send_wr.sg_list = rdma_req->send_sgl;
+		rdma_req->send_wr.imm_data = 0;
+
+		TAILQ_INSERT_TAIL(&rqpair->free_reqs, rdma_req, link);
+	}
+
+	return 0;
+fail:
+	client_rdma_free_reqs(rqpair);
+	return -ENOMEM;
+}
+
+static int
+client_rdma_register_reqs(struct client_rdma_qpair *rqpair)
+{
+	int i;
+	int rc;
+	uint32_t lkey;
+	SPDK_NOTICELOG("client_rdma_register_reqs: %d\n", rqpair->num_entries);
+	rc = client_rdma_reg_mr(rqpair->cm_id, &rqpair->cmd_mr,
+							rqpair->cmds, rqpair->num_entries * sizeof(*rqpair->cmds));
+
+	if (rc < 0)
+	{
+		goto fail;
+	}
+
+	lkey = client_rdma_mr_get_lkey(&rqpair->cmd_mr);
+
+	for (i = 0; i < rqpair->num_entries; i++)
+	{
+		rqpair->rdma_reqs[i].send_sgl[0].lkey = lkey;
+	}
+
+	return 0;
+
+fail:
+	client_rdma_unregister_reqs(rqpair);
+	return -ENOMEM;
+}
+
+static int client_rdma_connect(struct client_rdma_qpair *rqpair);
+
+static int
+client_rdma_route_resolved(struct client_rdma_qpair *rqpair, int ret)
+{
+	if (ret)
+	{
+		SPDK_ERRLOG("RDMA route resolution error\n");
+		return -1;
+	}
+
+	ret = client_rdma_qpair_init(rqpair);
+	if (ret < 0)
+	{
+		SPDK_ERRLOG("client_rdma_qpair_init() failed\n");
+		return -1;
+	}
+
+	return client_rdma_connect(rqpair);
+}
+
+static int
+client_rdma_addr_resolved(struct client_rdma_qpair *rqpair, int ret)
+{
+	if (ret)
+	{
+		SPDK_ERRLOG("RDMA address resolution error\n");
+		return -1;
+	}
+
+	if (rqpair->qpair.ctrlr->opts.transport_ack_timeout != SPDK_CLIENT_TRANSPORT_ACK_TIMEOUT_DISABLED)
+	{
+#ifdef SPDK_CONFIG_RDMA_SET_ACK_TIMEOUT
+		uint8_t timeout = rqpair->qpair.ctrlr->opts.transport_ack_timeout;
+		ret = rdma_set_option(rqpair->cm_id, RDMA_OPTION_ID,
+							  RDMA_OPTION_ID_ACK_TIMEOUT,
+							  &timeout, sizeof(timeout));
+		if (ret)
+		{
+			SPDK_NOTICELOG("Can't apply RDMA_OPTION_ID_ACK_TIMEOUT %d, ret %d\n", timeout, ret);
+		}
+#else
+		SPDK_DEBUGLOG(client, "transport_ack_timeout is not supported\n");
+#endif
+	}
+
+	ret = rdma_resolve_route(rqpair->cm_id, CLIENT_RDMA_TIME_OUT_IN_MS);
+	if (ret)
+	{
+		SPDK_ERRLOG("rdma_resolve_route\n");
+		return ret;
+	}
+
+	return client_rdma_process_event_start(rqpair, RDMA_CM_EVENT_ROUTE_RESOLVED,
+										   client_rdma_route_resolved);
+}
+
+static int
+client_rdma_resolve_addr(struct client_rdma_qpair *rqpair,
+						 struct sockaddr *src_addr,
+						 struct sockaddr *dst_addr)
+
+{
+	int ret;
+
+	ret = rdma_resolve_addr(rqpair->cm_id, src_addr, dst_addr,
+							CLIENT_RDMA_TIME_OUT_IN_MS);
+	if (ret)
+	{
+		SPDK_ERRLOG("rdma_resolve_addr, %d\n", errno);
+		return ret;
+	}
+
+	return client_rdma_process_event_start(rqpair, RDMA_CM_EVENT_ADDR_RESOLVED,
+										   client_rdma_addr_resolved);
+}
+
+static int client_rdma_stale_conn_retry(struct client_rdma_qpair *rqpair);
+
+static int
+client_rdma_connect_established(struct client_rdma_qpair *rqpair, int ret)
+{
+	if (ret == -ESTALE)
+	{
+		return client_rdma_stale_conn_retry(rqpair);
+	}
+	else if (ret)
+	{
+		SPDK_ERRLOG("RDMA connect error %d\n", ret);
+		return ret;
+	}
+
+	ret = client_rdma_register_reqs(rqpair);
+	SPDK_DEBUGLOG(client, "rc =%d\n", ret);
+	if (ret)
+	{
+		SPDK_ERRLOG("Unable to register rqpair RDMA requests\n");
+		return -1;
+	}
+	SPDK_DEBUGLOG(client, "RDMA requests registered\n");
+
+	ret = client_rdma_register_rsps(rqpair);
+	SPDK_DEBUGLOG(client, "rc =%d\n", ret);
+	if (ret < 0)
+	{
+		SPDK_ERRLOG("Unable to register rqpair RDMA responses\n");
+		return -1;
+	}
+	SPDK_DEBUGLOG(client, "RDMA responses registered\n");
+
+	rqpair->mr_map = spdk_rdma_create_mem_map(rqpair->rdma_qp->qp->pd, &g_client_hooks,
+											  SPDK_RDMA_MEMORY_MAP_ROLE_INITIATOR);
+	if (!rqpair->mr_map)
+	{
+		SPDK_ERRLOG("Unable to register RDMA memory translation map\n");
+		return -1;
+	}
+
+	rqpair->state = CLIENT_RDMA_QPAIR_STATE_RUNNING;
+
+	return 0;
+}
+
+static int
+client_rdma_connect(struct client_rdma_qpair *rqpair)
+{
+	struct rdma_conn_param param = {};
+	struct spdk_srv_rdma_request_private_data request_data = {};
+	struct ibv_device_attr attr;
+	int ret;
+	struct spdk_client_ctrlr *ctrlr;
+	struct client_rdma_ctrlr *rctrlr;
+
+	ret = ibv_query_device(rqpair->cm_id->verbs, &attr);
+	if (ret != 0)
+	{
+		SPDK_ERRLOG("Failed to query RDMA device attributes.\n");
+		return ret;
+	}
+
+	param.responder_resources = spdk_min(rqpair->num_entries, attr.max_qp_rd_atom);
+
+	ctrlr = rqpair->qpair.ctrlr;
+	if (!ctrlr)
+	{
+		return -1;
+	}
+	rctrlr = client_rdma_ctrlr(ctrlr);
+	assert(rctrlr != NULL);
+
+	request_data.qid = rqpair->qpair.id;
+	request_data.hrqsize = rqpair->num_entries;
+	request_data.hsqsize = rqpair->num_entries - 1;
+	request_data.cntlid = ctrlr->cntlid;
+
+	param.private_data = &request_data;
+	param.private_data_len = sizeof(request_data);
+	param.retry_count = ctrlr->opts.transport_retry_count;
+	param.rnr_retry_count = 7;
+
+	/* Fields below are ignored by rdma cm if qpair has been
+	 * created using rdma cm API. */
+	param.srq = 0;
+	param.qp_num = rqpair->rdma_qp->qp->qp_num;
+
+	ret = rdma_connect(rqpair->cm_id, &param);
+	if (ret)
+	{
+		SPDK_ERRLOG("client rdma connect error\n");
+		return ret;
+	}
+
+	return client_rdma_process_event_start(rqpair, RDMA_CM_EVENT_ESTABLISHED,
+										   client_rdma_connect_established);
+}
+
+static int
+client_rdma_parse_addr(struct sockaddr_storage *sa, int family, const char *addr, const char *service)
+{
+	struct addrinfo *res;
+	struct addrinfo hints;
+	int ret;
+
+	memset(&hints, 0, sizeof(hints));
+	hints.ai_family = family;
+	hints.ai_socktype = SOCK_STREAM;
+	hints.ai_protocol = 0;
+
+	ret = getaddrinfo(addr, service, &hints, &res);
+	if (ret)
+	{
+		SPDK_ERRLOG("getaddrinfo failed: %s (%d)\n", gai_strerror(ret), ret);
+		return ret;
+	}
+
+	if (res->ai_addrlen > sizeof(*sa))
+	{
+		SPDK_ERRLOG("getaddrinfo() ai_addrlen %zu too large\n", (size_t)res->ai_addrlen);
+		ret = EINVAL;
+	}
+	else
+	{
+		memcpy(sa, res->ai_addr, res->ai_addrlen);
+	}
+
+	freeaddrinfo(res);
+	return ret;
+}
+
+static int
+client_rdma_ctrlr_connect_qpair(struct spdk_client_ctrlr *ctrlr, struct spdk_client_qpair *qpair)
+{
+	struct sockaddr_storage dst_addr;
+	struct sockaddr_storage src_addr;
+	bool src_addr_specified;
+	int rc;
+	struct client_rdma_ctrlr *rctrlr;
+	struct client_rdma_qpair *rqpair;
+	int family;
+
+	rqpair = client_rdma_qpair(qpair);
+	rctrlr = client_rdma_ctrlr(ctrlr);
+	assert(rctrlr != NULL);
+
+	switch (qpair->trid->adrfam)
+	{
+	case SPDK_SRV_ADRFAM_IPV4:
+		family = AF_INET;
+		break;
+	case SPDK_SRV_ADRFAM_IPV6:
+		family = AF_INET6;
+		break;
+	default:
+		SPDK_ERRLOG("Unhandled ADRFAM %d\n", qpair->trid->adrfam);
+		return -1;
+	}
+
+	SPDK_DEBUGLOG(client, "adrfam %d ai_family %d\n", qpair->trid->adrfam, family);
+
+	memset(&dst_addr, 0, sizeof(dst_addr));
+
+	SPDK_DEBUGLOG(client, "trsvcid is %s\n", qpair->trid->trsvcid);
+	rc = client_rdma_parse_addr(&dst_addr, family, qpair->trid->traddr, qpair->trid->trsvcid);
+	if (rc != 0)
+	{
+		SPDK_ERRLOG("dst_addr client_rdma_parse_addr() failed\n");
+		return -1;
+	}
+
+	if (ctrlr->opts.src_addr[0] || ctrlr->opts.src_svcid[0])
+	{
+		memset(&src_addr, 0, sizeof(src_addr));
+		rc = client_rdma_parse_addr(&src_addr, family, ctrlr->opts.src_addr, ctrlr->opts.src_svcid);
+		if (rc != 0)
+		{
+			SPDK_ERRLOG("src_addr client_rdma_parse_addr() failed\n");
+			return -1;
+		}
+		src_addr_specified = true;
+	}
+	else
+	{
+		src_addr_specified = false;
+	}
+
+	rc = rdma_create_id(rctrlr->cm_channel, &rqpair->cm_id, rqpair, RDMA_PS_TCP);
+	if (rc < 0)
+	{
+		SPDK_ERRLOG("rdma_create_id() failed\n");
+		return -1;
+	}
+
+	rc = client_rdma_resolve_addr(rqpair,
+								  src_addr_specified ? (struct sockaddr *)&src_addr : NULL,
+								  (struct sockaddr *)&dst_addr);
+	if (rc < 0)
+	{
+		SPDK_ERRLOG("client_rdma_resolve_addr() failed\n");
+		return -1;
+	}
+
+	rqpair->state = CLIENT_RDMA_QPAIR_STATE_INITIALIZING;
+	return 0;
+}
+
+static int
+client_rdma_stale_conn_reconnect(struct client_rdma_qpair *rqpair)
+{
+	struct spdk_client_qpair *qpair = &rqpair->qpair;
+
+	if (spdk_get_ticks() < rqpair->evt_timeout_ticks)
+	{
+		return -EAGAIN;
+	}
+
+	return client_rdma_ctrlr_connect_qpair(qpair->ctrlr, qpair);
+}
+
+static int
+client_rdma_ctrlr_connect_qpair_poll(struct spdk_client_ctrlr *ctrlr, struct spdk_client_qpair *qpair)
+{
+	struct client_rdma_qpair *rqpair = client_rdma_qpair(qpair);
+	int rc;
+
+	if (rqpair->in_connect_poll)
+	{
+		return -EAGAIN;
+	}
+
+	rqpair->in_connect_poll = true;
+
+	switch (rqpair->state)
+	{
+	case CLIENT_RDMA_QPAIR_STATE_INVALID:
+		rc = -EAGAIN;
+		break;
+
+	case CLIENT_RDMA_QPAIR_STATE_INITIALIZING:
+	case CLIENT_RDMA_QPAIR_STATE_EXITING:
+		if (!client_qpair_is_admin_queue(qpair))
+		{
+			client_robust_mutex_lock(&ctrlr->ctrlr_lock);
+		}
+
+		rc = client_rdma_process_event_poll(rqpair);
+
+		if (!client_qpair_is_admin_queue(qpair))
+		{
+			client_robust_mutex_unlock(&ctrlr->ctrlr_lock);
+		}
+
+		if (rc == 0)
+		{
+			rc = -EAGAIN;
+		}
+		rqpair->in_connect_poll = false;
+
+		return rc;
+
+	case CLIENT_RDMA_QPAIR_STATE_STALE_CONN:
+		rc = client_rdma_stale_conn_reconnect(rqpair);
+		if (rc == 0)
+		{
+			rc = -EAGAIN;
+		}
+		break;
+	case CLIENT_RDMA_QPAIR_STATE_RUNNING:
+		client_qpair_set_state(qpair, CLIENT_QPAIR_CONNECTED);
+		qpair->cb(qpair->cb_args, 0);
+		rc = 0;
+		break;
+	default:
+		assert(false);
+		rc = -EINVAL;
+		break;
+	}
+
+	rqpair->in_connect_poll = false;
+
+	return rc;
+}
+
+static inline int
+client_rdma_get_memory_translation(struct client_request *req, struct client_rdma_qpair *rqpair,
+								   struct client_rdma_memory_translation_ctx *_ctx)
+{
+	struct spdk_memory_domain_translation_ctx ctx;
+	struct spdk_memory_domain_translation_result dma_translation = {.iov_count = 0};
+	struct spdk_rdma_memory_translation rdma_translation;
+	int rc;
+
+	assert(req);
+	assert(rqpair);
+	assert(_ctx);
+
+	if (req->payload.opts && req->payload.opts->memory_domain)
+	{
+		ctx.size = sizeof(struct spdk_memory_domain_translation_ctx);
+		ctx.rdma.ibv_qp = rqpair->rdma_qp->qp;
+		dma_translation.size = sizeof(struct spdk_memory_domain_translation_result);
+
+		rc = spdk_memory_domain_translate_data(req->payload.opts->memory_domain,
+											   req->payload.opts->memory_domain_ctx,
+											   rqpair->memory_domain->domain, &ctx, _ctx->addr,
+											   _ctx->length, &dma_translation);
+		if (spdk_unlikely(rc) || dma_translation.iov_count != 1)
+		{
+			SPDK_ERRLOG("DMA memory translation failed, rc %d, iov count %u\n", rc, dma_translation.iov_count);
+			return rc;
+		}
+
+		_ctx->lkey = dma_translation.rdma.lkey;
+		_ctx->rkey = dma_translation.rdma.rkey;
+		_ctx->addr = dma_translation.iov.iov_base;
+		_ctx->length = dma_translation.iov.iov_len;
+	}
+	else
+	{
+		rc = spdk_rdma_get_translation(rqpair->mr_map, _ctx->addr, _ctx->length, &rdma_translation);
+		if (spdk_unlikely(rc))
+		{
+			SPDK_ERRLOG("RDMA memory translation failed, rc %d\n", rc);
+			return rc;
+		}
+		if (rdma_translation.translation_type == SPDK_RDMA_TRANSLATION_MR)
+		{
+			_ctx->lkey = rdma_translation.mr_or_key.mr->lkey;
+			_ctx->rkey = rdma_translation.mr_or_key.mr->rkey;
+		}
+		else
+		{
+			_ctx->lkey = _ctx->rkey = (uint32_t)rdma_translation.mr_or_key.key;
+		}
+	}
+
+	return 0;
+}
+
+/*
+ * Build SGL describing empty payload.
+ */
+static int
+client_rdma_build_null_request(struct spdk_client_rdma_req *rdma_req)
+{
+	struct client_request *req = rdma_req->req;
+
+	req->cmd.psdt = SPDK_CLIENT_PSDT_SGL_MPTR_CONTIG;
+
+	/* The first element of this SGL is pointing at an
+	 * spdk_client_cmd object. For this particular command,
+	 * we only need the first 64 bytes corresponding to
+	 * the Client command. */
+	rdma_req->send_sgl[0].length = sizeof(struct spdk_req_cmd);
+
+	/* The RDMA SGL needs one element describing the Client command. */
+	rdma_req->send_wr.num_sge = 1;
+
+	req->cmd.dptr.sgl1.keyed.type = SPDK_CLIENT_SGL_TYPE_KEYED_DATA_BLOCK;
+	req->cmd.dptr.sgl1.keyed.subtype = SPDK_CLIENT_SGL_SUBTYPE_ADDRESS;
+	req->cmd.dptr.sgl1.keyed.length = 0;
+	req->cmd.dptr.sgl1.keyed.key = 0;
+	req->cmd.dptr.sgl1.address = 0;
+
+	return 0;
+}
+
+/*
+ * Build inline SGL describing contiguous payload buffer.
+ */
+static int
+client_rdma_build_contig_inline_request(struct client_rdma_qpair *rqpair,
+										struct spdk_client_rdma_req *rdma_req)
+{
+	struct client_request *req = rdma_req->req;
+	struct client_rdma_memory_translation_ctx ctx = {
+		.addr = req->payload.contig_or_cb_arg + req->payload_offset,
+		.length = req->payload_size};
+	int rc;
+
+	assert(ctx.length != 0);
+	assert(client_payload_type(&req->payload) == CLIENT_PAYLOAD_TYPE_CONTIG);
+
+	rc = client_rdma_get_memory_translation(req, rqpair, &ctx);
+	if (spdk_unlikely(rc))
+	{
+		return -1;
+	}
+
+	rdma_req->send_sgl[1].lkey = ctx.lkey;
+
+	/* The first element of this SGL is pointing at an
+	 * spdk_client_cmd object. For this particular command,
+	 * we only need the first 64 bytes corresponding to
+	 * the Client command. */
+	rdma_req->send_sgl[0].length = sizeof(struct spdk_req_cmd);
+
+	rdma_req->send_sgl[1].addr = (uint64_t)ctx.addr;
+	rdma_req->send_sgl[1].length = (uint32_t)ctx.length;
+
+	/* The RDMA SGL contains two elements. The first describes
+	 * the Client command and the second describes the data
+	 * payload. */
+	rdma_req->send_wr.num_sge = 2;
+
+	req->cmd.psdt = SPDK_CLIENT_PSDT_SGL_MPTR_CONTIG;
+	req->cmd.dptr.sgl1.unkeyed.type = SPDK_CLIENT_SGL_TYPE_DATA_BLOCK;
+	req->cmd.dptr.sgl1.unkeyed.subtype = SPDK_CLIENT_SGL_SUBTYPE_OFFSET;
+	req->cmd.dptr.sgl1.unkeyed.length = (uint32_t)ctx.length;
+	/* Inline only supported for icdoff == 0 currently.  This function will
+	 * not get called for controllers with other values. */
+	req->cmd.dptr.sgl1.address = (uint64_t)0;
+
+	return 0;
+}
+
+/*
+ * Build SGL describing contiguous payload buffer.
+ */
+static int
+client_rdma_build_contig_request(struct client_rdma_qpair *rqpair,
+								 struct spdk_client_rdma_req *rdma_req)
+{
+	struct client_request *req = rdma_req->req;
+	struct client_rdma_memory_translation_ctx ctx = {
+		.addr = req->payload.contig_or_cb_arg + req->payload_offset,
+		.length = req->payload_size};
+	int rc;
+
+	assert(req->payload_size != 0);
+	assert(client_payload_type(&req->payload) == CLIENT_PAYLOAD_TYPE_CONTIG);
+
+	if (spdk_unlikely(req->payload_size > CLIENT_RDMA_MAX_KEYED_SGL_LENGTH))
+	{
+		SPDK_ERRLOG("SGL length %u exceeds max keyed SGL block size %u\n",
+					req->payload_size, CLIENT_RDMA_MAX_KEYED_SGL_LENGTH);
+		return -1;
+	}
+
+	rc = client_rdma_get_memory_translation(req, rqpair, &ctx);
+	if (spdk_unlikely(rc))
+	{
+		return -1;
+	}
+
+	req->cmd.dptr.sgl1.keyed.key = ctx.rkey;
+
+	/* The first element of this SGL is pointing at an
+	 * spdk_client_cmd object. For this particular command,
+	 * we only need the first 64 bytes corresponding to
+	 * the Client command. */
+	rdma_req->send_sgl[0].length = sizeof(struct spdk_req_cmd);
+
+	/* The RDMA SGL needs one element describing the Client command. */
+	rdma_req->send_wr.num_sge = 1;
+
+	req->cmd.psdt = SPDK_CLIENT_PSDT_SGL_MPTR_CONTIG;
+	req->cmd.dptr.sgl1.keyed.type = SPDK_CLIENT_SGL_TYPE_KEYED_DATA_BLOCK;
+	req->cmd.dptr.sgl1.keyed.subtype = SPDK_CLIENT_SGL_SUBTYPE_ADDRESS;
+	req->cmd.dptr.sgl1.keyed.length = (uint32_t)ctx.length;
+	req->cmd.dptr.sgl1.address = (uint64_t)ctx.addr;
+
+	return 0;
+}
+
+/*
+ * Build SGL describing scattered payload buffer.
+ */
+static int
+client_rdma_build_sgl_request(struct client_rdma_qpair *rqpair,
+							  struct spdk_client_rdma_req *rdma_req)
+{
+	struct client_request *req = rdma_req->req;
+	struct spdk_client_cmd *cmd = &rqpair->cmds[rdma_req->id];
+	struct client_rdma_memory_translation_ctx ctx;
+	uint32_t remaining_size;
+	uint32_t sge_length;
+	int rc, max_num_sgl, num_sgl_desc;
+
+	assert(req->payload_size != 0);
+	assert(client_payload_type(&req->payload) == CLIENT_PAYLOAD_TYPE_SGL);
+	assert(req->payload.reset_sgl_fn != NULL);
+	assert(req->payload.next_sge_fn != NULL);
+	req->payload.reset_sgl_fn(req->payload.contig_or_cb_arg, req->payload_offset);
+
+	max_num_sgl = req->qpair->ctrlr->max_sges;
+
+	remaining_size = req->payload_size;
+	num_sgl_desc = 0;
+	do
+	{
+		rc = req->payload.next_sge_fn(req->payload.contig_or_cb_arg, &ctx.addr, &sge_length);
+		if (rc)
+		{
+			return -1;
+		}
+
+		sge_length = spdk_min(remaining_size, sge_length);
+
+		if (spdk_unlikely(sge_length > CLIENT_RDMA_MAX_KEYED_SGL_LENGTH))
+		{
+			SPDK_ERRLOG("SGL length %u exceeds max keyed SGL block size %u\n",
+						sge_length, CLIENT_RDMA_MAX_KEYED_SGL_LENGTH);
+			return -1;
+		}
+		ctx.length = sge_length;
+		rc = client_rdma_get_memory_translation(req, rqpair, &ctx);
+		if (spdk_unlikely(rc))
+		{
+			return -1;
+		}
+
+		cmd->sgl[num_sgl_desc].keyed.key = ctx.rkey;
+		cmd->sgl[num_sgl_desc].keyed.type = SPDK_CLIENT_SGL_TYPE_KEYED_DATA_BLOCK;
+		cmd->sgl[num_sgl_desc].keyed.subtype = SPDK_CLIENT_SGL_SUBTYPE_ADDRESS;
+		cmd->sgl[num_sgl_desc].keyed.length = (uint32_t)ctx.length;
+		cmd->sgl[num_sgl_desc].address = (uint64_t)ctx.addr;
+
+		remaining_size -= ctx.length;
+		num_sgl_desc++;
+	} while (remaining_size > 0 && num_sgl_desc < max_num_sgl);
+
+	/* Should be impossible if we did our sgl checks properly up the stack, but do a sanity check here. */
+	if (remaining_size > 0)
+	{
+		return -1;
+	}
+
+	req->cmd.psdt = SPDK_CLIENT_PSDT_SGL_MPTR_CONTIG;
+
+	/* The RDMA SGL needs one element describing some portion
+	 * of the spdk_client_cmd structure. */
+	rdma_req->send_wr.num_sge = 1;
+
+	/*
+	 * If only one SGL descriptor is required, it can be embedded directly in the command
+	 * as a data block descriptor.
+	 */
+	if (num_sgl_desc == 1)
+	{
+		/* The first element of this SGL is pointing at an
+		 * spdk_client_cmd object. For this particular command,
+		 * we only need the first 64 bytes corresponding to
+		 * the Client command. */
+		rdma_req->send_sgl[0].length = sizeof(struct spdk_req_cmd);
+
+		req->cmd.dptr.sgl1.keyed.type = cmd->sgl[0].keyed.type;
+		req->cmd.dptr.sgl1.keyed.subtype = cmd->sgl[0].keyed.subtype;
+		req->cmd.dptr.sgl1.keyed.length = cmd->sgl[0].keyed.length;
+		req->cmd.dptr.sgl1.keyed.key = cmd->sgl[0].keyed.key;
+		req->cmd.dptr.sgl1.address = cmd->sgl[0].address;
+	}
+	else
+	{
+		/*
+		 * Otherwise, The SGL descriptor embedded in the command must point to the list of
+		 * SGL descriptors used to describe the operation. In that case it is a last segment descriptor.
+		 */
+		uint32_t descriptors_size = sizeof(struct spdk_req_sgl_descriptor) * num_sgl_desc;
+
+		if (spdk_unlikely(descriptors_size > rqpair->qpair.ctrlr->ioccsz_bytes))
+		{
+			SPDK_ERRLOG("Size of SGL descriptors (%u) exceeds ICD (%u)\n",
+						descriptors_size, rqpair->qpair.ctrlr->ioccsz_bytes);
+			return -1;
+		}
+		rdma_req->send_sgl[0].length = sizeof(struct spdk_req_cmd) + descriptors_size;
+
+		req->cmd.dptr.sgl1.unkeyed.type = SPDK_CLIENT_SGL_TYPE_LAST_SEGMENT;
+		req->cmd.dptr.sgl1.unkeyed.subtype = SPDK_CLIENT_SGL_SUBTYPE_OFFSET;
+		req->cmd.dptr.sgl1.unkeyed.length = descriptors_size;
+		req->cmd.dptr.sgl1.address = (uint64_t)0;
+	}
+
+	return 0;
+}
+
+/*
+ * Build inline SGL describing sgl payload buffer.
+ */
+static int
+client_rdma_build_sgl_inline_request(struct client_rdma_qpair *rqpair,
+									 struct spdk_client_rdma_req *rdma_req)
+{
+	struct client_request *req = rdma_req->req;
+	struct client_rdma_memory_translation_ctx ctx;
+	uint32_t length;
+	int rc;
+
+	assert(req->payload_size != 0);
+	assert(client_payload_type(&req->payload) == CLIENT_PAYLOAD_TYPE_SGL);
+	assert(req->payload.reset_sgl_fn != NULL);
+	assert(req->payload.next_sge_fn != NULL);
+	req->payload.reset_sgl_fn(req->payload.contig_or_cb_arg, req->payload_offset);
+
+	rc = req->payload.next_sge_fn(req->payload.contig_or_cb_arg, &ctx.addr, &length);
+	if (rc)
+	{
+		return -1;
+	}
+
+	if (length < req->payload_size)
+	{
+		SPDK_DEBUGLOG(client, "Inline SGL request split so sending separately.\n");
+		return client_rdma_build_sgl_request(rqpair, rdma_req);
+	}
+
+	if (length > req->payload_size)
+	{
+		length = req->payload_size;
+	}
+
+	ctx.length = length;
+	rc = client_rdma_get_memory_translation(req, rqpair, &ctx);
+	if (spdk_unlikely(rc))
+	{
+		return -1;
+	}
+
+	rdma_req->send_sgl[1].addr = (uint64_t)ctx.addr;
+	rdma_req->send_sgl[1].length = (uint32_t)ctx.length;
+	rdma_req->send_sgl[1].lkey = ctx.lkey;
+	SPDK_DEBUGLOG(rdma, "client_rdma_build_sgl_inline_request length=%d\n", (uint32_t)ctx.length);
+	rdma_req->send_wr.num_sge = 2;
+
+	/* The first element of this SGL is pointing at an
+	 * spdk_client_cmd object. For this particular command,
+	 * we only need the first 64 bytes corresponding to
+	 * the Client command. */
+	rdma_req->send_sgl[0].length = sizeof(struct spdk_req_cmd);
+
+	req->cmd.psdt = SPDK_CLIENT_PSDT_SGL_MPTR_CONTIG;
+	req->cmd.dptr.sgl1.unkeyed.type = SPDK_CLIENT_SGL_TYPE_DATA_BLOCK;
+	req->cmd.dptr.sgl1.unkeyed.subtype = SPDK_CLIENT_SGL_SUBTYPE_OFFSET;
+	req->cmd.dptr.sgl1.unkeyed.length = (uint32_t)ctx.length;
+	/* Inline only supported for icdoff == 0 currently.  This function will
+	 * not get called for controllers with other values. */
+	req->cmd.dptr.sgl1.address = (uint64_t)0;
+
+	return 0;
+}
+
+static int
+client_rdma_req_init(struct client_rdma_qpair *rqpair, struct client_request *req,
+					 struct spdk_client_rdma_req *rdma_req)
+{
+	struct spdk_client_ctrlr *ctrlr = rqpair->qpair.ctrlr;
+	enum client_payload_type payload_type;
+	bool icd_supported;
+	int rc;
+
+	assert(rdma_req->req == NULL);
+	rdma_req->req = req;
+	req->cmd.cid = rdma_req->id;
+	payload_type = client_payload_type(&req->payload);
+	/*
+	 * Check if icdoff is non zero, to avoid interop conflicts with
+	 * targets with non-zero icdoff.  Both SPDK and the Linux kernel
+	 * targets use icdoff = 0.  For targets with non-zero icdoff, we
+	 * will currently just not use inline data for now.
+	 */
+	icd_supported = spdk_client_opc_get_data_transfer(req->cmd.opc) == SPDK_CLIENT_DATA_HOST_TO_CONTROLLER && req->payload_size <= ctrlr->ioccsz_bytes && ctrlr->icdoff == 0;
+	SPDK_DEBUGLOG(rdma, "debug client_rdma_req_init payload_size=%d, payload_type=%d, icd_supported=%d\n", req->payload_size, payload_type, icd_supported);
+	if (req->payload_size == 0)
+	{
+		rc = client_rdma_build_null_request(rdma_req);
+	}
+	else if (payload_type == CLIENT_PAYLOAD_TYPE_CONTIG)
+	{
+		if (icd_supported)
+		{
+			rc = client_rdma_build_contig_inline_request(rqpair, rdma_req);
+		}
+		else
+		{
+			rc = client_rdma_build_contig_request(rqpair, rdma_req);
+		}
+	}
+	else if (payload_type == CLIENT_PAYLOAD_TYPE_SGL)
+	{
+
+		if (icd_supported)
+		{
+			rc = client_rdma_build_sgl_inline_request(rqpair, rdma_req);
+		}
+		else
+		{
+			rc = client_rdma_build_sgl_request(rqpair, rdma_req);
+		}
+	}
+	else
+	{
+		rc = -1;
+	}
+
+	if (rc)
+	{
+		rdma_req->req = NULL;
+		return rc;
+	}
+
+	memcpy(&rqpair->cmds[rdma_req->id], &req->cmd, sizeof(req->cmd));
+	return 0;
+}
+
+static struct spdk_client_qpair *
+client_rdma_ctrlr_create_qpair(struct spdk_client_ctrlr *ctrlr,
+							   uint16_t qid, uint32_t qsize,
+							   enum spdk_client_qprio qprio,
+							   uint32_t num_requests,
+							   bool delay_cmd_submit)
+{
+	struct client_rdma_qpair *rqpair;
+	struct spdk_client_qpair *qpair;
+	int rc;
+
+	rqpair = client_rdma_calloc(1, sizeof(struct client_rdma_qpair));
+	if (!rqpair)
+	{
+		SPDK_ERRLOG("failed to get create rqpair\n");
+		return NULL;
+	}
+
+	rqpair->num_entries = qsize;
+	rqpair->delay_cmd_submit = delay_cmd_submit;
+	rqpair->state = CLIENT_RDMA_QPAIR_STATE_INVALID;
+	qpair = &rqpair->qpair;
+	rc = client_qpair_init(qpair, qid, ctrlr, qprio, num_requests, false);
+	if (rc != 0)
+	{
+		client_rdma_free(rqpair);
+		return NULL;
+	}
+
+	rc = client_rdma_alloc_reqs(rqpair);
+	SPDK_DEBUGLOG(client, "rc =%d\n", rc);
+	if (rc)
+	{
+		SPDK_ERRLOG("Unable to allocate rqpair RDMA requests\n");
+		client_rdma_free(rqpair);
+		return NULL;
+	}
+	SPDK_DEBUGLOG(client, "RDMA requests allocated\n");
+
+	rc = client_rdma_alloc_rsps(rqpair);
+	SPDK_DEBUGLOG(client, "rc =%d\n", rc);
+	if (rc < 0)
+	{
+		SPDK_ERRLOG("Unable to allocate rqpair RDMA responses\n");
+		client_rdma_free_reqs(rqpair);
+		client_rdma_free(rqpair);
+		return NULL;
+	}
+	SPDK_DEBUGLOG(client, "RDMA responses allocated\n");
+	SPDK_NOTICELOG("client_rdma_ctrlr_create_qpair num_entries: %d\n", rqpair->num_entries);
+	return qpair;
+}
+
+static void
+client_rdma_qpair_destroy(struct client_rdma_qpair *rqpair)
+{
+	struct spdk_client_qpair *qpair = &rqpair->qpair;
+	struct client_rdma_ctrlr *rctrlr;
+	struct client_rdma_cm_event_entry *entry, *tmp;
+
+	spdk_rdma_free_mem_map(&rqpair->mr_map);
+	client_rdma_unregister_reqs(rqpair);
+	client_rdma_unregister_rsps(rqpair);
+
+	if (rqpair->evt)
+	{
+		rdma_ack_cm_event(rqpair->evt);
+		rqpair->evt = NULL;
+	}
+
+	/*
+	 * This works because we have the controller lock both in
+	 * this function and in the function where we add new events.
+	 */
+	if (qpair->ctrlr != NULL)
+	{
+		rctrlr = client_rdma_ctrlr(qpair->ctrlr);
+		STAILQ_FOREACH_SAFE(entry, &rctrlr->pending_cm_events, link, tmp)
+		{
+			if (entry->evt->id->context == rqpair)
+			{
+				STAILQ_REMOVE(&rctrlr->pending_cm_events, entry, client_rdma_cm_event_entry, link);
+				rdma_ack_cm_event(entry->evt);
+				STAILQ_INSERT_HEAD(&rctrlr->free_cm_events, entry, link);
+			}
+		}
+	}
+
+	if (rqpair->cm_id)
+	{
+		if (rqpair->rdma_qp)
+		{
+			spdk_rdma_qp_destroy(rqpair->rdma_qp);
+			rqpair->rdma_qp = NULL;
+		}
+
+		rdma_destroy_id(rqpair->cm_id);
+		rqpair->cm_id = NULL;
+	}
+
+	if (rqpair->cq)
+	{
+		ibv_destroy_cq(rqpair->cq);
+		rqpair->cq = NULL;
+	}
+}
+
+static void client_rdma_qpair_abort_reqs(struct spdk_client_qpair *qpair, uint32_t dnr);
+
+static int
+client_rdma_qpair_disconnected(struct client_rdma_qpair *rqpair, int ret)
+{
+	struct spdk_client_qpair *qpair = &rqpair->qpair;
+
+	client_rdma_qpair_destroy(rqpair);
+
+	client_rdma_qpair_abort_reqs(&rqpair->qpair, 0);
+
+	if (ret)
+	{
+		SPDK_DEBUGLOG(client, "Target did not respond to qpair disconnect.\n");
+		goto quiet;
+	}
+
+	if (qpair->poll_group == NULL)
+	{
+		/* If poll group is not used, cq is already destroyed. So complete
+		 * disconnecting qpair immediately.
+		 */
+		goto quiet;
+	}
+
+	if (rqpair->current_num_sends != 0 || rqpair->current_num_recvs != 0)
+	{
+		rqpair->state = CLIENT_RDMA_QPAIR_STATE_LINGERING;
+		rqpair->evt_timeout_ticks = (CLIENT_RDMA_DISCONNECTED_QPAIR_TIMEOUT_US * spdk_get_ticks_hz()) /
+										SPDK_SEC_TO_USEC +
+									spdk_get_ticks();
+
+		return -EAGAIN;
+	}
+
+quiet:
+	rqpair->state = CLIENT_RDMA_QPAIR_STATE_EXITED;
+
+	client_transport_ctrlr_disconnect_qpair_done(&rqpair->qpair);
+
+	return 0;
+}
+
+static void
+_client_rdma_ctrlr_disconnect_qpair(struct spdk_client_ctrlr *ctrlr, struct spdk_client_qpair *qpair,
+									client_rdma_cm_event_cb disconnected_qpair_cb)
+{
+	struct client_rdma_qpair *rqpair = client_rdma_qpair(qpair);
+	int rc;
+
+	assert(disconnected_qpair_cb != NULL);
+
+	rqpair->state = CLIENT_RDMA_QPAIR_STATE_EXITING;
+
+	if (rqpair->cm_id)
+	{
+		if (rqpair->rdma_qp)
+		{
+			rc = spdk_rdma_qp_disconnect(rqpair->rdma_qp);
+			if ((qpair->ctrlr != NULL) && (rc == 0))
+			{
+				rc = client_rdma_process_event_start(rqpair, RDMA_CM_EVENT_DISCONNECTED,
+													 disconnected_qpair_cb);
+				if (rc == 0)
+				{
+					return;
+				}
+			}
+		}
+	}
+
+	disconnected_qpair_cb(rqpair, 0);
+}
+
+static int
+client_rdma_qpair_wait_until_quiet(struct client_rdma_qpair *rqpair)
+{
+	if (spdk_get_ticks() < rqpair->evt_timeout_ticks &&
+		(rqpair->current_num_sends != 0 || rqpair->current_num_recvs != 0))
+	{
+		return -EAGAIN;
+	}
+
+	rqpair->state = CLIENT_RDMA_QPAIR_STATE_EXITED;
+
+	client_transport_ctrlr_disconnect_qpair_done(&rqpair->qpair);
+
+	return 0;
+}
+
+static int client_rdma_ctrlr_disconnect_qpair_poll(struct spdk_client_ctrlr *ctrlr, struct spdk_client_qpair *qpair);
+static void
+client_rdma_ctrlr_disconnect_qpair(struct spdk_client_ctrlr *ctrlr, struct spdk_client_qpair *qpair)
+{
+	int rc;
+
+	_client_rdma_ctrlr_disconnect_qpair(ctrlr, qpair, client_rdma_qpair_disconnected);
+
+	/* If the qpair is in a poll group, disconnected_qpair_cb has to be called
+	 * asynchronously after the qpair is actually disconnected. Hence let
+	 * poll_group_process_completions() poll the qpair until then.
+	 *
+	 * If the qpair is not in a poll group, poll the qpair until it is actually
+	 * disconnected here.
+	 */
+	if (qpair->async || qpair->poll_group != NULL)
+	{
+		return;
+	}
+
+	while (1)
+	{
+		rc = client_rdma_ctrlr_disconnect_qpair_poll(ctrlr, qpair);
+		if (rc != -EAGAIN)
+		{
+			break;
+		}
+	}
+}
+
+static int
+client_rdma_stale_conn_disconnected(struct client_rdma_qpair *rqpair, int ret)
+{
+	struct spdk_client_qpair *qpair = &rqpair->qpair;
+
+	if (ret)
+	{
+		SPDK_DEBUGLOG(client, "Target did not respond to qpair disconnect.\n");
+	}
+
+	client_rdma_qpair_destroy(rqpair);
+
+	qpair->last_transport_failure_reason = qpair->transport_failure_reason;
+	qpair->transport_failure_reason = SPDK_CLIENT_QPAIR_FAILURE_NONE;
+
+	rqpair->state = CLIENT_RDMA_QPAIR_STATE_STALE_CONN;
+	rqpair->evt_timeout_ticks = (CLIENT_RDMA_STALE_CONN_RETRY_DELAY_US * spdk_get_ticks_hz()) /
+									SPDK_SEC_TO_USEC +
+								spdk_get_ticks();
+
+	return 0;
+}
+
+static int
+client_rdma_stale_conn_retry(struct client_rdma_qpair *rqpair)
+{
+	struct spdk_client_qpair *qpair = &rqpair->qpair;
+
+	if (rqpair->stale_conn_retry_count >= CLIENT_RDMA_STALE_CONN_RETRY_MAX)
+	{
+		SPDK_ERRLOG("Retry failed %d times, give up stale connection to qpair (cntlid:%u, qid:%u).\n",
+					CLIENT_RDMA_STALE_CONN_RETRY_MAX, qpair->ctrlr->cntlid, qpair->id);
+		return -ESTALE;
+	}
+
+	rqpair->stale_conn_retry_count++;
+
+	SPDK_NOTICELOG("%d times, retry stale connnection to qpair (cntlid:%u, qid:%u).\n",
+				   rqpair->stale_conn_retry_count, qpair->ctrlr->cntlid, qpair->id);
+
+	if (qpair->poll_group)
+	{
+		rqpair->cq = NULL;
+	}
+
+	_client_rdma_ctrlr_disconnect_qpair(qpair->ctrlr, qpair, client_rdma_stale_conn_disconnected);
+
+	return 0;
+}
+
+static int
+client_rdma_ctrlr_delete_io_qpair(struct spdk_client_ctrlr *ctrlr, struct spdk_client_qpair *qpair)
+{
+	struct client_rdma_qpair *rqpair;
+
+	assert(qpair != NULL);
+	rqpair = client_rdma_qpair(qpair);
+
+	if (rqpair->defer_deletion_to_pg)
+	{
+		client_qpair_set_state(qpair, CLIENT_QPAIR_DESTROYING);
+		return 0;
+	}
+
+	client_rdma_qpair_abort_reqs(qpair, 0);
+	client_qpair_deinit(qpair);
+
+	client_rdma_put_memory_domain(rqpair->memory_domain);
+
+	client_rdma_free_reqs(rqpair);
+	client_rdma_free_rsps(rqpair);
+	client_rdma_free(rqpair);
+
+	return 0;
+}
+
+static struct spdk_client_qpair *
+client_rdma_ctrlr_create_io_qpair(struct spdk_client_ctrlr *ctrlr, uint16_t qid,
+								  const struct spdk_client_io_qpair_opts *opts)
+{
+	SPDK_NOTICELOG("client_rdma_ctrlr_create_io_qpair io_queue_size: %d io_queue_requests: %d\n", opts->io_queue_size, opts->io_queue_requests);
+	return client_rdma_ctrlr_create_qpair(ctrlr, qid, opts->io_queue_size, opts->qprio,
+										  opts->io_queue_requests,
+										  opts->delay_cmd_submit);
+}
+
+static int
+client_rdma_ctrlr_enable(struct spdk_client_ctrlr *ctrlr)
+{
+	/* do nothing here */
+	return 0;
+}
+
+static int client_rdma_ctrlr_destruct(struct spdk_client_ctrlr *ctrlr);
+
+static struct spdk_client_ctrlr *client_rdma_ctrlr_construct(
+	const struct spdk_client_ctrlr_opts *opts,
+	void *devhandle)
+{
+	struct client_rdma_ctrlr *rctrlr;
+	struct ibv_context **contexts;
+	struct ibv_device_attr dev_attr;
+	int i, flag, rc;
+
+	rctrlr = client_rdma_calloc(1, sizeof(struct client_rdma_ctrlr));
+	if (rctrlr == NULL)
+	{
+		SPDK_ERRLOG("could not allocate ctrlr\n");
+		return NULL;
+	}
+
+	rctrlr->ctrlr.opts = *opts;
+
+	if (opts->transport_retry_count > CLIENT_RDMA_CTRLR_MAX_TRANSPORT_RETRY_COUNT)
+	{
+		SPDK_NOTICELOG("transport_retry_count exceeds max value %d, use max value\n",
+					   CLIENT_RDMA_CTRLR_MAX_TRANSPORT_RETRY_COUNT);
+		rctrlr->ctrlr.opts.transport_retry_count = CLIENT_RDMA_CTRLR_MAX_TRANSPORT_RETRY_COUNT;
+	}
+
+	if (opts->transport_ack_timeout > CLIENT_RDMA_CTRLR_MAX_TRANSPORT_ACK_TIMEOUT)
+	{
+		SPDK_NOTICELOG("transport_ack_timeout exceeds max value %d, use max value\n",
+					   CLIENT_RDMA_CTRLR_MAX_TRANSPORT_ACK_TIMEOUT);
+		rctrlr->ctrlr.opts.transport_ack_timeout = CLIENT_RDMA_CTRLR_MAX_TRANSPORT_ACK_TIMEOUT;
+	}
+
+	contexts = rdma_get_devices(NULL);
+	if (contexts == NULL)
+	{
+		SPDK_ERRLOG("rdma_get_devices() failed: %s (%d)\n", spdk_strerror(errno), errno);
+		client_rdma_free(rctrlr);
+		return NULL;
+	}
+
+	i = 0;
+	rctrlr->max_sge = CLIENT_RDMA_MAX_SGL_DESCRIPTORS;
+
+	while (contexts[i] != NULL)
+	{
+		rc = ibv_query_device(contexts[i], &dev_attr);
+		if (rc < 0)
+		{
+			SPDK_ERRLOG("Failed to query RDMA device attributes.\n");
+			rdma_free_devices(contexts);
+			client_rdma_free(rctrlr);
+			return NULL;
+		}
+		rctrlr->max_sge = spdk_min(rctrlr->max_sge, (uint16_t)dev_attr.max_sge);
+		i++;
+	}
+
+	rdma_free_devices(contexts);
+
+	rc = client_ctrlr_construct(&rctrlr->ctrlr);
+	if (rc != 0)
+	{
+		client_rdma_free(rctrlr);
+		return NULL;
+	}
+
+	STAILQ_INIT(&rctrlr->pending_cm_events);
+	STAILQ_INIT(&rctrlr->free_cm_events);
+	rctrlr->cm_events = client_rdma_calloc(CLIENT_RDMA_NUM_CM_EVENTS, sizeof(*rctrlr->cm_events));
+	if (rctrlr->cm_events == NULL)
+	{
+		SPDK_ERRLOG("unable to allocate buffers to hold CM events.\n");
+		goto destruct_ctrlr;
+	}
+
+	for (i = 0; i < CLIENT_RDMA_NUM_CM_EVENTS; i++)
+	{
+		STAILQ_INSERT_TAIL(&rctrlr->free_cm_events, &rctrlr->cm_events[i], link);
+	}
+
+	rctrlr->cm_channel = rdma_create_event_channel();
+	if (rctrlr->cm_channel == NULL)
+	{
+		SPDK_ERRLOG("rdma_create_event_channel() failed\n");
+		goto destruct_ctrlr;
+	}
+
+	flag = fcntl(rctrlr->cm_channel->fd, F_GETFL);
+	if (fcntl(rctrlr->cm_channel->fd, F_SETFL, flag | O_NONBLOCK) < 0)
+	{
+		SPDK_ERRLOG("Cannot set event channel to non blocking\n");
+		goto destruct_ctrlr;
+	}
+
+	if (client_ctrlr_add_process(&rctrlr->ctrlr, 0) != 0)
+	{
+		SPDK_ERRLOG("client_ctrlr_add_process() failed\n");
+		goto destruct_ctrlr;
+	}
+
+	SPDK_DEBUGLOG(client, "successfully initialized the srv ctrlr\n");
+	return &rctrlr->ctrlr;
+
+destruct_ctrlr:
+	client_ctrlr_destruct(&rctrlr->ctrlr);
+	return NULL;
+}
+
+static int
+client_rdma_ctrlr_destruct(struct spdk_client_ctrlr *ctrlr)
+{
+	struct client_rdma_ctrlr *rctrlr = client_rdma_ctrlr(ctrlr);
+	struct client_rdma_cm_event_entry *entry;
+
+	if (ctrlr->adminq)
+	{
+		client_rdma_ctrlr_delete_io_qpair(ctrlr, ctrlr->adminq);
+	}
+
+	STAILQ_FOREACH(entry, &rctrlr->pending_cm_events, link)
+	{
+		rdma_ack_cm_event(entry->evt);
+	}
+
+	STAILQ_INIT(&rctrlr->free_cm_events);
+	STAILQ_INIT(&rctrlr->pending_cm_events);
+	client_rdma_free(rctrlr->cm_events);
+
+	if (rctrlr->cm_channel)
+	{
+		rdma_destroy_event_channel(rctrlr->cm_channel);
+		rctrlr->cm_channel = NULL;
+	}
+
+	client_ctrlr_destruct_finish(ctrlr);
+
+	client_rdma_free(rctrlr);
+
+	return 0;
+}
+
+static int
+client_rdma_qpair_submit_request(struct spdk_client_qpair *qpair,
+								 struct client_request *req)
+{
+	struct client_rdma_qpair *rqpair;
+	struct spdk_client_rdma_req *rdma_req;
+	struct ibv_send_wr *wr;
+
+	rqpair = client_rdma_qpair(qpair);
+	assert(rqpair != NULL);
+	assert(req != NULL);
+
+	rdma_req = client_rdma_req_get(rqpair);
+	if (spdk_unlikely(!rdma_req))
+	{
+		if (rqpair->poller)
+		{
+			rqpair->poller->stats.queued_requests++;
+		}
+		/* Inform the upper layer to try again later. */
+		return -EAGAIN;
+	}
+
+	if (client_rdma_req_init(rqpair, req, rdma_req))
+	{
+		SPDK_ERRLOG("client_rdma_req_init() failed\n");
+		TAILQ_REMOVE(&rqpair->outstanding_reqs, rdma_req, link);
+		client_rdma_req_put(rqpair, rdma_req);
+		return -1;
+	}
+
+	wr = &rdma_req->send_wr;
+	wr->next = NULL;
+	client_rdma_trace_ibv_sge(wr->sg_list);
+	return client_rdma_qpair_queue_send_wr(rqpair, wr);
+}
+
+static int
+client_rdma_qpair_reset(struct spdk_client_qpair *qpair)
+{
+	/* Currently, doing nothing here */
+	return 0;
+}
+
+static void
+client_rdma_qpair_abort_reqs(struct spdk_client_qpair *qpair, uint32_t dnr)
+{
+	struct spdk_client_rdma_req *rdma_req, *tmp;
+	struct spdk_req_cpl cpl;
+	struct client_rdma_qpair *rqpair = client_rdma_qpair(qpair);
+
+	cpl.status.sc = SPDK_CLIENT_SC_ABORTED_SQ_DELETION;
+	cpl.status.sct = SPDK_CLIENT_SCT_GENERIC;
+	cpl.status.dnr = dnr;
+
+	/*
+	 * We cannot abort requests at the RDMA layer without
+	 * unregistering them. If we do, we can still get error
+	 * free completions on the shared completion queue.
+	 */
+	if (client_qpair_get_state(qpair) > CLIENT_QPAIR_DISCONNECTING &&
+		client_qpair_get_state(qpair) != CLIENT_QPAIR_DESTROYING)
+	{
+		client_ctrlr_disconnect_qpair(qpair);
+	}
+
+	TAILQ_FOREACH_SAFE(rdma_req, &rqpair->outstanding_reqs, link, tmp)
+	{
+		client_rdma_req_complete(rdma_req, &cpl);
+		client_rdma_req_put(rqpair, rdma_req);
+	}
+}
+
+static void
+client_rdma_qpair_check_timeout(struct spdk_client_qpair *qpair)
+{
+	uint64_t t02;
+	struct spdk_client_rdma_req *rdma_req, *tmp;
+	struct client_rdma_qpair *rqpair = client_rdma_qpair(qpair);
+	struct spdk_client_ctrlr *ctrlr = qpair->ctrlr;
+	struct spdk_client_ctrlr_process *active_proc;
+
+	/* Don't check timeouts during controller initialization. */
+	if (ctrlr->state != CLIENT_CTRLR_STATE_READY)
+	{
+		return;
+	}
+
+	if (client_qpair_is_admin_queue(qpair))
+	{
+		active_proc = client_ctrlr_get_current_process(ctrlr);
+	}
+	else
+	{
+		active_proc = qpair->active_proc;
+	}
+
+	/* Only check timeouts if the current process has a timeout callback. */
+	if (active_proc == NULL || active_proc->timeout_cb_fn == NULL)
+	{
+		return;
+	}
+
+	t02 = spdk_get_ticks();
+	TAILQ_FOREACH_SAFE(rdma_req, &rqpair->outstanding_reqs, link, tmp)
+	{
+		assert(rdma_req->req != NULL);
+
+		if (client_request_check_timeout(rdma_req->req, rdma_req->id, active_proc, t02))
+		{
+			/*
+			 * The requests are in order, so as soon as one has not timed out,
+			 * stop iterating.
+			 */
+			break;
+		}
+	}
+}
+
+static inline int
+client_rdma_request_ready(struct client_rdma_qpair *rqpair, struct spdk_client_rdma_req *rdma_req)
+{
+	client_rdma_req_complete(rdma_req, &rqpair->rsps[rdma_req->rsp_idx].cpl);
+	client_rdma_req_put(rqpair, rdma_req);
+	return client_rdma_post_recv(rqpair, rdma_req->rsp_idx);
+}
+
+#define MAX_COMPLETIONS_PER_POLL 128
+
+static void
+client_rdma_fail_qpair(struct spdk_client_qpair *qpair, int failure_reason)
+{
+	if (failure_reason == IBV_WC_RETRY_EXC_ERR)
+	{
+		qpair->transport_failure_reason = SPDK_CLIENT_QPAIR_FAILURE_REMOTE;
+	}
+	else if (qpair->transport_failure_reason == SPDK_CLIENT_QPAIR_FAILURE_NONE)
+	{
+		qpair->transport_failure_reason = SPDK_CLIENT_QPAIR_FAILURE_UNKNOWN;
+	}
+
+	client_ctrlr_disconnect_qpair(qpair);
+}
+
+static void
+client_rdma_conditional_fail_qpair(struct client_rdma_qpair *rqpair, struct client_rdma_poll_group *group)
+{
+	struct client_rdma_destroyed_qpair *qpair_tracker;
+
+	assert(rqpair);
+	if (group)
+	{
+		STAILQ_FOREACH(qpair_tracker, &group->destroyed_qpairs, link)
+		{
+			if (qpair_tracker->destroyed_qpair_tracker == rqpair)
+			{
+				return;
+			}
+		}
+	}
+	client_rdma_fail_qpair(&rqpair->qpair, 0);
+}
+
+static inline void
+client_rdma_log_wc_status(struct client_rdma_qpair *rqpair, struct ibv_wc *wc)
+{
+	struct client_rdma_wr *rdma_wr = (struct client_rdma_wr *)wc->wr_id;
+
+	if (wc->status == IBV_WC_WR_FLUSH_ERR)
+	{
+		/* If qpair is in ERR state, we will receive completions for all posted and not completed
+		 * Work Requests with IBV_WC_WR_FLUSH_ERR status. Don't log an error in that case */
+		SPDK_DEBUGLOG(client, "WC error, qid %u, qp state %d, request 0x%lu type %d, status: (%d): %s\n",
+					  rqpair->qpair.id, rqpair->qpair.state, wc->wr_id, rdma_wr->type, wc->status,
+					  ibv_wc_status_str(wc->status));
+	}
+	else
+	{
+		SPDK_ERRLOG("WC error, qid %u, qp state %d, request 0x%lu type %d, status: (%d): %s\n",
+					rqpair->qpair.id, rqpair->qpair.state, wc->wr_id, rdma_wr->type, wc->status,
+					ibv_wc_status_str(wc->status));
+	}
+}
+
+static inline bool
+client_rdma_is_rxe_device(struct ibv_device_attr *dev_attr)
+{
+	return dev_attr->vendor_id == SPDK_RDMA_RXE_VENDOR_ID_OLD ||
+		   dev_attr->vendor_id == SPDK_RDMA_RXE_VENDOR_ID_NEW;
+}
+
+static int
+client_rdma_cq_process_completions(struct ibv_cq *cq, uint32_t batch_size,
+								   struct client_rdma_poll_group *group,
+								   struct client_rdma_qpair *rdma_qpair,
+								   uint64_t *rdma_completions)
+{
+	struct ibv_wc wc[MAX_COMPLETIONS_PER_POLL];
+	struct client_rdma_qpair *rqpair;
+	struct spdk_client_rdma_req *rdma_req;
+	struct spdk_client_rdma_rsp *rdma_rsp;
+	struct client_rdma_wr *rdma_wr;
+	uint32_t reaped = 0;
+	int completion_rc = 0;
+	int rc, i;
+
+	rc = ibv_poll_cq(cq, batch_size, wc);
+	if (rc < 0)
+	{
+		SPDK_ERRLOG("Error polling CQ! (%d): %s\n",
+					errno, spdk_strerror(errno));
+		return -ECANCELED;
+	}
+	else if (rc == 0)
+	{
+		return 0;
+	}
+
+	for (i = 0; i < rc; i++)
+	{
+		rdma_wr = (struct client_rdma_wr *)wc[i].wr_id;
+		switch (rdma_wr->type)
+		{
+		case RDMA_WR_TYPE_RECV:
+			rdma_rsp = SPDK_CONTAINEROF(rdma_wr, struct spdk_client_rdma_rsp, rdma_wr);
+			rqpair = rdma_rsp->rqpair;
+			assert(rqpair->current_num_recvs > 0);
+			rqpair->current_num_recvs--;
+
+			if (wc[i].status)
+			{
+				client_rdma_log_wc_status(rqpair, &wc[i]);
+				client_rdma_conditional_fail_qpair(rqpair, group);
+				completion_rc = -ENXIO;
+				continue;
+			}
+
+			SPDK_DEBUGLOG(client, "CQ recv completion\n");
+
+			if (wc[i].byte_len < sizeof(struct spdk_req_cpl))
+			{
+				SPDK_ERRLOG("recv length %u less than expected response size\n", wc[i].byte_len);
+				client_rdma_conditional_fail_qpair(rqpair, group);
+				completion_rc = -ENXIO;
+				continue;
+			}
+			rdma_req = &rqpair->rdma_reqs[rdma_rsp->cpl.cid];
+			rdma_req->completion_flags |= CLIENT_RDMA_RECV_COMPLETED;
+			rdma_req->rsp_idx = rdma_rsp->idx;
+
+			if ((rdma_req->completion_flags & CLIENT_RDMA_SEND_COMPLETED) != 0)
+			{
+				if (spdk_unlikely(client_rdma_request_ready(rqpair, rdma_req)))
+				{
+					SPDK_ERRLOG("Unable to re-post rx descriptor\n");
+					client_rdma_conditional_fail_qpair(rqpair, group);
+					completion_rc = -ENXIO;
+					continue;
+				}
+				reaped++;
+				rqpair->num_completions++;
+			}
+			break;
+
+		case RDMA_WR_TYPE_SEND:
+			rdma_req = SPDK_CONTAINEROF(rdma_wr, struct spdk_client_rdma_req, rdma_wr);
+
+			/* If we are flushing I/O */
+			if (wc[i].status)
+			{
+				rqpair = rdma_req->req ? client_rdma_qpair(rdma_req->req->qpair) : NULL;
+				if (!rqpair)
+				{
+					rqpair = rdma_qpair != NULL ? rdma_qpair : client_rdma_poll_group_get_qpair_by_id(group, wc[i].qp_num);
+				}
+				if (!rqpair)
+				{
+					/* When poll_group is used, several qpairs share the same CQ and it is possible to
+					 * receive a completion with error (e.g. IBV_WC_WR_FLUSH_ERR) for already disconnected qpair
+					 * That happens due to qpair is destroyed while there are submitted but not completed send/receive
+					 * Work Requests
+					 * TODO: ibv qpair must be destroyed only when all submitted Work Requests are completed */
+					assert(group);
+					continue;
+				}
+				SPDK_NOTICELOG("******************cid %d %d\n", rdma_req->req->cmd.cid, rdma_req->id);
+				SPDK_NOTICELOG("******************num_entries %d\n", rqpair->num_entries);
+				SPDK_NOTICELOG("******************cmd address %v\n", &rqpair->cmds[rdma_req->req->cmd.cid]);
+				assert(rqpair->current_num_sends > 0);
+				rqpair->current_num_sends--;
+				client_rdma_log_wc_status(rqpair, &wc[i]);
+				client_rdma_conditional_fail_qpair(rqpair, group);
+				completion_rc = -ENXIO;
+				continue;
+			}
+
+			if (spdk_unlikely(rdma_req->req == NULL))
+			{
+				struct ibv_device_attr dev_attr;
+				int query_status;
+
+				/* Bug in Soft Roce - we may receive a completion without error status when qpair is disconnected/destroyed.
+				 * As sanity check - log an error if we use a real HW (it should never happen) */
+				query_status = ibv_query_device(cq->context, &dev_attr);
+				if (query_status == 0)
+				{
+					if (!client_rdma_is_rxe_device(&dev_attr))
+					{
+						SPDK_ERRLOG("Received malformed completion: request 0x%" PRIx64 " type %d\n", wc->wr_id,
+									rdma_wr->type);
+						assert(0);
+					}
+				}
+				else
+				{
+					SPDK_ERRLOG("Failed to query ib device\n");
+					assert(0);
+				}
+				continue;
+			}
+
+			rqpair = client_rdma_qpair(rdma_req->req->qpair);
+			rdma_req->completion_flags |= CLIENT_RDMA_SEND_COMPLETED;
+			rqpair->current_num_sends--;
+
+			if ((rdma_req->completion_flags & CLIENT_RDMA_RECV_COMPLETED) != 0)
+			{
+				if (spdk_unlikely(client_rdma_request_ready(rqpair, rdma_req)))
+				{
+					SPDK_ERRLOG("Unable to re-post rx descriptor\n");
+					client_rdma_conditional_fail_qpair(rqpair, group);
+					completion_rc = -ENXIO;
+					continue;
+				}
+				reaped++;
+				rqpair->num_completions++;
+			}
+			break;
+
+		default:
+			SPDK_ERRLOG("Received an unexpected opcode on the CQ: %d\n", rdma_wr->type);
+			return -ECANCELED;
+		}
+	}
+
+	*rdma_completions += rc;
+
+	if (completion_rc)
+	{
+		return completion_rc;
+	}
+	return reaped;
+}
+
+static void
+dummy_disconnected_qpair_cb(struct spdk_client_qpair *qpair, void *poll_group_ctx)
+{
+}
+
+static int
+client_rdma_qpair_process_completions(struct spdk_client_qpair *qpair,
+									  uint32_t max_completions)
+{
+	struct client_rdma_qpair *rqpair = client_rdma_qpair(qpair);
+	int rc = 0, batch_size;
+	struct ibv_cq *cq;
+	struct client_rdma_ctrlr *rctrlr = client_rdma_ctrlr(qpair->ctrlr);
+	uint64_t rdma_completions = 0;
+
+	/*
+	 * This is used during the connection phase. It's possible that we are still reaping error completions
+	 * from other qpairs so we need to call the poll group function. Also, it's more correct since the cq
+	 * is shared.
+	 */
+	if (qpair->poll_group != NULL)
+	{
+		return spdk_client_poll_group_process_completions(qpair->poll_group->group, max_completions,
+														  dummy_disconnected_qpair_cb);
+	}
+
+	if (max_completions == 0)
+	{
+		max_completions = rqpair->num_entries;
+	}
+	else
+	{
+		max_completions = spdk_min(max_completions, rqpair->num_entries);
+	}
+
+	switch (client_qpair_get_state(qpair))
+	{
+	case CLIENT_QPAIR_CONNECTING:
+		rc = client_rdma_ctrlr_connect_qpair_poll(qpair->ctrlr, qpair);
+		if (rc == 0)
+		{
+			/* Once the connection is completed, we can submit queued requests */
+			client_qpair_resubmit_requests(qpair, rqpair->num_entries);
+		}
+		else if (rc != -EAGAIN)
+		{
+			SPDK_ERRLOG("Failed to connect rqpair=%p\n", rqpair);
+			goto failed;
+		}
+		else if (rqpair->state <= CLIENT_RDMA_QPAIR_STATE_INITIALIZING)
+		{
+			return 0;
+		}
+		break;
+
+	case CLIENT_QPAIR_DISCONNECTING:
+		client_rdma_ctrlr_disconnect_qpair_poll(qpair->ctrlr, qpair);
+		return -ENXIO;
+
+	default:
+		if (client_qpair_is_admin_queue(qpair))
+		{
+			client_rdma_poll_events(rctrlr);
+		}
+		client_rdma_qpair_process_cm_event(rqpair);
+		break;
+	}
+
+	if (spdk_unlikely(qpair->transport_failure_reason != SPDK_CLIENT_QPAIR_FAILURE_NONE))
+	{
+		goto failed;
+	}
+
+	cq = rqpair->cq;
+
+	rqpair->num_completions = 0;
+	do
+	{
+		batch_size = spdk_min((max_completions - rqpair->num_completions), MAX_COMPLETIONS_PER_POLL);
+		rc = client_rdma_cq_process_completions(cq, batch_size, NULL, rqpair, &rdma_completions);
+
+		if (rc == 0)
+		{
+			break;
+			/* Handle the case where we fail to poll the cq. */
+		}
+		else if (rc == -ECANCELED)
+		{
+			goto failed;
+		}
+		else if (rc == -ENXIO)
+		{
+			return rc;
+		}
+	} while (rqpair->num_completions < max_completions);
+
+	if (spdk_unlikely(client_rdma_qpair_submit_sends(rqpair) ||
+					  client_rdma_qpair_submit_recvs(rqpair)))
+	{
+		goto failed;
+	}
+
+	if (spdk_unlikely(rqpair->qpair.ctrlr->timeout_enabled))
+	{
+		client_rdma_qpair_check_timeout(qpair);
+	}
+
+	return rqpair->num_completions;
+failed:
+	client_rdma_fail_qpair(qpair, 0);
+	return -ENXIO;
+}
+
+static uint32_t
+client_rdma_ctrlr_get_max_xfer_size(struct spdk_client_ctrlr *ctrlr)
+{
+	/* max_mr_size by ibv_query_device indicates the largest value that we can
+	 * set for a registered memory region.  It is independent from the actual
+	 * I/O size and is very likely to be larger than 2 MiB which is the
+	 * granularity we currently register memory regions.  Hence return
+	 * UINT32_MAX here and let the generic layer use the controller data to
+	 * moderate this value.
+	 */
+	return UINT32_MAX;
+}
+
+static uint16_t
+client_rdma_ctrlr_get_max_sges(struct spdk_client_ctrlr *ctrlr)
+{
+	struct client_rdma_ctrlr *rctrlr = client_rdma_ctrlr(ctrlr);
+
+	return rctrlr->max_sge;
+}
+
+static int
+client_rdma_qpair_iterate_requests(struct spdk_client_qpair *qpair,
+								   int (*iter_fn)(struct client_request *req, void *arg),
+								   void *arg)
+{
+	struct client_rdma_qpair *rqpair = client_rdma_qpair(qpair);
+	struct spdk_client_rdma_req *rdma_req, *tmp;
+	int rc;
+
+	assert(iter_fn != NULL);
+
+	TAILQ_FOREACH_SAFE(rdma_req, &rqpair->outstanding_reqs, link, tmp)
+	{
+		assert(rdma_req->req != NULL);
+
+		rc = iter_fn(rdma_req->req, arg);
+		if (rc != 0)
+		{
+			return rc;
+		}
+	}
+
+	return 0;
+}
+
+static void
+client_rdma_admin_qpair_abort_aers(struct spdk_client_qpair *qpair)
+{
+	struct spdk_client_rdma_req *rdma_req, *tmp;
+	struct spdk_req_cpl cpl;
+	struct client_rdma_qpair *rqpair = client_rdma_qpair(qpair);
+
+	cpl.status.sc = SPDK_CLIENT_SC_ABORTED_SQ_DELETION;
+	cpl.status.sct = SPDK_CLIENT_SCT_GENERIC;
+
+	TAILQ_FOREACH_SAFE(rdma_req, &rqpair->outstanding_reqs, link, tmp)
+	{
+		assert(rdma_req->req != NULL);
+
+		if (rdma_req->req->cmd.opc != SPDK_CLIENT_OPC_ASYNC_EVENT_REQUEST)
+		{
+			continue;
+		}
+
+		client_rdma_req_complete(rdma_req, &cpl);
+		client_rdma_req_put(rqpair, rdma_req);
+	}
+}
+
+static int
+client_rdma_poller_create(struct client_rdma_poll_group *group, struct ibv_context *ctx)
+{
+	struct client_rdma_poller *poller;
+
+	poller = calloc(1, sizeof(*poller));
+	if (poller == NULL)
+	{
+		SPDK_ERRLOG("Unable to allocate poller.\n");
+		return -ENOMEM;
+	}
+
+	poller->device = ctx;
+	poller->cq = ibv_create_cq(poller->device, DEFAULT_CLIENT_RDMA_CQ_SIZE, group, NULL, 0);
+
+	if (poller->cq == NULL)
+	{
+		free(poller);
+		return -EINVAL;
+	}
+
+	STAILQ_INSERT_HEAD(&group->pollers, poller, link);
+	group->num_pollers++;
+	poller->current_num_wc = DEFAULT_CLIENT_RDMA_CQ_SIZE;
+	poller->required_num_wc = 0;
+	return 0;
+}
+
+static void
+client_rdma_poll_group_free_pollers(struct client_rdma_poll_group *group)
+{
+	struct client_rdma_poller *poller, *tmp_poller;
+
+	STAILQ_FOREACH_SAFE(poller, &group->pollers, link, tmp_poller)
+	{
+		if (poller->cq)
+		{
+			ibv_destroy_cq(poller->cq);
+		}
+		STAILQ_REMOVE(&group->pollers, poller, client_rdma_poller, link);
+		free(poller);
+	}
+}
+
+static struct spdk_client_transport_poll_group *
+client_rdma_poll_group_create(void)
+{
+	struct client_rdma_poll_group *group;
+	struct ibv_context **contexts;
+	int i = 0;
+
+	group = calloc(1, sizeof(*group));
+	if (group == NULL)
+	{
+		SPDK_ERRLOG("Unable to allocate poll group.\n");
+		return NULL;
+	}
+
+	STAILQ_INIT(&group->pollers);
+
+	contexts = rdma_get_devices(NULL);
+	if (contexts == NULL)
+	{
+		SPDK_ERRLOG("rdma_get_devices() failed: %s (%d)\n", spdk_strerror(errno), errno);
+		free(group);
+		return NULL;
+	}
+
+	while (contexts[i] != NULL)
+	{
+		if (client_rdma_poller_create(group, contexts[i]))
+		{
+			client_rdma_poll_group_free_pollers(group);
+			free(group);
+			rdma_free_devices(contexts);
+			return NULL;
+		}
+		i++;
+	}
+
+	rdma_free_devices(contexts);
+	STAILQ_INIT(&group->destroyed_qpairs);
+	return &group->group;
+}
+
+struct client_rdma_qpair *
+client_rdma_poll_group_get_qpair_by_id(struct client_rdma_poll_group *group, uint32_t qp_num)
+{
+	struct spdk_client_qpair *qpair;
+	struct client_rdma_destroyed_qpair *rqpair_tracker;
+	struct client_rdma_qpair *rqpair;
+
+	STAILQ_FOREACH(qpair, &group->group.disconnected_qpairs, poll_group_stailq)
+	{
+		rqpair = client_rdma_qpair(qpair);
+		if (CLIENT_RDMA_POLL_GROUP_CHECK_QPN(rqpair, qp_num))
+		{
+			return rqpair;
+		}
+	}
+
+	STAILQ_FOREACH(qpair, &group->group.connected_qpairs, poll_group_stailq)
+	{
+		rqpair = client_rdma_qpair(qpair);
+		if (CLIENT_RDMA_POLL_GROUP_CHECK_QPN(rqpair, qp_num))
+		{
+			return rqpair;
+		}
+	}
+
+	STAILQ_FOREACH(rqpair_tracker, &group->destroyed_qpairs, link)
+	{
+		if (CLIENT_RDMA_POLL_GROUP_CHECK_QPN(rqpair_tracker->destroyed_qpair_tracker, qp_num))
+		{
+			return rqpair_tracker->destroyed_qpair_tracker;
+		}
+	}
+
+	return NULL;
+}
+
+/* static int
+client_rdma_resize_cq(struct client_rdma_qpair *rqpair, struct client_rdma_poller *poller)
+{
+	int current_num_wc, required_num_wc;
+
+	required_num_wc = poller->required_num_wc + WC_PER_QPAIR(rqpair->num_entries);
+	current_num_wc = poller->current_num_wc;
+	if (current_num_wc < required_num_wc)
+	{
+		current_num_wc = spdk_max(current_num_wc * 2, required_num_wc);
+	}
+
+	if (poller->current_num_wc != current_num_wc)
+	{
+		SPDK_DEBUGLOG(client, "Resize RDMA CQ from %d to %d\n", poller->current_num_wc,
+					  current_num_wc);
+		if (ibv_resize_cq(poller->cq, current_num_wc))
+		{
+			SPDK_ERRLOG("RDMA CQ resize failed: errno %d: %s\n", errno, spdk_strerror(errno));
+			return -1;
+		}
+
+		poller->current_num_wc = current_num_wc;
+	}
+
+	poller->required_num_wc = required_num_wc;
+	return 0;
+}
+ */
+static int
+client_rdma_poll_group_connect_qpair(struct spdk_client_qpair *qpair)
+{
+	/* 	struct client_rdma_qpair *rqpair = client_rdma_qpair(qpair);
+		struct client_rdma_poll_group *group = client_rdma_poll_group(qpair->poll_group);
+		struct client_rdma_poller *poller;
+
+		assert(rqpair->cq == NULL);
+
+		STAILQ_FOREACH(poller, &group->pollers, link)
+		{
+			if (poller->device == rqpair->cm_id->verbs)
+			{
+				if (client_rdma_resize_cq(rqpair, poller))
+				{
+					return -EPROTO;
+				}
+				rqpair->cq = poller->cq;
+				rqpair->poller = poller;
+				break;
+			}
+		}
+
+		if (rqpair->cq == NULL)
+		{
+			SPDK_ERRLOG("Unable to find a cq for qpair %p on poll group %p\n", qpair, qpair->poll_group);
+			return -EINVAL;
+		}
+	 */
+	return 0;
+}
+
+static int
+client_rdma_poll_group_disconnect_qpair(struct spdk_client_qpair *qpair)
+{
+	struct client_rdma_qpair *rqpair = client_rdma_qpair(qpair);
+	struct client_rdma_poll_group *group;
+	struct client_rdma_destroyed_qpair *destroyed_qpair;
+
+	group = client_rdma_poll_group(qpair->poll_group);
+
+	rqpair->cq = NULL;
+
+	/*
+	 * If this fails, the system is in serious trouble,
+	 * just let the qpair get cleaned up immediately.
+	 */
+	destroyed_qpair = calloc(1, sizeof(*destroyed_qpair));
+	if (destroyed_qpair == NULL)
+	{
+		return 0;
+	}
+
+	destroyed_qpair->destroyed_qpair_tracker = rqpair;
+	destroyed_qpair->timeout_ticks = spdk_get_ticks() +
+									 (CLIENT_RDMA_DESTROYED_QPAIR_EXPIRATION_TIMEOUT_US *
+									  spdk_get_ticks_hz()) /
+										 SPDK_SEC_TO_USEC;
+	STAILQ_INSERT_TAIL(&group->destroyed_qpairs, destroyed_qpair, link);
+
+	rqpair->defer_deletion_to_pg = true;
+
+	return 0;
+}
+
+static int
+client_rdma_poll_group_add(struct spdk_client_transport_poll_group *tgroup,
+						   struct spdk_client_qpair *qpair)
+{
+	return 0;
+}
+
+static int
+client_rdma_poll_group_remove(struct spdk_client_transport_poll_group *tgroup,
+							  struct spdk_client_qpair *qpair)
+{
+	assert(qpair->poll_group_tailq_head == &tgroup->disconnected_qpairs);
+
+	return 0;
+}
+
+static void
+client_rdma_poll_group_delete_qpair(struct client_rdma_poll_group *group,
+									struct client_rdma_destroyed_qpair *qpair_tracker)
+{
+	struct client_rdma_qpair *rqpair = qpair_tracker->destroyed_qpair_tracker;
+
+	rqpair->defer_deletion_to_pg = false;
+	if (client_qpair_get_state(&rqpair->qpair) == CLIENT_QPAIR_DESTROYING)
+	{
+		client_rdma_ctrlr_delete_io_qpair(rqpair->qpair.ctrlr, &rqpair->qpair);
+	}
+	STAILQ_REMOVE(&group->destroyed_qpairs, qpair_tracker, client_rdma_destroyed_qpair, link);
+	free(qpair_tracker);
+}
+
+static int
+client_rdma_ctrlr_disconnect_qpair_poll(struct spdk_client_ctrlr *ctrlr, struct spdk_client_qpair *qpair)
+{
+	struct client_rdma_qpair *rqpair = client_rdma_qpair(qpair);
+	int rc;
+
+	switch (rqpair->state)
+	{
+	case CLIENT_RDMA_QPAIR_STATE_EXITING:
+		if (!client_qpair_is_admin_queue(qpair))
+		{
+			client_robust_mutex_lock(&ctrlr->ctrlr_lock);
+		}
+
+		rc = client_rdma_process_event_poll(rqpair);
+
+		if (!client_qpair_is_admin_queue(qpair))
+		{
+			client_robust_mutex_unlock(&ctrlr->ctrlr_lock);
+		}
+		break;
+
+	case CLIENT_RDMA_QPAIR_STATE_LINGERING:
+		rc = client_rdma_qpair_wait_until_quiet(rqpair);
+		break;
+	case CLIENT_RDMA_QPAIR_STATE_EXITED:
+		rc = 0;
+		break;
+
+	default:
+		assert(false);
+		rc = -EAGAIN;
+		break;
+	}
+
+	return rc;
+}
+
+static int64_t
+client_rdma_poll_group_process_completions(struct spdk_client_transport_poll_group *tgroup,
+										   uint32_t completions_per_qpair, spdk_client_disconnected_qpair_cb disconnected_qpair_cb)
+{
+	struct spdk_client_qpair *qpair, *tmp_qpair;
+	struct client_rdma_destroyed_qpair *qpair_tracker, *tmp_qpair_tracker;
+	struct client_rdma_qpair *rqpair;
+	struct client_rdma_poll_group *group;
+	struct client_rdma_poller *poller;
+	int num_qpairs = 0, batch_size, rc;
+	int64_t total_completions = 0;
+	uint64_t completions_allowed = 0;
+	uint64_t completions_per_poller = 0;
+	uint64_t poller_completions = 0;
+	uint64_t rdma_completions;
+
+	if (completions_per_qpair == 0)
+	{
+		completions_per_qpair = MAX_COMPLETIONS_PER_POLL;
+	}
+
+	group = client_rdma_poll_group(tgroup);
+	STAILQ_FOREACH_SAFE(qpair, &tgroup->disconnected_qpairs, poll_group_stailq, tmp_qpair)
+	{
+		rc = client_rdma_ctrlr_disconnect_qpair_poll(qpair->ctrlr, qpair);
+		if (rc == 0)
+		{
+			disconnected_qpair_cb(qpair, tgroup->group->ctx);
+		}
+	}
+
+	STAILQ_FOREACH_SAFE(qpair, &tgroup->connected_qpairs, poll_group_stailq, tmp_qpair)
+	{
+		rqpair = client_rdma_qpair(qpair);
+		rqpair->num_completions = 0;
+
+		if (spdk_unlikely(client_qpair_get_state(qpair) == CLIENT_QPAIR_CONNECTING))
+		{
+			rc = client_rdma_ctrlr_connect_qpair_poll(qpair->ctrlr, qpair);
+			if (rc == 0)
+			{
+				/* Once the connection is completed, we can submit queued requests */
+				client_qpair_resubmit_requests(qpair, rqpair->num_entries);
+			}
+			else if (rc != -EAGAIN)
+			{
+				SPDK_ERRLOG("Failed to connect rqpair=%p\n", rqpair);
+				client_rdma_fail_qpair(qpair, 0);
+				continue;
+			}
+		}
+		else
+		{
+			client_rdma_qpair_process_cm_event(rqpair);
+		}
+
+		if (spdk_unlikely(qpair->transport_failure_reason != SPDK_CLIENT_QPAIR_FAILURE_NONE))
+		{
+			client_rdma_fail_qpair(qpair, 0);
+			disconnected_qpair_cb(qpair, tgroup->group->ctx);
+			continue;
+		}
+		num_qpairs++;
+	}
+
+	completions_allowed = completions_per_qpair * num_qpairs;
+	completions_per_poller = spdk_max(completions_allowed / group->num_pollers, 1);
+
+	STAILQ_FOREACH(poller, &group->pollers, link)
+	{
+		poller_completions = 0;
+		rdma_completions = 0;
+		do
+		{
+			poller->stats.polls++;
+			batch_size = spdk_min((completions_per_poller - poller_completions), MAX_COMPLETIONS_PER_POLL);
+			rc = client_rdma_cq_process_completions(poller->cq, batch_size, group, NULL, &rdma_completions);
+			if (rc <= 0)
+			{
+				if (rc == -ECANCELED)
+				{
+					return -EIO;
+				}
+				else if (rc == 0)
+				{
+					poller->stats.idle_polls++;
+				}
+				break;
+			}
+
+			poller_completions += rc;
+		} while (poller_completions < completions_per_poller);
+		total_completions += poller_completions;
+		poller->stats.completions += rdma_completions;
+	}
+
+	STAILQ_FOREACH_SAFE(qpair, &tgroup->connected_qpairs, poll_group_stailq, tmp_qpair)
+	{
+		rqpair = client_rdma_qpair(qpair);
+
+		if (spdk_unlikely(rqpair->state <= CLIENT_RDMA_QPAIR_STATE_INITIALIZING))
+		{
+			continue;
+		}
+		if (spdk_unlikely(qpair->ctrlr->timeout_enabled))
+		{
+			client_rdma_qpair_check_timeout(qpair);
+		}
+
+		client_rdma_qpair_submit_sends(rqpair);
+		client_rdma_qpair_submit_recvs(rqpair);
+		if (rqpair->num_completions > 0)
+		{
+			client_qpair_resubmit_requests(&rqpair->qpair, rqpair->num_completions);
+		}
+	}
+
+	return total_completions;
+}
+
+static int
+client_rdma_poll_group_destroy(struct spdk_client_transport_poll_group *tgroup)
+{
+	struct client_rdma_poll_group *group = client_rdma_poll_group(tgroup);
+	struct client_rdma_destroyed_qpair *qpair_tracker, *tmp_qpair_tracker;
+	struct client_rdma_qpair *rqpair;
+
+	if (!STAILQ_EMPTY(&tgroup->connected_qpairs) || !STAILQ_EMPTY(&tgroup->disconnected_qpairs))
+	{
+		return -EBUSY;
+	}
+
+	STAILQ_FOREACH_SAFE(qpair_tracker, &group->destroyed_qpairs, link, tmp_qpair_tracker)
+	{
+		rqpair = qpair_tracker->destroyed_qpair_tracker;
+		if (client_qpair_get_state(&rqpair->qpair) == CLIENT_QPAIR_DESTROYING)
+		{
+			rqpair->defer_deletion_to_pg = false;
+			client_rdma_ctrlr_delete_io_qpair(rqpair->qpair.ctrlr, &rqpair->qpair);
+		}
+
+		STAILQ_REMOVE(&group->destroyed_qpairs, qpair_tracker, client_rdma_destroyed_qpair, link);
+		free(qpair_tracker);
+	}
+
+	client_rdma_poll_group_free_pollers(group);
+	free(group);
+
+	return 0;
+}
+
+static int
+client_rdma_poll_group_get_stats(struct spdk_client_transport_poll_group *tgroup,
+								 struct spdk_client_transport_poll_group_stat **_stats)
+{
+	struct client_rdma_poll_group *group;
+	struct spdk_client_transport_poll_group_stat *stats;
+	struct spdk_client_rdma_device_stat *device_stat;
+	struct client_rdma_poller *poller;
+	uint32_t i = 0;
+
+	if (tgroup == NULL || _stats == NULL)
+	{
+		SPDK_ERRLOG("Invalid stats or group pointer\n");
+		return -EINVAL;
+	}
+
+	group = client_rdma_poll_group(tgroup);
+	stats = calloc(1, sizeof(*stats));
+	if (!stats)
+	{
+		SPDK_ERRLOG("Can't allocate memory for RDMA stats\n");
+		return -ENOMEM;
+	}
+	stats->trtype = SPDK_CLIENT_TRANSPORT_RDMA;
+	stats->rdma.num_devices = group->num_pollers;
+	stats->rdma.device_stats = calloc(stats->rdma.num_devices, sizeof(*stats->rdma.device_stats));
+	if (!stats->rdma.device_stats)
+	{
+		SPDK_ERRLOG("Can't allocate memory for RDMA device stats\n");
+		free(stats);
+		return -ENOMEM;
+	}
+
+	STAILQ_FOREACH(poller, &group->pollers, link)
+	{
+		device_stat = &stats->rdma.device_stats[i];
+		device_stat->name = poller->device->device->name;
+		device_stat->polls = poller->stats.polls;
+		device_stat->idle_polls = poller->stats.idle_polls;
+		device_stat->completions = poller->stats.completions;
+		device_stat->queued_requests = poller->stats.queued_requests;
+		device_stat->total_send_wrs = poller->stats.rdma_stats.send.num_submitted_wrs;
+		device_stat->send_doorbell_updates = poller->stats.rdma_stats.send.doorbell_updates;
+		device_stat->total_recv_wrs = poller->stats.rdma_stats.recv.num_submitted_wrs;
+		device_stat->recv_doorbell_updates = poller->stats.rdma_stats.recv.doorbell_updates;
+		i++;
+	}
+
+	*_stats = stats;
+
+	return 0;
+}
+
+static void
+client_rdma_poll_group_free_stats(struct spdk_client_transport_poll_group *tgroup,
+								  struct spdk_client_transport_poll_group_stat *stats)
+{
+	if (stats)
+	{
+		free(stats->rdma.device_stats);
+	}
+	free(stats);
+}
+
+static int
+client_rdma_ctrlr_get_memory_domains(const struct spdk_client_ctrlr *ctrlr,
+									 struct spdk_memory_domain **domains, int array_size)
+{
+	struct client_rdma_qpair *rqpair = client_rdma_qpair(ctrlr->adminq);
+
+	if (domains && array_size > 0)
+	{
+		domains[0] = rqpair->memory_domain->domain;
+	}
+
+	return 1;
+}
+
+void spdk_client_rdma_init_hooks(struct spdk_client_rdma_hooks *hooks)
+{
+	g_client_hooks = *hooks;
+}
+
+const struct spdk_client_transport_ops rdma_trans_ops = {
+	.name = "RDMA",
+	.type = SPDK_CLIENT_TRANSPORT_RDMA,
+	.ctrlr_construct = client_rdma_ctrlr_construct,
+	.ctrlr_destruct = client_rdma_ctrlr_destruct,
+	.ctrlr_enable = client_rdma_ctrlr_enable,
+
+	.ctrlr_get_max_xfer_size = client_rdma_ctrlr_get_max_xfer_size,
+	.ctrlr_get_max_sges = client_rdma_ctrlr_get_max_sges,
+
+	.ctrlr_create_io_qpair = client_rdma_ctrlr_create_io_qpair,
+	.ctrlr_delete_io_qpair = client_rdma_ctrlr_delete_io_qpair,
+	.ctrlr_connect_qpair = client_rdma_ctrlr_connect_qpair,
+	.ctrlr_disconnect_qpair = client_rdma_ctrlr_disconnect_qpair,
+
+	.ctrlr_get_memory_domains = client_rdma_ctrlr_get_memory_domains,
+
+	.qpair_abort_reqs = client_rdma_qpair_abort_reqs,
+	.qpair_reset = client_rdma_qpair_reset,
+	.qpair_submit_request = client_rdma_qpair_submit_request,
+	.qpair_process_completions = client_rdma_qpair_process_completions,
+	.qpair_iterate_requests = client_rdma_qpair_iterate_requests,
+	.admin_qpair_abort_aers = client_rdma_admin_qpair_abort_aers,
+
+	.poll_group_create = client_rdma_poll_group_create,
+	.poll_group_connect_qpair = client_rdma_poll_group_connect_qpair,
+	.poll_group_disconnect_qpair = client_rdma_poll_group_disconnect_qpair,
+	.poll_group_add = client_rdma_poll_group_add,
+	.poll_group_remove = client_rdma_poll_group_remove,
+	.poll_group_process_completions = client_rdma_poll_group_process_completions,
+	.poll_group_destroy = client_rdma_poll_group_destroy,
+	.poll_group_get_stats = client_rdma_poll_group_get_stats,
+	.poll_group_free_stats = client_rdma_poll_group_free_stats,
+};
+
+SPDK_CLIENT_TRANSPORT_REGISTER(rdma, &rdma_trans_ops);
\ No newline at end of file
diff --git a/lib/rdma_server/rdma_s.c b/lib/rdma_server/rdma_s.c
new file mode 100644
index 000000000..5c2b610d1
--- /dev/null
+++ b/lib/rdma_server/rdma_s.c
@@ -0,0 +1,5030 @@
+/*-
+ *   BSD LICENSE
+ *
+ *   Copyright (c) Intel Corporation. All rights reserved.
+ *   Copyright (c) 2019-2021 Mellanox Technologies LTD. All rights reserved.
+ *   Copyright (c) 2021, 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ *
+ *   Redistribution and use in source and binary forms, with or without
+ *   modification, are permitted provided that the following conditions
+ *   are met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in
+ *       the documentation and/or other materials provided with the
+ *       distribution.
+ *     * Neither the name of Intel Corporation nor the names of its
+ *       contributors may be used to endorse or promote products derived
+ *       from this software without specific prior written permission.
+ *
+ *   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ *   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ *   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ *   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ *   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ *   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ *   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ *   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ *   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ *   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ *   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "spdk/stdinc.h"
+
+#include "spdk/config.h"
+#include "spdk/thread.h"
+#include "spdk/likely.h"
+
+#include "spdk/string.h"
+#include "spdk/trace.h"
+#include "spdk/util.h"
+
+#include "spdk_internal/assert.h"
+#include "spdk/log.h"
+#include "spdk_internal/rdma.h"
+#include "spdk_internal/rdma_server.h"
+#include "spdk_internal/trace_defs.h"
+
+const struct spdk_srv_transport_ops spdk_srv_transport_rdma;
+void *g_rpc_dispatcher[SPDK_CLIENT_SUBMIT_TYPES_TOTAL] = {NULL, NULL};
+
+/*
+ RDMA Connection Resource Defaults
+ */
+#define SRV_DEFAULT_TX_SGE SPDK_SRV_MAX_SGL_ENTRIES
+#define SRV_DEFAULT_RSP_SGE 1
+#define SRV_DEFAULT_RX_SGE 2
+
+/* The RDMA completion queue size */
+#define DEFAULT_SRV_RDMA_CQ_SIZE 4096
+#define MAX_WR_PER_QP(queue_depth) (queue_depth * 3 + 2)
+
+#define MAX_RPC_REQ_QUEUE_DEPTH 4096
+
+static int g_spdk_srv_ibv_query_mask =
+	IBV_QP_STATE |
+	IBV_QP_PKEY_INDEX |
+	IBV_QP_PORT |
+	IBV_QP_ACCESS_FLAGS |
+	IBV_QP_AV |
+	IBV_QP_PATH_MTU |
+	IBV_QP_DEST_QPN |
+	IBV_QP_RQ_PSN |
+	IBV_QP_MAX_DEST_RD_ATOMIC |
+	IBV_QP_MIN_RNR_TIMER |
+	IBV_QP_SQ_PSN |
+	IBV_QP_TIMEOUT |
+	IBV_QP_RETRY_CNT |
+	IBV_QP_RNR_RETRY |
+	IBV_QP_MAX_QP_RD_ATOMIC;
+
+enum spdk_srv_rdma_request_state
+{
+	/* The request is not currently in use */
+	RDMA_REQUEST_STATE_FREE = 0,
+
+	/* Initial state when request first received */
+	RDMA_REQUEST_STATE_NEW,
+
+	/* The request is queued until a data buffer is available. */
+	RDMA_REQUEST_STATE_NEED_BUFFER,
+
+	/* The request is waiting on RDMA queue depth availability
+	 * to transfer data from the host to the controller.
+	 */
+	RDMA_REQUEST_STATE_DATA_TRANSFER_TO_CONTROLLER_PENDING,
+
+	/* The request is currently transferring data from the host to the controller. */
+	RDMA_REQUEST_STATE_TRANSFERRING_HOST_TO_CONTROLLER,
+
+	/* The request is ready to execute at the block device */
+	RDMA_REQUEST_STATE_READY_TO_EXECUTE,
+
+	/* The request is currently executing at the block device */
+	RDMA_REQUEST_STATE_EXECUTING,
+
+	/* The request finished executing at the block device */
+	RDMA_REQUEST_STATE_EXECUTED,
+
+	/* The request is waiting on RDMA queue depth availability
+	 * to transfer data from the controller to the host.
+	 */
+	RDMA_REQUEST_STATE_DATA_TRANSFER_TO_HOST_PENDING,
+
+	/* The request is ready to send a completion */
+	RDMA_REQUEST_STATE_READY_TO_COMPLETE,
+
+	/* The request is currently transferring data from the controller to the host. */
+	RDMA_REQUEST_STATE_TRANSFERRING_CONTROLLER_TO_HOST,
+
+	/* The request currently has an outstanding completion without an
+	 * associated data transfer.
+	 */
+	RDMA_REQUEST_STATE_COMPLETING,
+
+	/* The request completed and can be marked free. */
+	RDMA_REQUEST_STATE_COMPLETED,
+
+	/* Terminator */
+	RDMA_REQUEST_NUM_STATES,
+};
+
+enum spdk_srv_rdma_wr_type
+{
+	RDMA_WR_TYPE_RECV,
+	RDMA_WR_TYPE_SEND,
+	RDMA_WR_TYPE_DATA,
+};
+
+struct spdk_srv_rdma_wr
+{
+	enum spdk_srv_rdma_wr_type type;
+};
+
+/* This structure holds commands as they are received off the wire.
+ * It must be dynamically paired with a full request object
+ * (spdk_srv_rdma_request) to service a request. It is separate
+ * from the request because RDMA does not appear to order
+ * completions, so occasionally we'll get a new incoming
+ * command when there aren't any free request objects.
+ */
+struct spdk_srv_rdma_recv
+{
+	struct ibv_recv_wr wr;
+	struct ibv_sge sgl[SRV_DEFAULT_RX_SGE];
+
+	struct spdk_srv_rdma_conn *conn;
+
+	/* In-capsule data buffer */
+	uint8_t *buf;
+
+	struct spdk_srv_rdma_wr rdma_wr;
+	uint64_t receive_tsc;
+
+	STAILQ_ENTRY(spdk_srv_rdma_recv)
+	link;
+};
+
+struct spdk_srv_rdma_request_data
+{
+	struct spdk_srv_rdma_wr rdma_wr;
+	struct ibv_send_wr wr;
+	struct ibv_sge sgl[SPDK_SRV_MAX_SGL_ENTRIES];
+};
+
+struct spdk_srv_rdma_request
+{
+	struct spdk_srv_request req;
+
+	enum spdk_srv_rdma_request_state state;
+
+	/* Data offset in req.iov */
+	uint32_t offset;
+
+	struct spdk_srv_rdma_recv *recv;
+
+	struct
+	{
+		struct spdk_srv_rdma_wr rdma_wr;
+		struct ibv_send_wr wr;
+		struct ibv_sge sgl[SRV_DEFAULT_RSP_SGE];
+	} rsp;
+
+	struct spdk_srv_rdma_request_data data;
+
+	uint32_t iovpos;
+
+	uint32_t num_outstanding_data_wr;
+	uint64_t receive_tsc;
+
+	STAILQ_ENTRY(spdk_srv_rdma_request)
+	state_link;
+};
+
+struct spdk_srv_rdma_resource_opts
+{
+	struct spdk_srv_rdma_conn *conn;
+	/* qp points either to an ibv_qp object or an ibv_srq object depending on the value of shared. */
+	void *qp;
+	struct ibv_pd *pd;
+	uint32_t max_queue_depth;
+	uint32_t in_capsule_data_size;
+	bool shared;
+};
+
+struct spdk_srv_rpc_request;
+
+struct spdk_srv_rdma_resources
+{
+	/* Array of size "max_queue_depth" containing RDMA requests. */
+	struct spdk_srv_rdma_request *reqs;
+
+	/* Array of size "max_queue_depth" containing RDMA recvs. */
+	struct spdk_srv_rdma_recv *recvs;
+
+	/* Array of size "max_queue_depth" containing 64 byte capsules
+	 * used for receive.
+	 */
+	struct spdk_req_cmd *cmds;
+	struct ibv_mr *cmds_mr;
+
+	/* Array of size "max_queue_depth" containing 16 byte completions
+	 * to be sent back to the user.
+	 */
+	struct spdk_req_cpl *cpls;
+	struct ibv_mr *cpls_mr;
+
+	/* Array of size "max_queue_depth * InCapsuleDataSize" containing
+	 * buffers to be used for in capsule data.
+	 */
+	void *bufs;
+	struct ibv_mr *bufs_mr;
+
+	/* Receives that are waiting for a request object */
+	STAILQ_HEAD(, spdk_srv_rdma_recv)
+	incoming_queue;
+
+	/* Queue to track free requests */
+	STAILQ_HEAD(, spdk_srv_rdma_request)
+	free_queue;
+
+	struct spdk_srv_rpc_request *rpc_reqs;
+	/* Queue to track inflight requests */
+	STAILQ_HEAD(, spdk_srv_rpc_request)
+	inflight_rpc_queue;
+};
+
+typedef void (*spdk_srv_rdma_conn_ibv_event)(struct spdk_srv_rdma_conn *rconn);
+
+struct spdk_srv_rdma_ibv_event_ctx
+{
+	struct spdk_srv_rdma_conn *rconn;
+	spdk_srv_rdma_conn_ibv_event cb_fn;
+	/* Link to other ibv events associated with this conn */
+	STAILQ_ENTRY(spdk_srv_rdma_ibv_event_ctx)
+	link;
+};
+
+struct spdk_srv_rdma_conn
+{
+	struct spdk_srv_conn conn;
+
+	struct spdk_srv_rdma_device *device;
+	struct spdk_srv_rdma_poller *poller;
+
+	struct spdk_rdma_qp *rdma_qp;
+	struct rdma_cm_id *cm_id;
+	struct spdk_rdma_srq *srq;
+	struct rdma_cm_id *listen_id;
+
+	/* The maximum number of I/O outstanding on this connection at one time */
+	uint16_t max_queue_depth;
+
+	/* The maximum number of active RDMA READ and ATOMIC operations at one time */
+	uint16_t max_read_depth;
+
+	/* The maximum number of RDMA SEND operations at one time */
+	uint32_t max_send_depth;
+
+	/* The current number of outstanding WRs from this conn's
+	 * recv queue. Should not exceed device->attr.max_queue_depth.
+	 */
+	uint16_t current_recv_depth;
+
+	/* The current number of active RDMA READ operations */
+	uint16_t current_read_depth;
+
+	/* The current number of posted WRs from this conn's
+	 * send queue. Should not exceed max_send_depth.
+	 */
+	uint32_t current_send_depth;
+
+	/* The maximum number of SGEs per WR on the send queue */
+	uint32_t max_send_sge;
+
+	/* The maximum number of SGEs per WR on the recv queue */
+	uint32_t max_recv_sge;
+
+	struct spdk_srv_rdma_resources *resources;
+
+	STAILQ_HEAD(, spdk_srv_rdma_request)
+	pending_rdma_read_queue;
+
+	STAILQ_HEAD(, spdk_srv_rdma_request)
+	pending_rdma_write_queue;
+
+	// rpc 请求完成后,将对应的rdma_request,放在这个队列里面等待执行
+	STAILQ_HEAD(, spdk_srv_rdma_request)
+	pending_complete_queue;
+
+	/* Number of requests not in the free state */
+	uint32_t qd;
+
+	TAILQ_ENTRY(spdk_srv_rdma_conn)
+	link;
+
+	STAILQ_ENTRY(spdk_srv_rdma_conn)
+	recv_link;
+
+	STAILQ_ENTRY(spdk_srv_rdma_conn)
+	send_link;
+
+	/* IBV queue pair attributes: they are used to manage
+	 * qp state and recover from errors.
+	 */
+	enum ibv_qp_state ibv_state;
+
+	/*
+	 * io_channel which is used to destroy conn when it is removed from poll group
+	 */
+	struct spdk_io_channel *destruct_channel;
+
+	/* List of ibv async events */
+	STAILQ_HEAD(, spdk_srv_rdma_ibv_event_ctx)
+	ibv_events;
+
+	/* Lets us know that we have received the last_wqe event. */
+	bool last_wqe_reached;
+
+	/* Indicate that srv_rdma_close_conn is called */
+	bool to_close;
+};
+
+enum rpc_request_state
+{
+	FREE,
+	WAIT_OTHER_SUBREQUEST,
+	PROCESS_DATA,
+	PENDING_READ,
+	READ_COMPLETE,
+	FINISH,
+};
+
+struct spdk_srv_rpc_request
+{
+	uint32_t rpc_opc;
+	uint32_t submit_type;
+	// 用于接收全部的rpc请求数据
+	struct iovec *in_iovs;
+	uint32_t in_iov_cnt_total;
+	uint32_t in_iov_cnt_left;
+	uint32_t in_real_length;
+	uint32_t iov_offset;
+
+	void *out_data;
+
+	// submit = 1时使用out_ioves和out_iov_cnt
+	struct iovec *out_iovs;
+	int out_iov_cnt;
+
+	uint32_t out_status;
+	uint32_t out_real_length;
+
+	uint32_t out_rdma_send_left;
+
+	// 通知服务端 RPC请求响应发送完毕的回调
+	spdk_srv_rpc_service_complete_cb service_cb;
+	void *service_cb_arg;
+	STAILQ_ENTRY(spdk_srv_rpc_request)
+	stq_link;
+	enum rpc_request_state state;
+
+	bool check_md5;						// true 表示response需要计算MD5
+	uint8_t md5sum[SPDK_MD5DIGEST_LEN]; // 用于存储response的MD5
+
+	struct spdk_srv_rdma_conn *rconn;
+	STAILQ_HEAD(, spdk_srv_rdma_request)
+	wait_rpc_handle_queue;
+	uint32_t rpc_index;
+};
+
+static int md5init(struct spdk_md5ctx *md5ctx)
+{
+	int rc;
+
+	if (md5ctx == NULL)
+	{
+		return -1;
+	}
+
+	md5ctx->md5ctx = EVP_MD_CTX_create();
+	if (md5ctx->md5ctx == NULL)
+	{
+		return -1;
+	}
+
+	rc = EVP_DigestInit_ex(md5ctx->md5ctx, EVP_md5(), NULL);
+	/* For EVP_DigestInit_ex, 1 == success, 0 == failure. */
+	if (rc == 0)
+	{
+		EVP_MD_CTX_destroy(md5ctx->md5ctx);
+		md5ctx->md5ctx = NULL;
+	}
+	return rc;
+}
+
+static int md5final(void *md5, struct spdk_md5ctx *md5ctx)
+{
+	int rc;
+
+	if (md5ctx == NULL || md5 == NULL)
+	{
+		return -1;
+	}
+	rc = EVP_DigestFinal_ex(md5ctx->md5ctx, (unsigned char *)md5, NULL);
+	EVP_MD_CTX_destroy(md5ctx->md5ctx);
+	md5ctx->md5ctx = NULL;
+	return rc;
+}
+
+static int md5update(struct spdk_md5ctx *md5ctx, const void *data, size_t len)
+{
+	int rc;
+
+	if (md5ctx == NULL)
+	{
+		return -1;
+	}
+	if (data == NULL || len == 0)
+	{
+		return 0;
+	}
+	rc = EVP_DigestUpdate(md5ctx->md5ctx, data, len);
+	return rc;
+}
+
+// TODO:完成RPC 回调注册
+void spdk_srv_rpc_register_dispatcher(void *dispatcher, int submit_type)
+{
+	if (g_rpc_dispatcher[submit_type] == NULL)
+	{
+		g_rpc_dispatcher[submit_type] = dispatcher;
+		SPDK_INFOLOG(rdma, "rpc dispatcher with submit type %d successfull registered\n", submit_type);
+	}
+	else
+	{
+		SPDK_ERRLOG("rpc dispatcher with submit type %d already registered\n", submit_type);
+	}
+}
+
+void spdk_srv_rpc_request_handle_complete_cb(void *cb_arg, int status, char *rsp_data, int len, spdk_srv_rpc_service_complete_cb service_cb, void *service_cb_arg)
+{
+	int ret = 0;
+	struct spdk_srv_rdma_request *rdma_req, *temp_rdma_req;
+	struct spdk_srv_rdma_conn *rconn;
+	struct spdk_req_cpl *rsp;
+	struct spdk_md5ctx md5ctx;
+
+	/* If the number of buffers is too large, then we know the I/O is larger than allowed.
+	 *  Fail it.
+	 */
+
+	struct spdk_srv_rpc_request *req = cb_arg;
+	SPDK_DEBUGLOG(rdma, "spdk_srv_rpc_request_handle_complete_cb req addr:%p, index:%d, state:%d\n", (uintptr_t)req, req->rpc_index, req->state);
+	rconn = req->rconn;
+	if (req->rconn == NULL)
+	{
+		SPDK_ERRLOG("Fatal error req addr:%p, index:%d, state:%d\n", (uintptr_t)req, req->rpc_index, req->state);
+	}
+	uint32_t max_io_size = req->rconn->conn.transport->opts.max_io_size;
+
+	// 提前计算好client要切分的子请求个数，也就是要响应的次数
+	req->out_rdma_send_left = SPDK_CEIL_DIV(len, max_io_size);
+	req->out_real_length = len;
+	req->out_data = rsp_data;
+	req->out_status = status;
+	req->service_cb = service_cb;
+	req->service_cb_arg = service_cb_arg;
+
+	SPDK_DEBUGLOG(rdma, "spdk_srv_rpc_request_handle_complete_cb %d %p %p %p\n", len, rsp_data, service_cb, service_cb_arg);
+
+	if (req->check_md5)
+	{
+		md5init(&md5ctx);
+		md5update(&md5ctx, rsp_data, len);
+		md5final(req->md5sum, &md5ctx);
+	}
+
+	// TODO: 把链表上rdma request放到pending_complete_queue上
+	rdma_req = STAILQ_FIRST(&req->wait_rpc_handle_queue);
+	assert(rdma_req->recv != NULL);
+	SPDK_DEBUGLOG(rdma, "spdk_srv_rpc_request_handle_complete_cb rconn=%p\n", rconn);
+	SPDK_DEBUGLOG(rdma, "spdk_srv_rpc_request_handle_complete_cb queue_head=%p\n", &rconn->pending_complete_queue);
+	while (rdma_req)
+	{
+		SPDK_DEBUGLOG(rdma, "000 rdma_req=%p\n", rdma_req);
+		// 提前处理rsp的一些字段
+		rsp = rdma_req->req.rsp;
+		rsp->sqid = 0;
+		rsp->status.p = 0;
+		rsp->cdw0 = status; // 这个其实没什么用，因为在rpc_read的时候,会把这个status再带回去
+		rsp->cid = rdma_req->req.cmd->cid;
+		rsp->cdw1 = req->out_real_length;
+		temp_rdma_req = STAILQ_NEXT(rdma_req, state_link);
+		STAILQ_REMOVE_HEAD(&req->wait_rpc_handle_queue, state_link);
+		SPDK_DEBUGLOG(rdma, "111 rdma_req=%p stqe_next=%p temp_rdma_req=%p out_real_length=%d\n", rdma_req, rdma_req->state_link.stqe_next, temp_rdma_req, req->out_real_length);
+		STAILQ_INSERT_TAIL(&rconn->pending_complete_queue, rdma_req, state_link);
+		assert(rdma_req->recv != NULL);
+		rdma_req = temp_rdma_req;
+	}
+
+	req->state = PENDING_READ;
+	SPDK_DEBUGLOG(rdma, "spdk_srv_rpc_request_handle_complete_cb finish\n");
+}
+
+void spdk_srv_rpc_request_handle_complete_iovs_cb(void *cb_arg, int status, struct iovec *iovs, int iov_cnt, int len, spdk_srv_rpc_service_complete_cb service_cb, void *service_cb_arg)
+{
+	int ret = 0;
+	struct spdk_srv_rdma_request *rdma_req, *temp_rdma_req;
+	struct spdk_srv_rdma_conn *rconn;
+	struct spdk_req_cpl *rsp;
+	struct spdk_md5ctx md5ctx;
+
+	/* If the number of buffers is too large, then we know the I/O is larger than allowed.
+	 *  Fail it.
+	 */
+
+	struct spdk_srv_rpc_request *req = cb_arg;
+	SPDK_DEBUGLOG(rdma, "spdk_srv_rpc_request_handle_complete_iovs_cb req addr:%p, index:%d, state:%d\n", (uintptr_t)req, req->rpc_index, req->state);
+	rconn = req->rconn;
+	if (req->rconn == NULL)
+	{
+		SPDK_ERRLOG("Fatal error req addr:%p, index:%d, state:%d\n", (uintptr_t)req, req->rpc_index, req->state);
+	}
+	uint32_t max_io_size = req->rconn->conn.transport->opts.max_io_size;
+
+	// 提前计算好client要切分的子请求个数，也就是要响应的次数
+
+	assert(req->submit_type == SPDK_CLIENT_SUBMIT_IOVES);
+	req->out_rdma_send_left = SPDK_CEIL_DIV(len, max_io_size);
+	req->out_real_length = len;
+	req->out_iovs = iovs;
+	req->out_iov_cnt = iov_cnt;
+	req->out_status = status;
+	req->service_cb = service_cb;
+	req->service_cb_arg = service_cb_arg;
+
+	SPDK_DEBUGLOG(rdma, "spdk_srv_rpc_request_handle_complete_iovs_cb %d %p %p\n", len, service_cb, service_cb_arg);
+
+	if (req->check_md5)
+	{
+		md5init(&md5ctx);
+		for (int i = 0; i < iov_cnt; i++)
+		{
+			md5update(&md5ctx, iovs[i].iov_base, iovs[i].iov_len);
+		}
+		md5final(req->md5sum, &md5ctx);
+	}
+
+	// TODO: 把链表上rdma request放到pending_complete_queue上
+	rdma_req = STAILQ_FIRST(&req->wait_rpc_handle_queue);
+	assert(rdma_req->recv != NULL);
+	SPDK_DEBUGLOG(rdma, "spdk_srv_rpc_request_handle_complete_iovs_cb rconn=%p\n", rconn);
+	SPDK_DEBUGLOG(rdma, "spdk_srv_rpc_request_handle_complete_iovs_cb queue_head=%p\n", &rconn->pending_complete_queue);
+	while (rdma_req)
+	{
+		SPDK_DEBUGLOG(rdma, "000 rdma_req=%p\n", rdma_req);
+		// 提前处理rsp的一些字段
+		rsp = rdma_req->req.rsp;
+		rsp->sqid = 0;
+		rsp->status.p = 0;
+		rsp->cdw0 = status; // 这个其实没什么用，因为在rpc_read的时候,会把这个status再带回去
+		rsp->cid = rdma_req->req.cmd->cid;
+		rsp->cdw1 = req->out_real_length;
+		temp_rdma_req = STAILQ_NEXT(rdma_req, state_link);
+		STAILQ_REMOVE_HEAD(&req->wait_rpc_handle_queue, state_link);
+		SPDK_DEBUGLOG(rdma, "111 rdma_req=%p stqe_next=%p temp_rdma_req=%p out_real_length=%d\n", rdma_req, rdma_req->state_link.stqe_next, temp_rdma_req, req->out_real_length);
+		STAILQ_INSERT_TAIL(&rconn->pending_complete_queue, rdma_req, state_link);
+		assert(rdma_req->recv != NULL);
+		rdma_req = temp_rdma_req;
+	}
+	req->state = PENDING_READ;
+	SPDK_DEBUGLOG(rdma, "spdk_srv_rpc_request_handle_complete_cb finish\n");
+}
+
+struct spdk_srv_rdma_poller_stat
+{
+	uint64_t completions;
+	uint64_t polls;
+	uint64_t idle_polls;
+	uint64_t requests;
+	uint64_t request_latency;
+	uint64_t pending_free_request;
+	uint64_t pending_rdma_read;
+	uint64_t pending_rdma_write;
+	struct spdk_rdma_qp_stats qp_stats;
+};
+
+struct spdk_srv_rdma_poller
+{
+	struct spdk_srv_rdma_device *device;
+	struct spdk_srv_rdma_poll_group *group;
+
+	int num_cqe;
+	int required_num_wr;
+	struct ibv_cq *cq;
+
+	/* The maximum number of I/O outstanding on the shared receive queue at one time */
+	uint16_t max_srq_depth;
+
+	/* Shared receive queue */
+	struct spdk_rdma_srq *srq;
+
+	struct spdk_srv_rdma_resources *resources;
+	struct spdk_srv_rdma_poller_stat stat;
+
+	TAILQ_HEAD(, spdk_srv_rdma_conn)
+	conns;
+
+	STAILQ_HEAD(, spdk_srv_rdma_conn)
+	conns_pending_recv;
+
+	STAILQ_HEAD(, spdk_srv_rdma_conn)
+	conns_pending_send;
+
+	TAILQ_ENTRY(spdk_srv_rdma_poller)
+	link;
+};
+
+struct spdk_srv_rdma_poll_group_stat
+{
+	uint64_t pending_data_buffer;
+};
+
+struct spdk_srv_rdma_poll_group
+{
+	struct spdk_srv_transport_poll_group group;
+	struct spdk_srv_rdma_poll_group_stat stat;
+	TAILQ_HEAD(, spdk_srv_rdma_poller)
+	pollers;
+	TAILQ_ENTRY(spdk_srv_rdma_poll_group)
+	link;
+};
+
+struct spdk_srv_rdma_conn_sched
+{
+	struct spdk_srv_rdma_poll_group *next_admin_pg;
+	struct spdk_srv_rdma_poll_group *next_io_pg;
+};
+
+/* Assuming rdma_cm uses just one protection domain per ibv_context. */
+struct spdk_srv_rdma_device
+{
+	struct ibv_device_attr attr;
+	struct ibv_context *context;
+
+	struct spdk_rdma_mem_map *map;
+	struct ibv_pd *pd;
+
+	int num_srq;
+
+	TAILQ_ENTRY(spdk_srv_rdma_device)
+	link;
+};
+
+struct spdk_srv_rdma_port
+{
+	const struct spdk_srv_transport_id *trid;
+	struct rdma_cm_id *id;
+	struct spdk_srv_rdma_device *device;
+	TAILQ_ENTRY(spdk_srv_rdma_port)
+	link;
+};
+
+struct rdma_transport_opts
+{
+	int num_cqe;
+	uint32_t max_srq_depth;
+	bool no_srq;
+	bool no_wr_batching;
+	int acceptor_backlog;
+};
+
+struct spdk_srv_rdma_transport
+{
+	struct spdk_srv_transport transport;
+	struct rdma_transport_opts rdma_opts;
+
+	struct spdk_srv_rdma_conn_sched conn_sched;
+
+	struct rdma_event_channel *event_channel;
+
+	struct spdk_mempool *data_wr_pool;
+
+	struct spdk_poller *accept_poller;
+	pthread_mutex_t lock;
+
+	/* fields used to poll RDMA/IB events */
+	nfds_t npoll_fds;
+	struct pollfd *poll_fds;
+
+	TAILQ_HEAD(, spdk_srv_rdma_device)
+	devices;
+	TAILQ_HEAD(, spdk_srv_rdma_port)
+	ports;
+	TAILQ_HEAD(, spdk_srv_rdma_poll_group)
+	poll_groups;
+};
+
+static const struct spdk_json_object_decoder rdma_transport_opts_decoder[] = {
+	{"num_cqe", offsetof(struct rdma_transport_opts, num_cqe),
+	 spdk_json_decode_int32, true},
+	{"max_srq_depth", offsetof(struct rdma_transport_opts, max_srq_depth),
+	 spdk_json_decode_uint32, true},
+	{"no_srq", offsetof(struct rdma_transport_opts, no_srq),
+	 spdk_json_decode_bool, true},
+	{"no_wr_batching", offsetof(struct rdma_transport_opts, no_wr_batching),
+	 spdk_json_decode_bool, true},
+	{"acceptor_backlog", offsetof(struct rdma_transport_opts, acceptor_backlog),
+	 spdk_json_decode_int32, true},
+};
+
+static bool
+srv_rdma_request_process(struct spdk_srv_rdma_transport *rtransport,
+						 struct spdk_srv_rdma_request *rdma_req);
+
+static void
+_poller_submit_sends(struct spdk_srv_rdma_transport *rtransport,
+					 struct spdk_srv_rdma_poller *rpoller);
+
+static void
+_poller_submit_recvs(struct spdk_srv_rdma_transport *rtransport,
+					 struct spdk_srv_rdma_poller *rpoller);
+
+static inline int
+srv_rdma_check_ibv_state(enum ibv_qp_state state)
+{
+	switch (state)
+	{
+	case IBV_QPS_RESET:
+	case IBV_QPS_INIT:
+	case IBV_QPS_RTR:
+	case IBV_QPS_RTS:
+	case IBV_QPS_SQD:
+	case IBV_QPS_SQE:
+	case IBV_QPS_ERR:
+		return 0;
+	default:
+		return -1;
+	}
+}
+
+static enum ibv_qp_state
+srv_rdma_update_ibv_state(struct spdk_srv_rdma_conn *rconn)
+{
+	enum ibv_qp_state old_state, new_state;
+	struct ibv_qp_attr qp_attr;
+	struct ibv_qp_init_attr init_attr;
+	int rc;
+
+	old_state = rconn->ibv_state;
+	rc = ibv_query_qp(rconn->rdma_qp->qp, &qp_attr,
+					  g_spdk_srv_ibv_query_mask, &init_attr);
+
+	if (rc)
+	{
+		SPDK_ERRLOG("Failed to get updated RDMA queue pair state!\n");
+		return IBV_QPS_ERR + 1;
+	}
+
+	new_state = qp_attr.qp_state;
+	rconn->ibv_state = new_state;
+	qp_attr.ah_attr.port_num = qp_attr.port_num;
+
+	rc = srv_rdma_check_ibv_state(new_state);
+	if (rc)
+	{
+		SPDK_ERRLOG("QP#%d: bad state updated: %u, maybe hardware issue\n", rconn->conn.qid, new_state);
+		/*
+		 * IBV_QPS_UNKNOWN undefined if lib version smaller than libibverbs-1.1.8
+		 * IBV_QPS_UNKNOWN is the enum element after IBV_QPS_ERR
+		 */
+		return IBV_QPS_ERR + 1;
+	}
+
+	if (old_state != new_state)
+	{
+		spdk_trace_record(TRACE_RDMA_QP_STATE_CHANGE, 0, 0, (uintptr_t)rconn, new_state);
+	}
+	return new_state;
+}
+
+static void
+srv_rdma_request_free_data(struct spdk_srv_rdma_request *rdma_req,
+						   struct spdk_srv_rdma_transport *rtransport)
+{
+	struct spdk_srv_rdma_request_data *data_wr;
+	struct ibv_send_wr *next_send_wr;
+	uint64_t req_wrid;
+
+	rdma_req->num_outstanding_data_wr = 0;
+	data_wr = &rdma_req->data;
+	req_wrid = data_wr->wr.wr_id;
+	while (data_wr && data_wr->wr.wr_id == req_wrid)
+	{
+		memset(data_wr->sgl, 0, sizeof(data_wr->wr.sg_list[0]) * data_wr->wr.num_sge);
+		data_wr->wr.num_sge = 0;
+		next_send_wr = data_wr->wr.next;
+		if (data_wr != &rdma_req->data)
+		{
+			spdk_mempool_put(rtransport->data_wr_pool, data_wr);
+		}
+		data_wr = (!next_send_wr || next_send_wr == &rdma_req->rsp.wr) ? NULL : SPDK_CONTAINEROF(next_send_wr, struct spdk_srv_rdma_request_data, wr);
+	}
+}
+
+static void
+srv_rdma_dump_request(struct spdk_srv_rdma_request *req)
+{
+	SPDK_ERRLOG("\t\tRequest Data From Pool: %d\n", req->req.data_from_pool);
+	if (req->req.cmd)
+	{
+		SPDK_ERRLOG("\t\tRequest opcode: %d\n", req->req.cmd->opc);
+	}
+	if (req->recv)
+	{
+		SPDK_ERRLOG("\t\tRequest recv wr_id%lu\n", req->recv->wr.wr_id);
+	}
+}
+
+static void
+srv_rdma_dump_conn_contents(struct spdk_srv_rdma_conn *rconn)
+{
+	int i;
+
+	SPDK_ERRLOG("Dumping contents of queue pair (QID %d)\n", rconn->conn.qid);
+	for (i = 0; i < rconn->max_queue_depth; i++)
+	{
+		if (rconn->resources->reqs[i].state != RDMA_REQUEST_STATE_FREE)
+		{
+			srv_rdma_dump_request(&rconn->resources->reqs[i]);
+		}
+	}
+}
+
+static void
+srv_rdma_resources_destroy(struct spdk_srv_rdma_resources *resources)
+{
+	if (resources->cmds_mr)
+	{
+		ibv_dereg_mr(resources->cmds_mr);
+	}
+
+	if (resources->cpls_mr)
+	{
+		ibv_dereg_mr(resources->cpls_mr);
+	}
+
+	if (resources->bufs_mr)
+	{
+		ibv_dereg_mr(resources->bufs_mr);
+	}
+
+	spdk_free(resources->cmds);
+	spdk_free(resources->cpls);
+	spdk_free(resources->bufs);
+	free(resources->rpc_reqs);
+	free(resources->reqs);
+	free(resources->recvs);
+	free(resources);
+}
+
+static struct spdk_srv_rdma_resources *
+srv_rdma_resources_create(struct spdk_srv_rdma_resource_opts *opts)
+{
+	struct spdk_srv_rdma_resources *resources;
+	struct spdk_srv_rdma_request *rdma_req;
+	struct spdk_srv_rdma_recv *rdma_recv;
+	struct spdk_rdma_qp *qp = NULL;
+	struct spdk_rdma_srq *srq = NULL;
+	struct ibv_recv_wr *bad_wr = NULL;
+	struct spdk_srv_rpc_request *rpc_req;
+	uint32_t i;
+	int rc = 0;
+
+	resources = calloc(1, sizeof(struct spdk_srv_rdma_resources));
+	if (!resources)
+	{
+		SPDK_ERRLOG("Unable to allocate resources for receive queue.\n");
+		return NULL;
+	}
+
+	resources->reqs = calloc(opts->max_queue_depth, sizeof(*resources->reqs));
+	resources->recvs = calloc(opts->max_queue_depth, sizeof(*resources->recvs));
+	resources->cmds = spdk_zmalloc(opts->max_queue_depth * sizeof(*resources->cmds),
+								   0x1000, NULL, SPDK_ENV_LCORE_ID_ANY, SPDK_MALLOC_DMA);
+	resources->cpls = spdk_zmalloc(opts->max_queue_depth * sizeof(*resources->cpls),
+								   0x1000, NULL, SPDK_ENV_LCORE_ID_ANY, SPDK_MALLOC_DMA);
+
+	resources->rpc_reqs = calloc(MAX_RPC_REQ_QUEUE_DEPTH, sizeof(*resources->rpc_reqs));
+
+	if (opts->in_capsule_data_size > 0)
+	{
+		resources->bufs = spdk_zmalloc(opts->max_queue_depth * opts->in_capsule_data_size,
+									   0x1000, NULL, SPDK_ENV_LCORE_ID_ANY,
+									   SPDK_MALLOC_DMA);
+	}
+
+	if (!resources->reqs || !resources->recvs || !resources->cmds ||
+		!resources->cpls || (opts->in_capsule_data_size && !resources->bufs))
+	{
+		SPDK_ERRLOG("Unable to allocate sufficient memory for RDMA queue.\n");
+		goto cleanup;
+	}
+
+	resources->cmds_mr = ibv_reg_mr(opts->pd, resources->cmds,
+									opts->max_queue_depth * sizeof(*resources->cmds),
+									IBV_ACCESS_LOCAL_WRITE);
+	resources->cpls_mr = ibv_reg_mr(opts->pd, resources->cpls,
+									opts->max_queue_depth * sizeof(*resources->cpls),
+									0);
+
+	if (opts->in_capsule_data_size)
+	{
+		resources->bufs_mr = ibv_reg_mr(opts->pd, resources->bufs,
+										opts->max_queue_depth *
+											opts->in_capsule_data_size,
+										IBV_ACCESS_LOCAL_WRITE | IBV_ACCESS_REMOTE_WRITE);
+	}
+
+	if (!resources->cmds_mr || !resources->cpls_mr ||
+		(opts->in_capsule_data_size &&
+		 !resources->bufs_mr))
+	{
+		goto cleanup;
+	}
+	SPDK_DEBUGLOG(rdma, "Command Array: %p Length: %lx LKey: %x\n",
+				  resources->cmds, opts->max_queue_depth * sizeof(*resources->cmds),
+				  resources->cmds_mr->lkey);
+	SPDK_DEBUGLOG(rdma, "Completion Array: %p Length: %lx LKey: %x\n",
+				  resources->cpls, opts->max_queue_depth * sizeof(*resources->cpls),
+				  resources->cpls_mr->lkey);
+	if (resources->bufs && resources->bufs_mr)
+	{
+		SPDK_DEBUGLOG(rdma, "In Capsule Data Array: %p Length: %x LKey: %x\n",
+					  resources->bufs, opts->max_queue_depth * opts->in_capsule_data_size, resources->bufs_mr->lkey);
+	}
+
+	/* Initialize queues */
+	STAILQ_INIT(&resources->incoming_queue);
+	STAILQ_INIT(&resources->free_queue);
+
+	if (opts->shared)
+	{
+		srq = (struct spdk_rdma_srq *)opts->qp;
+	}
+	else
+	{
+		qp = (struct spdk_rdma_qp *)opts->qp;
+	}
+
+	for (i = 0; i < opts->max_queue_depth; i++)
+	{
+		rdma_recv = &resources->recvs[i];
+		rdma_recv->conn = opts->conn;
+
+		/* Set up memory to receive commands */
+		if (resources->bufs)
+		{
+			rdma_recv->buf = (void *)((uintptr_t)resources->bufs + (i *
+																	opts->in_capsule_data_size));
+		}
+
+		rdma_recv->rdma_wr.type = RDMA_WR_TYPE_RECV;
+
+		rdma_recv->sgl[0].addr = (uintptr_t)&resources->cmds[i];
+		rdma_recv->sgl[0].length = sizeof(resources->cmds[i]);
+		rdma_recv->sgl[0].lkey = resources->cmds_mr->lkey;
+		rdma_recv->wr.num_sge = 1;
+
+		if (rdma_recv->buf && resources->bufs_mr)
+		{
+			rdma_recv->sgl[1].addr = (uintptr_t)rdma_recv->buf;
+			rdma_recv->sgl[1].length = opts->in_capsule_data_size;
+			rdma_recv->sgl[1].lkey = resources->bufs_mr->lkey;
+			rdma_recv->wr.num_sge++;
+		}
+
+		rdma_recv->wr.wr_id = (uintptr_t)&rdma_recv->rdma_wr;
+		rdma_recv->wr.sg_list = rdma_recv->sgl;
+		if (srq)
+		{
+			spdk_rdma_srq_queue_recv_wrs(srq, &rdma_recv->wr);
+		}
+		else
+		{
+			spdk_rdma_qp_queue_recv_wrs(qp, &rdma_recv->wr);
+		}
+	}
+
+	for (i = 0; i < opts->max_queue_depth; i++)
+	{
+		rdma_req = &resources->reqs[i];
+
+		if (opts->conn != NULL)
+		{
+			rdma_req->req.conn = &opts->conn->conn;
+		}
+		else
+		{
+			rdma_req->req.conn = NULL;
+		}
+		rdma_req->req.cmd = NULL;
+
+		/* Set up memory to send responses */
+		rdma_req->req.rsp = &resources->cpls[i];
+
+		rdma_req->rsp.sgl[0].addr = (uintptr_t)&resources->cpls[i];
+		rdma_req->rsp.sgl[0].length = sizeof(resources->cpls[i]);
+		rdma_req->rsp.sgl[0].lkey = resources->cpls_mr->lkey;
+
+		rdma_req->rsp.rdma_wr.type = RDMA_WR_TYPE_SEND;
+		rdma_req->rsp.wr.wr_id = (uintptr_t)&rdma_req->rsp.rdma_wr;
+		rdma_req->rsp.wr.next = NULL;
+		rdma_req->rsp.wr.opcode = IBV_WR_SEND;
+		rdma_req->rsp.wr.send_flags = IBV_SEND_SIGNALED;
+		rdma_req->rsp.wr.sg_list = rdma_req->rsp.sgl;
+		rdma_req->rsp.wr.num_sge = SPDK_COUNTOF(rdma_req->rsp.sgl);
+
+		/* Set up memory for data buffers */
+		rdma_req->data.rdma_wr.type = RDMA_WR_TYPE_DATA;
+		rdma_req->data.wr.wr_id = (uintptr_t)&rdma_req->data.rdma_wr;
+		rdma_req->data.wr.next = NULL;
+		rdma_req->data.wr.send_flags = IBV_SEND_SIGNALED;
+		rdma_req->data.wr.sg_list = rdma_req->data.sgl;
+		rdma_req->data.wr.num_sge = SPDK_COUNTOF(rdma_req->data.sgl);
+
+		/* Initialize request state to FREE */
+		rdma_req->state = RDMA_REQUEST_STATE_FREE;
+		STAILQ_INSERT_TAIL(&resources->free_queue, rdma_req, state_link);
+	}
+
+	for (i = 0; i < MAX_RPC_REQ_QUEUE_DEPTH; i++)
+	{
+		rpc_req = &resources->rpc_reqs[i];
+		memset(rpc_req, 0, sizeof(*rpc_req));
+		rpc_req->state = FREE;
+		STAILQ_INIT(&rpc_req->wait_rpc_handle_queue);
+	}
+
+	if (srq)
+	{
+		rc = spdk_rdma_srq_flush_recv_wrs(srq, &bad_wr);
+	}
+	else
+	{
+		rc = spdk_rdma_qp_flush_recv_wrs(qp, &bad_wr);
+	}
+
+	if (rc)
+	{
+		goto cleanup;
+	}
+
+	return resources;
+
+cleanup:
+	srv_rdma_resources_destroy(resources);
+	return NULL;
+}
+
+static void
+srv_rdma_conn_clean_ibv_events(struct spdk_srv_rdma_conn *rconn)
+{
+	struct spdk_srv_rdma_ibv_event_ctx *ctx, *tctx;
+	STAILQ_FOREACH_SAFE(ctx, &rconn->ibv_events, link, tctx)
+	{
+		ctx->rconn = NULL;
+		/* Memory allocated for ctx is freed in srv_rdma_conn_process_ibv_event */
+		STAILQ_REMOVE(&rconn->ibv_events, ctx, spdk_srv_rdma_ibv_event_ctx, link);
+	}
+}
+
+static void
+srv_rdma_conn_destroy(struct spdk_srv_rdma_conn *rconn)
+{
+	struct spdk_srv_rdma_recv *rdma_recv, *recv_tmp;
+	struct ibv_recv_wr *bad_recv_wr = NULL;
+	int rc;
+
+	spdk_trace_record(TRACE_RDMA_QP_DESTROY, 0, 0, (uintptr_t)rconn);
+
+	if (rconn->qd != 0)
+	{
+		struct spdk_srv_conn *conn = &rconn->conn;
+		struct spdk_srv_rdma_transport *rtransport = SPDK_CONTAINEROF(conn->transport,
+																	  struct spdk_srv_rdma_transport, transport);
+		struct spdk_srv_rdma_request *req;
+		uint32_t i, max_req_count = 0;
+
+		SPDK_WARNLOG("Destroying conn when queue depth is %d\n", rconn->qd);
+
+		if (rconn->srq == NULL)
+		{
+			srv_rdma_dump_conn_contents(rconn);
+			max_req_count = rconn->max_queue_depth;
+		}
+		else if (rconn->poller && rconn->resources)
+		{
+			max_req_count = rconn->poller->max_srq_depth;
+		}
+
+		SPDK_DEBUGLOG(rdma, "Release incomplete requests\n");
+		for (i = 0; i < max_req_count; i++)
+		{
+			req = &rconn->resources->reqs[i];
+			if (req->req.conn == conn && req->state != RDMA_REQUEST_STATE_FREE)
+			{
+				/* srv_rdma_request_process checks conn ibv and internal state
+				 * and completes a request */
+				srv_rdma_request_process(rtransport, req);
+			}
+		}
+		assert(rconn->qd == 0);
+	}
+
+	if (rconn->poller)
+	{
+		TAILQ_REMOVE(&rconn->poller->conns, rconn, link);
+
+		if (rconn->srq != NULL && rconn->resources != NULL)
+		{
+			/* Drop all received but unprocessed commands for this queue and return them to SRQ */
+			STAILQ_FOREACH_SAFE(rdma_recv, &rconn->resources->incoming_queue, link, recv_tmp)
+			{
+				if (rconn == rdma_recv->conn)
+				{
+					STAILQ_REMOVE(&rconn->resources->incoming_queue, rdma_recv, spdk_srv_rdma_recv, link);
+					spdk_rdma_srq_queue_recv_wrs(rconn->srq, &rdma_recv->wr);
+					rc = spdk_rdma_srq_flush_recv_wrs(rconn->srq, &bad_recv_wr);
+					if (rc)
+					{
+						SPDK_ERRLOG("Unable to re-post rx descriptor\n");
+					}
+				}
+			}
+		}
+	}
+
+	if (rconn->cm_id)
+	{
+		if (rconn->rdma_qp != NULL)
+		{
+			spdk_rdma_qp_destroy(rconn->rdma_qp);
+			rconn->rdma_qp = NULL;
+		}
+		rdma_destroy_id(rconn->cm_id);
+
+		if (rconn->poller != NULL && rconn->srq == NULL)
+		{
+			rconn->poller->required_num_wr -= MAX_WR_PER_QP(rconn->max_queue_depth);
+		}
+	}
+
+	if (rconn->srq == NULL && rconn->resources != NULL)
+	{
+		srv_rdma_resources_destroy(rconn->resources);
+	}
+
+	srv_rdma_conn_clean_ibv_events(rconn);
+
+	if (rconn->destruct_channel)
+	{
+		spdk_put_io_channel(rconn->destruct_channel);
+		rconn->destruct_channel = NULL;
+	}
+
+	free(rconn);
+}
+
+static int
+srv_rdma_resize_cq(struct spdk_srv_rdma_conn *rconn, struct spdk_srv_rdma_device *device)
+{
+	struct spdk_srv_rdma_poller *rpoller;
+	int rc, num_cqe, required_num_wr;
+
+	/* Enlarge CQ size dynamically */
+	rpoller = rconn->poller;
+	required_num_wr = rpoller->required_num_wr + MAX_WR_PER_QP(rconn->max_queue_depth);
+	num_cqe = rpoller->num_cqe;
+	if (num_cqe < required_num_wr)
+	{
+		num_cqe = spdk_max(num_cqe * 2, required_num_wr);
+		num_cqe = spdk_min(num_cqe, device->attr.max_cqe);
+	}
+
+	if (rpoller->num_cqe != num_cqe)
+	{
+		if (device->context->device->transport_type == IBV_TRANSPORT_IWARP)
+		{
+			SPDK_ERRLOG("iWARP doesn't support CQ resize. Current capacity %u, required %u\n"
+						"Using CQ of insufficient size may lead to CQ overrun\n",
+						rpoller->num_cqe, num_cqe);
+			return -1;
+		}
+		if (required_num_wr > device->attr.max_cqe)
+		{
+			SPDK_ERRLOG("RDMA CQE requirement (%d) exceeds device max_cqe limitation (%d)\n",
+						required_num_wr, device->attr.max_cqe);
+			return -1;
+		}
+
+		SPDK_DEBUGLOG(rdma, "Resize RDMA CQ from %d to %d\n", rpoller->num_cqe, num_cqe);
+		rc = ibv_resize_cq(rpoller->cq, num_cqe);
+		if (rc)
+		{
+			SPDK_ERRLOG("RDMA CQ resize failed: errno %d: %s\n", errno, spdk_strerror(errno));
+			return -1;
+		}
+
+		rpoller->num_cqe = num_cqe;
+	}
+
+	rpoller->required_num_wr = required_num_wr;
+	return 0;
+}
+
+static int
+srv_rdma_conn_initialize(struct spdk_srv_conn *conn)
+{
+	struct spdk_srv_rdma_conn *rconn;
+	struct spdk_srv_rdma_transport *rtransport;
+	struct spdk_srv_transport *transport;
+	struct spdk_srv_rdma_resource_opts opts;
+	struct spdk_srv_rdma_device *device;
+	struct spdk_rdma_qp_init_attr qp_init_attr = {};
+
+	rconn = SPDK_CONTAINEROF(conn, struct spdk_srv_rdma_conn, conn);
+	device = rconn->device;
+
+	qp_init_attr.qp_context = rconn;
+	qp_init_attr.pd = device->pd;
+	qp_init_attr.send_cq = rconn->poller->cq;
+	qp_init_attr.recv_cq = rconn->poller->cq;
+
+	if (rconn->srq)
+	{
+		qp_init_attr.srq = rconn->srq->srq;
+	}
+	else
+	{
+		qp_init_attr.cap.max_recv_wr = rconn->max_queue_depth;
+	}
+
+	/* SEND, READ, and WRITE operations */
+	qp_init_attr.cap.max_send_wr = (uint32_t)rconn->max_queue_depth * 2;
+	qp_init_attr.cap.max_send_sge = spdk_min((uint32_t)device->attr.max_sge, SRV_DEFAULT_TX_SGE);
+	qp_init_attr.cap.max_recv_sge = spdk_min((uint32_t)device->attr.max_sge, SRV_DEFAULT_RX_SGE);
+	qp_init_attr.stats = &rconn->poller->stat.qp_stats;
+
+	if (rconn->srq == NULL && srv_rdma_resize_cq(rconn, device) < 0)
+	{
+		SPDK_ERRLOG("Failed to resize the completion queue. Cannot initialize conn.\n");
+		goto error;
+	}
+
+	rconn->rdma_qp = spdk_rdma_qp_create(rconn->cm_id, &qp_init_attr);
+	if (!rconn->rdma_qp)
+	{
+		goto error;
+	}
+
+	rconn->max_send_depth = spdk_min((uint32_t)(rconn->max_queue_depth * 2),
+									 qp_init_attr.cap.max_send_wr);
+	rconn->max_send_sge = spdk_min(SRV_DEFAULT_TX_SGE, qp_init_attr.cap.max_send_sge);
+	rconn->max_recv_sge = spdk_min(SRV_DEFAULT_RX_SGE, qp_init_attr.cap.max_recv_sge);
+	spdk_trace_record(TRACE_RDMA_QP_CREATE, 0, 0, (uintptr_t)rconn);
+	SPDK_DEBUGLOG(rdma, "New RDMA Connection: %p\n", conn);
+
+	if (rconn->poller->srq == NULL)
+	{
+		rtransport = SPDK_CONTAINEROF(conn->transport, struct spdk_srv_rdma_transport, transport);
+		transport = &rtransport->transport;
+
+		opts.qp = rconn->rdma_qp;
+		opts.pd = rconn->cm_id->pd;
+		opts.conn = rconn;
+		opts.shared = false;
+		opts.max_queue_depth = rconn->max_queue_depth;
+		opts.in_capsule_data_size = transport->opts.in_capsule_data_size;
+
+		rconn->resources = srv_rdma_resources_create(&opts);
+
+		if (!rconn->resources)
+		{
+			SPDK_ERRLOG("Unable to allocate resources for receive queue.\n");
+			rdma_destroy_qp(rconn->cm_id);
+			goto error;
+		}
+	}
+	else
+	{
+		rconn->resources = rconn->poller->resources;
+	}
+
+	rconn->current_recv_depth = 0;
+	STAILQ_INIT(&rconn->pending_rdma_read_queue);
+	STAILQ_INIT(&rconn->pending_rdma_write_queue);
+	STAILQ_INIT(&rconn->pending_complete_queue);
+
+	return 0;
+
+error:
+	rdma_destroy_id(rconn->cm_id);
+	rconn->cm_id = NULL;
+	return -1;
+}
+
+/* Append the given recv wr structure to the resource structs outstanding recvs list. */
+/* This function accepts either a single wr or the first wr in a linked list. */
+static void
+srv_rdma_conn_queue_recv_wrs(struct spdk_srv_rdma_conn *rconn, struct ibv_recv_wr *first)
+{
+	struct spdk_srv_rdma_transport *rtransport = SPDK_CONTAINEROF(rconn->conn.transport,
+																  struct spdk_srv_rdma_transport, transport);
+
+	if (rconn->srq != NULL)
+	{
+		spdk_rdma_srq_queue_recv_wrs(rconn->srq, first);
+	}
+	else
+	{
+		if (spdk_rdma_qp_queue_recv_wrs(rconn->rdma_qp, first))
+		{
+			STAILQ_INSERT_TAIL(&rconn->poller->conns_pending_recv, rconn, recv_link);
+		}
+	}
+
+	if (rtransport->rdma_opts.no_wr_batching)
+	{
+		_poller_submit_recvs(rtransport, rconn->poller);
+	}
+}
+
+static int
+request_transfer_in(struct spdk_srv_request *req)
+{
+	struct spdk_srv_rdma_request *rdma_req;
+	struct spdk_srv_conn *conn;
+	struct spdk_srv_rdma_conn *rconn;
+	struct spdk_srv_rdma_transport *rtransport;
+
+	conn = req->conn;
+	rdma_req = SPDK_CONTAINEROF(req, struct spdk_srv_rdma_request, req);
+	rconn = SPDK_CONTAINEROF(conn, struct spdk_srv_rdma_conn, conn);
+	rtransport = SPDK_CONTAINEROF(rconn->conn.transport,
+								  struct spdk_srv_rdma_transport, transport);
+
+	assert(req->xfer == SPDK_SRV_DATA_HOST_TO_CONTROLLER);
+	assert(rdma_req != NULL);
+
+	if (spdk_rdma_qp_queue_send_wrs(rconn->rdma_qp, &rdma_req->data.wr))
+	{
+		STAILQ_INSERT_TAIL(&rconn->poller->conns_pending_send, rconn, send_link);
+	}
+	if (rtransport->rdma_opts.no_wr_batching)
+	{
+		_poller_submit_sends(rtransport, rconn->poller);
+	}
+
+	rconn->current_read_depth += rdma_req->num_outstanding_data_wr;
+	rconn->current_send_depth += rdma_req->num_outstanding_data_wr;
+	return 0;
+}
+
+static int
+request_transfer_out(struct spdk_srv_request *req, int *data_posted)
+{
+	int num_outstanding_data_wr = 0;
+	struct spdk_srv_rdma_request *rdma_req;
+	struct spdk_srv_conn *conn;
+	struct spdk_srv_rdma_conn *rconn;
+	struct spdk_req_cpl *rsp;
+	struct ibv_send_wr *first = NULL;
+	struct spdk_srv_rdma_transport *rtransport;
+
+	*data_posted = 0;
+	conn = req->conn;
+	rsp = req->rsp;
+	rdma_req = SPDK_CONTAINEROF(req, struct spdk_srv_rdma_request, req);
+	rconn = SPDK_CONTAINEROF(conn, struct spdk_srv_rdma_conn, conn);
+	rtransport = SPDK_CONTAINEROF(rconn->conn.transport,
+								  struct spdk_srv_rdma_transport, transport);
+
+	/* Advance our sq_head pointer */
+	if (conn->sq_head == conn->sq_head_max)
+	{
+		conn->sq_head = 0;
+	}
+	else
+	{
+		conn->sq_head++;
+	}
+	rsp->sqhd = conn->sq_head;
+
+	/* queue the capsule for the recv buffer */
+	assert(rdma_req->recv != NULL);
+
+	srv_rdma_conn_queue_recv_wrs(rconn, &rdma_req->recv->wr);
+
+	rdma_req->recv = NULL;
+	assert(rconn->current_recv_depth > 0);
+	rconn->current_recv_depth--;
+
+	/* Build the response which consists of optional
+	 * RDMA WRITEs to transfer data, plus an RDMA SEND
+	 * containing the response.
+	 */
+	first = &rdma_req->rsp.wr;
+
+	if (rsp->status.sc != SPDK_SRV_SC_SUCCESS)
+	{
+		/* On failure, data was not read from the controller. So clear the
+		 * number of outstanding data WRs to zero.
+		 */
+		rdma_req->num_outstanding_data_wr = 0;
+	}
+	else if (req->xfer == SPDK_SRV_DATA_CONTROLLER_TO_HOST)
+	{
+		first = &rdma_req->data.wr;
+		*data_posted = 1;
+		num_outstanding_data_wr = rdma_req->num_outstanding_data_wr;
+	}
+	if (spdk_rdma_qp_queue_send_wrs(rconn->rdma_qp, first))
+	{
+		STAILQ_INSERT_TAIL(&rconn->poller->conns_pending_send, rconn, send_link);
+	}
+	SPDK_DEBUGLOG(rdma, "no_wr_batching=%d\n", rtransport->rdma_opts.no_wr_batching);
+	if (rtransport->rdma_opts.no_wr_batching)
+	{
+		_poller_submit_sends(rtransport, rconn->poller);
+	}
+
+	/* +1 for the rsp wr */
+	rconn->current_send_depth += num_outstanding_data_wr + 1;
+
+	return 0;
+}
+
+static int
+srv_rdma_event_accept(struct rdma_cm_id *id, struct spdk_srv_rdma_conn *rconn)
+{
+	struct spdk_srv_rdma_accept_private_data accept_data;
+	struct rdma_conn_param ctrlr_event_data = {};
+	int rc;
+
+	accept_data.recfmt = 0;
+	accept_data.crqsize = rconn->max_queue_depth;
+
+	ctrlr_event_data.private_data = &accept_data;
+	ctrlr_event_data.private_data_len = sizeof(accept_data);
+	if (id->ps == RDMA_PS_TCP)
+	{
+		ctrlr_event_data.responder_resources = 0; /* We accept 0 reads from the host */
+		ctrlr_event_data.initiator_depth = rconn->max_read_depth;
+	}
+
+	/* Configure infinite retries for the initiator side conn.
+	 * When using a shared receive queue on the target side,
+	 * we need to pass this value to the initiator to prevent the
+	 * initiator side NIC from completing SEND requests back to the
+	 * initiator with status rnr_retry_count_exceeded. */
+	if (rconn->srq != NULL)
+	{
+		ctrlr_event_data.rnr_retry_count = 0x7;
+	}
+
+	/* When conn is created without use of rdma cm API, an additional
+	 * information must be provided to initiator in the connection response:
+	 * whether conn is using SRQ and its qp_num
+	 * Fields below are ignored by rdma cm if conn has been
+	 * created using rdma cm API. */
+	ctrlr_event_data.srq = rconn->srq ? 1 : 0;
+	ctrlr_event_data.qp_num = rconn->rdma_qp->qp->qp_num;
+
+	rc = spdk_rdma_qp_accept(rconn->rdma_qp, &ctrlr_event_data);
+	if (rc)
+	{
+		SPDK_ERRLOG("Error %d on spdk_rdma_qp_accept\n", errno);
+	}
+	else
+	{
+		SPDK_DEBUGLOG(rdma, "Sent back the accept\n");
+	}
+
+	return rc;
+}
+
+static void
+srv_rdma_event_reject(struct rdma_cm_id *id, enum spdk_srv_rdma_transport_error error)
+{
+	struct spdk_srv_rdma_reject_private_data rej_data;
+
+	rej_data.recfmt = 0;
+	rej_data.sts = error;
+
+	rdma_reject(id, &rej_data, sizeof(rej_data));
+}
+
+static int
+srv_rdma_connect(struct spdk_srv_transport *transport, struct rdma_cm_event *event)
+{
+	struct spdk_srv_rdma_transport *rtransport;
+	struct spdk_srv_rdma_conn *rconn = NULL;
+	struct spdk_srv_rdma_port *port;
+	struct rdma_conn_param *rdma_param = NULL;
+	const struct spdk_srv_rdma_request_private_data *private_data = NULL;
+	uint16_t max_queue_depth;
+	uint16_t max_read_depth;
+
+	rtransport = SPDK_CONTAINEROF(transport, struct spdk_srv_rdma_transport, transport);
+
+	assert(event->id != NULL);		  /* Impossible. Can't even reject the connection. */
+	assert(event->id->verbs != NULL); /* Impossible. No way to handle this. */
+
+	rdma_param = &event->param.conn;
+	if (rdma_param->private_data == NULL ||
+		rdma_param->private_data_len < sizeof(struct spdk_srv_rdma_request_private_data))
+	{
+		SPDK_ERRLOG("connect request: no private data provided\n");
+		srv_rdma_event_reject(event->id, SPDK_SRV_RDMA_ERROR_INVALID_PRIVATE_DATA_LENGTH);
+		return -1;
+	}
+
+	private_data = rdma_param->private_data;
+	if (private_data->recfmt != 0)
+	{
+		SPDK_ERRLOG("Received RDMA private data with RECFMT != 0\n");
+		srv_rdma_event_reject(event->id, SPDK_SRV_RDMA_ERROR_INVALID_RECFMT);
+		return -1;
+	}
+
+	SPDK_DEBUGLOG(rdma, "Connect Recv on fabric intf name %s, dev_name %s\n",
+				  event->id->verbs->device->name, event->id->verbs->device->dev_name);
+
+	port = event->listen_id->context;
+	SPDK_DEBUGLOG(rdma, "Listen Id was %p with verbs %p. ListenAddr: %p\n",
+				  event->listen_id, event->listen_id->verbs, port);
+
+	/* Figure out the supported queue depth. This is a multi-step process
+	 * that takes into account hardware maximums, host provided values,
+	 * and our target's internal memory limits */
+
+	SPDK_DEBUGLOG(rdma, "Calculating Queue Depth\n");
+
+	/* Start with the maximum queue depth allowed by the target */
+	max_queue_depth = rtransport->transport.opts.max_queue_depth;
+	max_read_depth = rtransport->transport.opts.max_queue_depth;
+	SPDK_DEBUGLOG(rdma, "Target Max Queue Depth: %d\n",
+				  rtransport->transport.opts.max_queue_depth);
+
+	/* Next check the local NIC's hardware limitations */
+	SPDK_DEBUGLOG(rdma,
+				  "Local NIC Max Send/Recv Queue Depth: %d Max Read/Write Queue Depth: %d\n",
+				  port->device->attr.max_qp_wr, port->device->attr.max_qp_rd_atom);
+	max_queue_depth = spdk_min(max_queue_depth, port->device->attr.max_qp_wr);
+	max_read_depth = spdk_min(max_read_depth, port->device->attr.max_qp_init_rd_atom);
+
+	/* Next check the remote NIC's hardware limitations */
+	SPDK_DEBUGLOG(rdma,
+				  "Host (Initiator) NIC Max Incoming RDMA R/W operations: %d Max Outgoing RDMA R/W operations: %d\n",
+				  rdma_param->initiator_depth, rdma_param->responder_resources);
+	if (rdma_param->initiator_depth > 0)
+	{
+		max_read_depth = spdk_min(max_read_depth, rdma_param->initiator_depth);
+	}
+
+	/* Finally check for the host software requested values, which are
+	 * optional. */
+	if (rdma_param->private_data != NULL &&
+		rdma_param->private_data_len >= sizeof(struct spdk_srv_rdma_request_private_data))
+	{
+		SPDK_DEBUGLOG(rdma, "Host Receive Queue Size: %d\n", private_data->hrqsize);
+		SPDK_DEBUGLOG(rdma, "Host Send Queue Size: %d\n", private_data->hsqsize);
+		max_queue_depth = spdk_min(max_queue_depth, private_data->hrqsize);
+		max_queue_depth = spdk_min(max_queue_depth, private_data->hsqsize + 1);
+	}
+
+	SPDK_DEBUGLOG(rdma, "Final Negotiated Queue Depth: %d R/W Depth: %d\n",
+				  max_queue_depth, max_read_depth);
+
+	rconn = calloc(1, sizeof(struct spdk_srv_rdma_conn));
+	if (rconn == NULL)
+	{
+		SPDK_ERRLOG("Could not allocate new connection.\n");
+		srv_rdma_event_reject(event->id, SPDK_SRV_RDMA_ERROR_NO_RESOURCES);
+		return -1;
+	}
+
+	rconn->device = port->device;
+	rconn->max_queue_depth = max_queue_depth;
+	rconn->max_read_depth = max_read_depth;
+	rconn->cm_id = event->id;
+	rconn->listen_id = event->listen_id;
+	rconn->conn.transport = transport;
+	STAILQ_INIT(&rconn->ibv_events);
+	/* use qid from the private data to determine the conn type
+	   qid will be set to the appropriate value when the controller is created */
+	rconn->conn.qid = private_data->qid;
+
+	event->id->context = &rconn->conn;
+
+	spdk_srv_tgt_new_conn(transport->tgt, &rconn->conn);
+
+	return 0;
+}
+
+static inline void
+srv_rdma_setup_wr(struct ibv_send_wr *wr, struct ibv_send_wr *next,
+				  enum spdk_srv_data_transfer xfer)
+{
+	if (xfer == SPDK_SRV_DATA_CONTROLLER_TO_HOST)
+	{
+		wr->opcode = IBV_WR_RDMA_WRITE;
+		wr->send_flags = 0;
+		wr->next = next;
+	}
+	else if (xfer == SPDK_SRV_DATA_HOST_TO_CONTROLLER)
+	{
+		wr->opcode = IBV_WR_RDMA_READ;
+		wr->send_flags = IBV_SEND_SIGNALED;
+		wr->next = NULL;
+	}
+	else
+	{
+		assert(0);
+	}
+}
+
+static int
+srv_request_alloc_wrs(struct spdk_srv_rdma_transport *rtransport,
+					  struct spdk_srv_rdma_request *rdma_req,
+					  uint32_t num_sgl_descriptors)
+{
+	struct spdk_srv_rdma_request_data *work_requests[SPDK_SRV_MAX_SGL_ENTRIES];
+	struct spdk_srv_rdma_request_data *current_data_wr;
+	uint32_t i;
+
+	if (num_sgl_descriptors > SPDK_SRV_MAX_SGL_ENTRIES)
+	{
+		SPDK_ERRLOG("Requested too much entries (%u), the limit is %u\n",
+					num_sgl_descriptors, SPDK_SRV_MAX_SGL_ENTRIES);
+		return -EINVAL;
+	}
+
+	if (spdk_mempool_get_bulk(rtransport->data_wr_pool, (void **)work_requests, num_sgl_descriptors))
+	{
+		return -ENOMEM;
+	}
+
+	current_data_wr = &rdma_req->data;
+
+	for (i = 0; i < num_sgl_descriptors; i++)
+	{
+		srv_rdma_setup_wr(&current_data_wr->wr, &work_requests[i]->wr, rdma_req->req.xfer);
+		current_data_wr->wr.next = &work_requests[i]->wr;
+		current_data_wr = work_requests[i];
+		current_data_wr->wr.sg_list = current_data_wr->sgl;
+		current_data_wr->wr.wr_id = rdma_req->data.wr.wr_id;
+	}
+
+	srv_rdma_setup_wr(&current_data_wr->wr, &rdma_req->rsp.wr, rdma_req->req.xfer);
+
+	return 0;
+}
+
+static inline void
+srv_rdma_setup_request(struct spdk_srv_rdma_request *rdma_req)
+{
+	struct ibv_send_wr *wr = &rdma_req->data.wr;
+	struct spdk_req_sgl_descriptor *sgl = &rdma_req->req.cmd->dptr.sgl1;
+
+	wr->wr.rdma.rkey = sgl->keyed.key;
+	wr->wr.rdma.remote_addr = sgl->address;
+	srv_rdma_setup_wr(wr, &rdma_req->rsp.wr, rdma_req->req.xfer);
+}
+
+static inline void
+srv_rdma_update_remote_addr(struct spdk_srv_rdma_request *rdma_req, uint32_t num_wrs)
+{
+	struct ibv_send_wr *wr = &rdma_req->data.wr;
+	struct spdk_req_sgl_descriptor *sgl = &rdma_req->req.cmd->dptr.sgl1;
+	uint32_t i;
+	int j;
+	uint64_t remote_addr_offset = 0;
+
+	for (i = 0; i < num_wrs; ++i)
+	{
+		wr->wr.rdma.rkey = sgl->keyed.key;
+		wr->wr.rdma.remote_addr = sgl->address + remote_addr_offset;
+		for (j = 0; j < wr->num_sge; ++j)
+		{
+			remote_addr_offset += wr->sg_list[j].length;
+		}
+		wr = wr->next;
+	}
+}
+
+static int
+srv_rdma_fill_wr_sgl(struct spdk_srv_rdma_poll_group *rgroup,
+					 struct spdk_srv_rdma_device *device,
+					 struct spdk_srv_rdma_request *rdma_req,
+					 struct ibv_send_wr *wr,
+					 uint32_t total_length,
+					 uint32_t num_extra_wrs)
+{
+	struct spdk_rdma_memory_translation mem_translation;
+	struct spdk_dif_ctx *dif_ctx = NULL;
+	struct ibv_sge *sg_ele;
+	struct iovec *iov;
+	uint32_t remaining_data_block = 0;
+	uint32_t lkey, remaining;
+	int rc;
+
+	wr->num_sge = 0;
+
+	while (total_length && (num_extra_wrs || wr->num_sge < SPDK_SRV_MAX_SGL_ENTRIES))
+	{
+		iov = &rdma_req->req.iov[rdma_req->iovpos];
+		rc = spdk_rdma_get_translation(device->map, iov->iov_base, iov->iov_len, &mem_translation);
+		if (spdk_unlikely(rc))
+		{
+			return false;
+		}
+
+		lkey = spdk_rdma_memory_translation_get_lkey(&mem_translation);
+		sg_ele = &wr->sg_list[wr->num_sge];
+		remaining = spdk_min((uint32_t)iov->iov_len - rdma_req->offset, total_length);
+
+		if (spdk_likely(!dif_ctx))
+		{
+			sg_ele->lkey = lkey;
+			sg_ele->addr = (uintptr_t)iov->iov_base + rdma_req->offset;
+			sg_ele->length = remaining;
+			SPDK_DEBUGLOG(rdma, "sge[%d] %p addr 0x%" PRIx64 ", len %u\n", wr->num_sge, sg_ele, sg_ele->addr,
+						  sg_ele->length);
+			rdma_req->offset += sg_ele->length;
+			total_length -= sg_ele->length;
+			wr->num_sge++;
+
+			if (rdma_req->offset == iov->iov_len)
+			{
+				rdma_req->offset = 0;
+				rdma_req->iovpos++;
+			}
+		}
+		else
+		{
+			uint32_t data_block_size = dif_ctx->block_size - dif_ctx->md_size;
+			uint32_t md_size = dif_ctx->md_size;
+			uint32_t sge_len;
+
+			while (remaining)
+			{
+				if (wr->num_sge >= SPDK_SRV_MAX_SGL_ENTRIES)
+				{
+					if (num_extra_wrs > 0 && wr->next)
+					{
+						wr = wr->next;
+						wr->num_sge = 0;
+						sg_ele = &wr->sg_list[wr->num_sge];
+						num_extra_wrs--;
+					}
+					else
+					{
+						break;
+					}
+				}
+				sg_ele->lkey = lkey;
+				sg_ele->addr = (uintptr_t)((char *)iov->iov_base + rdma_req->offset);
+				sge_len = spdk_min(remaining, remaining_data_block);
+				sg_ele->length = sge_len;
+				SPDK_DEBUGLOG(rdma, "sge[%d] %p addr 0x%" PRIx64 ", len %u\n", wr->num_sge, sg_ele, sg_ele->addr,
+							  sg_ele->length);
+				remaining -= sge_len;
+				remaining_data_block -= sge_len;
+				rdma_req->offset += sge_len;
+				total_length -= sge_len;
+
+				sg_ele++;
+				wr->num_sge++;
+
+				if (remaining_data_block == 0)
+				{
+					/* skip metadata */
+					rdma_req->offset += md_size;
+					total_length -= md_size;
+					/* Metadata that do not fit this IO buffer will be included in the next IO buffer */
+					remaining -= spdk_min(remaining, md_size);
+					remaining_data_block = data_block_size;
+				}
+
+				if (remaining == 0)
+				{
+					/* By subtracting the size of the last IOV from the offset, we ensure that we skip
+					   the remaining metadata bits at the beginning of the next buffer */
+					rdma_req->offset -= spdk_min(iov->iov_len, rdma_req->offset);
+					rdma_req->iovpos++;
+				}
+			}
+		}
+	}
+
+	if (total_length)
+	{
+		SPDK_ERRLOG("Not enough SG entries to hold data buffer\n");
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static inline uint32_t
+srv_rdma_calc_num_wrs(uint32_t length, uint32_t io_unit_size, uint32_t block_size)
+{
+	/* estimate the number of SG entries and WRs needed to process the request */
+	uint32_t num_sge = 0;
+	uint32_t i;
+	uint32_t num_buffers = SPDK_CEIL_DIV(length, io_unit_size);
+
+	for (i = 0; i < num_buffers && length > 0; i++)
+	{
+		uint32_t buffer_len = spdk_min(length, io_unit_size);
+		uint32_t num_sge_in_block = SPDK_CEIL_DIV(buffer_len, block_size);
+
+		if (num_sge_in_block * block_size > buffer_len)
+		{
+			++num_sge_in_block;
+		}
+		num_sge += num_sge_in_block;
+		length -= buffer_len;
+	}
+	return SPDK_CEIL_DIV(num_sge, SPDK_SRV_MAX_SGL_ENTRIES);
+}
+
+static int
+srv_rdma_request_fill_iovs(struct spdk_srv_rdma_transport *rtransport,
+						   struct spdk_srv_rdma_device *device,
+						   struct spdk_srv_rdma_request *rdma_req,
+						   uint32_t length)
+{
+	struct spdk_srv_rdma_conn *rconn;
+	struct spdk_srv_rdma_poll_group *rgroup;
+	struct spdk_srv_request *req = &rdma_req->req;
+	struct ibv_send_wr *wr = &rdma_req->data.wr;
+	int rc;
+	uint32_t num_wrs = 1;
+
+	rconn = SPDK_CONTAINEROF(req->conn, struct spdk_srv_rdma_conn, conn);
+	rgroup = rconn->poller->group;
+
+	/* rdma wr specifics */
+	srv_rdma_setup_request(rdma_req);
+
+	rc = spdk_srv_request_get_buffers(req, &rgroup->group, &rtransport->transport,
+									  length);
+	if (rc != 0)
+	{
+		return rc;
+	}
+
+	assert(req->iovcnt <= rconn->max_send_sge);
+
+	rdma_req->iovpos = 0;
+
+	rc = srv_rdma_fill_wr_sgl(rgroup, device, rdma_req, wr, length, num_wrs - 1);
+	if (spdk_unlikely(rc != 0))
+	{
+		goto err_exit;
+	}
+
+	if (spdk_unlikely(num_wrs > 1))
+	{
+		srv_rdma_update_remote_addr(rdma_req, num_wrs);
+	}
+
+	/* set the number of outstanding data WRs for this request. */
+	rdma_req->num_outstanding_data_wr = num_wrs;
+
+	return rc;
+
+err_exit:
+	spdk_srv_request_free_buffers(req, &rgroup->group, &rtransport->transport);
+	srv_rdma_request_free_data(rdma_req, rtransport);
+	req->iovcnt = 0;
+	return rc;
+}
+
+static int
+srv_rdma_request_fill_iovs_multi_sgl(struct spdk_srv_rdma_transport *rtransport,
+									 struct spdk_srv_rdma_device *device,
+									 struct spdk_srv_rdma_request *rdma_req)
+{
+	struct spdk_srv_rdma_conn *rconn;
+	struct spdk_srv_rdma_poll_group *rgroup;
+	struct ibv_send_wr *current_wr;
+	struct spdk_srv_request *req = &rdma_req->req;
+	struct spdk_req_sgl_descriptor *inline_segment, *desc;
+	uint32_t num_sgl_descriptors;
+	uint32_t lengths[SPDK_SRV_MAX_SGL_ENTRIES], total_length = 0;
+	uint32_t i;
+	int rc;
+
+	rconn = SPDK_CONTAINEROF(rdma_req->req.conn, struct spdk_srv_rdma_conn, conn);
+	rgroup = rconn->poller->group;
+
+	inline_segment = &req->cmd->dptr.sgl1;
+	assert(inline_segment->generic.type == SPDK_SRV_SGL_TYPE_LAST_SEGMENT);
+	assert(inline_segment->unkeyed.subtype == SPDK_SRV_SGL_SUBTYPE_OFFSET);
+
+	num_sgl_descriptors = inline_segment->unkeyed.length / sizeof(struct spdk_req_sgl_descriptor);
+	assert(num_sgl_descriptors <= SPDK_SRV_MAX_SGL_ENTRIES);
+
+	desc = (struct spdk_req_sgl_descriptor *)rdma_req->recv->buf + inline_segment->address;
+	for (i = 0; i < num_sgl_descriptors; i++)
+	{
+		if (spdk_likely(!req->dif_enabled))
+		{
+			lengths[i] = desc->keyed.length;
+		}
+
+		total_length += lengths[i];
+		desc++;
+	}
+
+	if (total_length > rtransport->transport.opts.max_io_size)
+	{
+		SPDK_ERRLOG("Multi SGL length 0x%x exceeds max io size 0x%x\n",
+					total_length, rtransport->transport.opts.max_io_size);
+		req->rsp->status.sc = SPDK_SRV_SC_DATA_SGL_LENGTH_INVALID;
+		return -EINVAL;
+	}
+
+	if (srv_request_alloc_wrs(rtransport, rdma_req, num_sgl_descriptors - 1) != 0)
+	{
+		return -ENOMEM;
+	}
+
+	rc = spdk_srv_request_get_buffers(req, &rgroup->group, &rtransport->transport, total_length);
+	if (rc != 0)
+	{
+		srv_rdma_request_free_data(rdma_req, rtransport);
+		return rc;
+	}
+
+	/* The first WR must always be the embedded data WR. This is how we unwind them later. */
+	current_wr = &rdma_req->data.wr;
+	assert(current_wr != NULL);
+
+	req->length = 0;
+	rdma_req->iovpos = 0;
+	desc = (struct spdk_req_sgl_descriptor *)rdma_req->recv->buf + inline_segment->address;
+	for (i = 0; i < num_sgl_descriptors; i++)
+	{
+		/* The descriptors must be keyed data block descriptors with an address, not an offset. */
+		if (spdk_unlikely(desc->generic.type != SPDK_SRV_SGL_TYPE_KEYED_DATA_BLOCK ||
+						  desc->keyed.subtype != SPDK_SRV_SGL_SUBTYPE_ADDRESS))
+		{
+			rc = -EINVAL;
+			goto err_exit;
+		}
+
+		rc = srv_rdma_fill_wr_sgl(rgroup, device, rdma_req, current_wr, lengths[i], 0);
+		if (rc != 0)
+		{
+			rc = -ENOMEM;
+			goto err_exit;
+		}
+
+		req->length += desc->keyed.length;
+		current_wr->wr.rdma.rkey = desc->keyed.key;
+		current_wr->wr.rdma.remote_addr = desc->address;
+		current_wr = current_wr->next;
+		desc++;
+	}
+
+#ifdef SPDK_CONFIG_RDMA_SEND_WITH_INVAL
+	/* Go back to the last descriptor in the list. */
+	desc--;
+	if ((device->attr.device_cap_flags & IBV_DEVICE_MEM_MGT_EXTENSIONS) != 0)
+	{
+		if (desc->keyed.subtype == SPDK_SRV_SGL_SUBTYPE_INVALIDATE_KEY)
+		{
+			rdma_req->rsp.wr.opcode = IBV_WR_SEND_WITH_INV;
+			rdma_req->rsp.wr.imm_data = desc->keyed.key;
+		}
+	}
+#endif
+
+	rdma_req->num_outstanding_data_wr = num_sgl_descriptors;
+
+	return 0;
+
+err_exit:
+	spdk_srv_request_free_buffers(req, &rgroup->group, &rtransport->transport);
+	srv_rdma_request_free_data(rdma_req, rtransport);
+	return rc;
+}
+
+static int
+srv_rdma_request_parse_sgl(struct spdk_srv_rdma_transport *rtransport,
+						   struct spdk_srv_rdma_device *device,
+						   struct spdk_srv_rdma_request *rdma_req)
+{
+	struct spdk_srv_request *req = &rdma_req->req;
+	struct spdk_req_cpl *rsp;
+	struct spdk_req_sgl_descriptor *sgl;
+	int rc;
+	uint32_t length;
+
+	rsp = req->rsp;
+	sgl = &req->cmd->dptr.sgl1;
+
+	if (sgl->generic.type == SPDK_SRV_SGL_TYPE_KEYED_DATA_BLOCK &&
+		(sgl->keyed.subtype == SPDK_SRV_SGL_SUBTYPE_ADDRESS ||
+		 sgl->keyed.subtype == SPDK_SRV_SGL_SUBTYPE_INVALIDATE_KEY))
+	{
+
+		length = sgl->keyed.length;
+		if (length > rtransport->transport.opts.max_io_size)
+		{
+			SPDK_ERRLOG("SGL length 0x%x exceeds max io size 0x%x\n",
+						length, rtransport->transport.opts.max_io_size);
+			rsp->status.sc = SPDK_SRV_SC_DATA_SGL_LENGTH_INVALID;
+			return -1;
+		}
+#ifdef SPDK_CONFIG_RDMA_SEND_WITH_INVAL
+		if ((device->attr.device_cap_flags & IBV_DEVICE_MEM_MGT_EXTENSIONS) != 0)
+		{
+			if (sgl->keyed.subtype == SPDK_SRV_SGL_SUBTYPE_INVALIDATE_KEY)
+			{
+				rdma_req->rsp.wr.opcode = IBV_WR_SEND_WITH_INV;
+				rdma_req->rsp.wr.imm_data = sgl->keyed.key;
+			}
+		}
+#endif
+
+		/* fill request length and populate iovs */
+		req->length = length;
+
+		rc = srv_rdma_request_fill_iovs(rtransport, device, rdma_req, length);
+		if (spdk_unlikely(rc < 0))
+		{
+			if (rc == -EINVAL)
+			{
+				SPDK_ERRLOG("SGL length exceeds the max I/O size\n");
+				rsp->status.sc = SPDK_SRV_SC_DATA_SGL_LENGTH_INVALID;
+				return -1;
+			}
+			/* No available buffers. Queue this request up. */
+			SPDK_DEBUGLOG(rdma, "No available large data buffers. Queueing request %p\n", rdma_req);
+			return 0;
+		}
+
+		/* backward compatible */
+		req->data = req->iov[0].iov_base;
+
+		SPDK_DEBUGLOG(rdma, "Request %p took %d buffer/s from central pool\n", rdma_req,
+					  req->iovcnt);
+
+		return 0;
+	}
+	else if (sgl->generic.type == SPDK_SRV_SGL_TYPE_DATA_BLOCK &&
+			 sgl->unkeyed.subtype == SPDK_SRV_SGL_SUBTYPE_OFFSET)
+	{
+		uint64_t offset = sgl->address;
+		uint32_t max_len = rtransport->transport.opts.in_capsule_data_size;
+
+		SPDK_DEBUGLOG(rdma, "In-capsule data: offset 0x%" PRIx64 ", length 0x%x\n",
+					  offset, sgl->unkeyed.length);
+
+		if (offset > max_len)
+		{
+			SPDK_ERRLOG("In-capsule offset 0x%" PRIx64 " exceeds capsule length 0x%x\n",
+						offset, max_len);
+			rsp->status.sc = SPDK_SRV_SC_INVALID_SGL_OFFSET;
+			return -1;
+		}
+		max_len -= (uint32_t)offset;
+
+		if (sgl->unkeyed.length > max_len)
+		{
+			SPDK_ERRLOG("In-capsule data length 0x%x exceeds capsule length 0x%x\n",
+						sgl->unkeyed.length, max_len);
+			rsp->status.sc = SPDK_SRV_SC_DATA_SGL_LENGTH_INVALID;
+			return -1;
+		}
+
+		rdma_req->num_outstanding_data_wr = 0;
+		req->data = rdma_req->recv->buf + offset;
+		req->data_from_pool = false;
+		req->length = sgl->unkeyed.length;
+
+		req->iov[0].iov_base = req->data;
+		req->iov[0].iov_len = req->length;
+		req->iovcnt = 1;
+		SPDK_DEBUGLOG(rdma, "In-capsule data: iov_base %p, iov_length %p\n",
+					  req->iov[0].iov_base, req->iov[0].iov_len);
+
+		return 0;
+	}
+	else if (sgl->generic.type == SPDK_SRV_SGL_TYPE_LAST_SEGMENT &&
+			 sgl->unkeyed.subtype == SPDK_SRV_SGL_SUBTYPE_OFFSET)
+	{
+
+		rc = srv_rdma_request_fill_iovs_multi_sgl(rtransport, device, rdma_req);
+		if (rc == -ENOMEM)
+		{
+			SPDK_DEBUGLOG(rdma, "No available large data buffers. Queueing request %p\n", rdma_req);
+			return 0;
+		}
+		else if (rc == -EINVAL)
+		{
+			SPDK_ERRLOG("Multi SGL element request length exceeds the max I/O size\n");
+			rsp->status.sc = SPDK_SRV_SC_DATA_SGL_LENGTH_INVALID;
+			return -1;
+		}
+
+		/* backward compatible */
+		req->data = req->iov[0].iov_base;
+
+		SPDK_DEBUGLOG(rdma, "Request %p took %d buffer/s from central pool\n", rdma_req,
+					  req->iovcnt);
+
+		return 0;
+	}
+
+	SPDK_ERRLOG("Invalid Srv I/O Command SGL:  Type 0x%x, Subtype 0x%x\n",
+				sgl->generic.type, sgl->generic.subtype);
+	rsp->status.sc = SPDK_SRV_SC_SGL_DESCRIPTOR_TYPE_INVALID;
+	return -1;
+}
+
+static void
+_srv_rdma_request_free(struct spdk_srv_rdma_request *rdma_req,
+					   struct spdk_srv_rdma_transport *rtransport)
+{
+	struct spdk_srv_rdma_conn *rconn;
+	struct spdk_srv_rdma_poll_group *rgroup;
+
+	rconn = SPDK_CONTAINEROF(rdma_req->req.conn, struct spdk_srv_rdma_conn, conn);
+	if (rdma_req->req.data_from_pool)
+	{
+		rgroup = rconn->poller->group;
+
+		spdk_srv_request_free_buffers(&rdma_req->req, &rgroup->group, &rtransport->transport);
+	}
+	srv_rdma_request_free_data(rdma_req, rtransport);
+	rdma_req->req.length = 0;
+	rdma_req->req.iovcnt = 0;
+	rdma_req->req.data = NULL;
+	rdma_req->rsp.wr.next = NULL;
+	rdma_req->data.wr.next = NULL;
+	rdma_req->offset = 0;
+	rconn->qd--;
+
+	STAILQ_INSERT_HEAD(&rconn->resources->free_queue, rdma_req, state_link);
+	rdma_req->state = RDMA_REQUEST_STATE_FREE;
+}
+
+static void srv_rpc_write_request_exec(struct spdk_srv_rdma_request *rdma_req)
+{
+	uint32_t rpc_index;
+	uint32_t data_length;
+	uint32_t rpc_opc;
+	uint32_t submit_type;
+	uint32_t lba_start, subrequest_id;
+	int iov_offset;
+	struct spdk_srv_rdma_conn *rconn;
+	struct spdk_srv_rpc_request *rpc_req;
+	uint8_t md5sum[SPDK_MD5DIGEST_LEN];
+	rpc_index = rdma_req->req.cmd->rsvd2;
+	data_length = rdma_req->req.cmd->rsvd3;
+	rpc_opc = rdma_req->req.cmd->rpc_opc;
+	submit_type = rdma_req->req.cmd->cdw13;
+	uint32_t check_md5 = rdma_req->req.cmd->cdw14;
+	rconn = SPDK_CONTAINEROF(rdma_req->req.conn, struct spdk_srv_rdma_conn, conn);
+	rpc_req = &rconn->resources->rpc_reqs[rpc_index];
+	uint32_t io_unit_size = rconn->conn.transport->opts.io_unit_size;
+	uint32_t max_io_size = rconn->conn.transport->opts.max_io_size;
+	struct spdk_md5ctx md5ctx;
+	int out_data_offset = 0;
+	int out_remain_len = 0;
+	int copy_len = 0;
+	int md5_batch_len = 0;
+	int md5_total_len = 0;
+	struct iovec *iovec;
+	assert(rdma_req->recv != NULL);
+	SPDK_DEBUGLOG(rdma, "start srv_rpc_write_request_exec rdma_req:%p, rpc_req:%p, rpc_index:%d, state:%d opc:%d rpc_opc:%d\n", (uintptr_t)rdma_req, (uintptr_t)rpc_req, rpc_index, rpc_req->state, rdma_req->req.cmd->opc, rdma_req->req.cmd->rpc_opc);
+
+	if (rpc_req->state == FREE)
+	{
+		rpc_req->in_real_length = data_length;
+		rpc_req->in_iov_cnt_total = SPDK_CEIL_DIV(data_length, io_unit_size);
+		rpc_req->in_iov_cnt_left = rpc_req->in_iov_cnt_total;
+		rpc_req->rpc_index = rpc_index;
+		rpc_req->rconn = rconn;
+		rpc_req->rpc_opc = rpc_opc;
+		rpc_req->submit_type = submit_type;
+		SPDK_DEBUGLOG(rdma, "in_iov_cnt_total :%d\n", rpc_req->in_iov_cnt_total);
+		if (rpc_req->in_iov_cnt_total > SPDK_SRV_MAX_SGL_ENTRIES)
+		{
+			// 说明rpc会被分裂为多个子请求，这里需要分配一个iovec *的数组,用于存储全部子请求的iovec的指针
+			// 需在在上层应用处理完rpc请求，调用传递的callback函数时，释放
+			rpc_req->in_iovs = calloc(rpc_req->in_iov_cnt_total, sizeof(struct iovec));
+			if (rpc_req->in_iovs == NULL)
+			{
+				exit(-1);
+			}
+			// 检查一下LBA起始的数值，按理说应该是256的整数倍,因为sector_size 是512字节，一次最大传输单元是128K
+			lba_start = rdma_req->req.cmd->cdw10;
+			subrequest_id = lba_start / (max_io_size / 512);
+			iov_offset = subrequest_id * SPDK_SRV_MAX_SGL_ENTRIES;
+			for (int i = 0; i < rdma_req->req.iovcnt; i++)
+			{
+				rpc_req->in_iovs[iov_offset + i].iov_base = rdma_req->req.iov[i].iov_base;
+				rpc_req->in_iovs[iov_offset + i].iov_len = rdma_req->req.iov[i].iov_len;
+				rpc_req->in_iov_cnt_left--;
+			}
+			rpc_req->state = WAIT_OTHER_SUBREQUEST;
+		}
+		else
+		{
+			// 没有子请求，不必再分数组了，直接指向rdma_req->req里面的定长数组
+			rpc_req->in_iovs = (struct iovec **)rdma_req->req.iov;
+			rpc_req->in_iov_cnt_left = 0;
+			rpc_req->state = PROCESS_DATA;
+			SPDK_DEBUGLOG(rdma, " 2222222 srv_rpc_write_request_exec state:%d addr:%ld len:%d\n", rpc_req->state, rdma_req->req.iov[0], rdma_req->req.iovcnt);
+		}
+		// rdma_req请求挂到spdk_srv_rpc_request里的队列上
+		STAILQ_INSERT_TAIL(&rpc_req->wait_rpc_handle_queue, rdma_req, state_link);
+	}
+	else if (rpc_req->state == WAIT_OTHER_SUBREQUEST)
+	{
+		lba_start = rdma_req->req.cmd->cdw10;
+		subrequest_id = lba_start / (max_io_size / 512);
+		iov_offset = subrequest_id * SPDK_SRV_MAX_SGL_ENTRIES;
+		for (int i = 0; i < rdma_req->req.iovcnt; i++)
+		{
+			rpc_req->in_iovs[iov_offset + i].iov_base = rdma_req->req.iov[i].iov_base;
+			rpc_req->in_iovs[iov_offset + i].iov_len = rdma_req->req.iov[i].iov_len;
+			rpc_req->in_iov_cnt_left--;
+		}
+		if (rpc_req->in_iov_cnt_left == 0)
+		{
+			rpc_req->state = PROCESS_DATA;
+		}
+		// rdma_req请求挂到spdk_srv_rpc_request里的队列上
+		STAILQ_INSERT_TAIL(&rpc_req->wait_rpc_handle_queue, rdma_req, state_link);
+	}
+	else
+	{
+		SPDK_ERRLOG("srv_rpc_write_request_exec assert error request:%p, state:%d, rpc_index\n", rdma_req, rpc_req->state, rpc_req->rpc_index);
+		exit(-1);
+	}
+	SPDK_DEBUGLOG(rdma, "11111111 srv_rpc_write_request_exec rpc_req:%p, index:%d, state:%d\n", (uintptr_t)rpc_req, rpc_req->rpc_index, rpc_req->state);
+	if (rpc_req->state == PROCESS_DATA)
+	{
+		assert(g_rpc_dispatcher != NULL);
+		assert(rdma_req->recv != NULL);
+		// check md5sum
+		if (check_md5 == 1)
+		{
+			rpc_req->check_md5 = true;
+			SPDK_DEBUGLOG(rdma, "check md5sum start iov_cnt:%d, real_length:%d\n", rpc_req->in_iov_cnt_total, rpc_req->in_real_length);
+			md5_total_len = rpc_req->in_real_length;
+			md5_batch_len = 0;
+			md5init(&md5ctx);
+			for (int i = 0; i < rpc_req->in_iov_cnt_total; i++)
+			{
+				md5_batch_len = spdk_min(rpc_req->in_iovs[i].iov_len, md5_total_len);
+				md5update(&md5ctx, rpc_req->in_iovs[i].iov_base, md5_batch_len);
+				md5_total_len -= md5_batch_len;
+				SPDK_DEBUGLOG(rdma, "checking md5sum iov_len:%d, md5_total_len:%d\n", rpc_req->in_iovs[i].iov_len, md5_total_len);
+			}
+			assert(md5_total_len == 0);
+			md5final(md5sum, &md5ctx);
+			for (int i = 0; i < SPDK_MD5DIGEST_LEN; i++)
+			{
+				assert(md5sum[i] == rdma_req->req.cmd->md5sum[i]);
+				SPDK_DEBUGLOG(rdma, "check md5sum compare caled:%d, receved:%d\n", md5sum[i], rdma_req->req.cmd->md5sum[i]);
+			}
+			SPDK_DEBUGLOG(rdma, "check md5sum success\n");
+		}
+		if (rpc_req->submit_type == SPDK_CLIENT_SUBMIT_CONTING)
+		{
+			(*(spdk_srv_rpc_dispatcher)g_rpc_dispatcher[rpc_req->submit_type])(rpc_req->rpc_opc, rpc_req->in_iovs, rpc_req->in_iov_cnt_total, rpc_req->in_real_length, spdk_srv_rpc_request_handle_complete_cb, rpc_req);
+		}
+		else if (rpc_req->submit_type == SPDK_CLIENT_SUBMIT_IOVES)
+		{
+			(*(spdk_srv_rpc_dispatcher_iovs)g_rpc_dispatcher[rpc_req->submit_type])(rpc_req->rpc_opc, rpc_req->in_iovs, rpc_req->in_iov_cnt_total, rpc_req->in_real_length, spdk_srv_rpc_request_handle_complete_iovs_cb, rpc_req);
+		}
+	}
+	return;
+}
+
+static void srv_rpc_read_request_exec(struct spdk_srv_rdma_request *rdma_req)
+{
+	uint32_t rpc_index;
+	uint32_t data_length;
+	uint32_t lba_start, subrequest_id;
+	struct spdk_srv_rdma_conn *rconn;
+	struct spdk_srv_rpc_request *rpc_req;
+	rpc_index = rdma_req->req.cmd->rsvd2;
+	data_length = rdma_req->req.cmd->rsvd3;
+	rconn = SPDK_CONTAINEROF(rdma_req->req.conn, struct spdk_srv_rdma_conn, conn);
+	rpc_req = &rconn->resources->rpc_reqs[rpc_index];
+	uint32_t io_unit_size = rconn->conn.transport->opts.io_unit_size;
+	uint32_t max_io_size = rconn->conn.transport->opts.max_io_size;
+	struct spdk_req_cpl *rsp;
+	int out_data_offset = 0;
+	int out_remain_len = 0;
+	int copy_len = 0;
+
+	// 理论上只需要第一次拷贝数据之前需要seek到某个pos和offset，seeked用于在循环中标识是否执行过seek操作
+	bool seeked = false;
+	int seeked_offset = 0;
+	int last_seeked_offset = 0;
+
+	int iovpos_dst = 0;
+	int iovpos_src = 0;
+	int offset_dst = 0;
+	int offset_src = 0;
+	int iov_remain_length_dst = 0;
+	int iov_remain_length_src = 0;
+	struct iovec *iov_src;
+	struct iovec *iov_dst;
+
+	assert(rdma_req->recv != NULL);
+	SPDK_DEBUGLOG(rdma, "start srv_rpc_read_request_exec rdma_req:%p, rpc_req:%p, rpc_index:%d, state:%d opc:%d rpc_opc:%d\n", (uintptr_t)rdma_req, (uintptr_t)rpc_req, rpc_index, rpc_req->state, rdma_req->req.cmd->opc, rdma_req->req.cmd->rpc_opc);
+
+	if (rpc_req->state == PENDING_READ)
+	{
+		lba_start = rdma_req->req.cmd->cdw10;
+		out_data_offset = lba_start * 512;
+		out_remain_len = rpc_req->out_real_length - out_data_offset;
+		out_remain_len = spdk_min(out_remain_len, max_io_size);
+		iovpos_dst = 0;
+		SPDK_DEBUGLOG(rdma, "lba_start:%d out_data_offset:%d out_remain_len:%d\n", lba_start, out_data_offset, out_remain_len);
+		while (out_remain_len > 0)
+		{
+			if (rpc_req->submit_type == SPDK_CLIENT_SUBMIT_CONTING)
+			{
+				iov_dst = &rdma_req->req.iov[iovpos_dst];
+				copy_len = spdk_min(iov_dst->iov_len, out_remain_len);
+				memcpy(iov_dst->iov_base, rpc_req->out_data + out_data_offset, copy_len);
+				out_data_offset += copy_len;
+				out_remain_len -= copy_len;
+				iovpos_dst++;
+			}
+			else if (rpc_req->submit_type == SPDK_CLIENT_SUBMIT_IOVES)
+			{
+				if (!seeked)
+				{
+					for (int i = 0; i < rpc_req->out_iov_cnt; i++)
+					{
+						last_seeked_offset = seeked_offset;
+						seeked_offset += rpc_req->out_iovs[i].iov_len;
+						if (seeked_offset > out_data_offset)
+						{
+							offset_src = out_data_offset - last_seeked_offset;
+							break;
+						}
+						else if (seeked_offset == out_data_offset)
+						{
+							iovpos_src++;
+							offset_src = 0;
+							break;
+						}
+						else
+						{
+							iovpos_src++;
+						}
+					}
+					seeked = true;
+				}
+				iov_dst = &rdma_req->req.iov[iovpos_dst];
+
+				iov_remain_length_dst = iov_dst->iov_len - offset_dst;
+				while (iov_remain_length_dst > 0)
+				{
+					iov_src = &rpc_req->out_iovs[iovpos_src];
+					iov_remain_length_src = iov_src->iov_len - offset_src;
+					copy_len = spdk_min(iov_remain_length_dst, iov_remain_length_src);
+					memcpy(iov_dst->iov_base + offset_dst, iov_src->iov_base + offset_src, copy_len);
+					offset_dst = offset_dst + copy_len;
+					offset_src = offset_src + copy_len;
+					iov_remain_length_dst = iov_remain_length_dst - copy_len;
+					iov_remain_length_src = iov_remain_length_src - copy_len;
+					out_remain_len = out_remain_len - copy_len;
+					if (iov_remain_length_dst == 0)
+					{
+						iovpos_dst++;
+						offset_dst = 0;
+					}
+					else if (iov_remain_length_src == 0)
+					{
+						iovpos_src++;
+						offset_src = 0;
+					}
+					else
+					{
+						assert(iov_remain_length_dst == 0 || iov_remain_length_src == 0);
+						SPDK_ERRLOG("srv_rpc_read_request_exec HIT CRITIAL ERROR\n");
+					}
+				}
+			}
+			else
+			{
+				assert(rpc_req->submit_type < SPDK_CLIENT_SUBMIT_TYPES_TOTAL);
+				SPDK_ERRLOG("not supported submit_type %d\n", rpc_req->submit_type);
+			}
+		};
+		assert(out_remain_len == 0);
+		SPDK_DEBUGLOG(rdma, "iovpos_dst:%d req.iovcnt:%d\n", iovpos_dst, rdma_req->req.iovcnt);
+		assert(iovpos_dst == rdma_req->req.iovcnt);
+
+		// 如果需要计算md5，提前把md5值放到rsp里面的md5sum字段里面
+		//  提前处理rsp的一些字段
+		rsp = rdma_req->req.rsp;
+		rsp->cdw0 = rpc_req->out_status;
+		if (rpc_req->check_md5)
+		{
+			memcpy(rsp->md5sum, rpc_req->md5sum, SPDK_MD5DIGEST_LEN);
+		}
+	}
+	else
+	{
+		SPDK_ERRLOG("srv_rpc_read_request_exec assert error request:%p, state:%d, rpc_index:%d\n", rdma_req, rpc_req->state, rpc_req->rpc_index);
+		assert(-1);
+	}
+	rpc_req->out_rdma_send_left--;
+	if (rpc_req->out_rdma_send_left == 0)
+	{
+		rpc_req->state = FINISH;
+		(*rpc_req->service_cb)(rpc_req->service_cb_arg, 0);
+		srv_rpc_request_free(rpc_req);
+	}
+	SPDK_DEBUGLOG(rdma, "11111111 srv_rpc_read_request_exec rpc_req:%p, state:%d\n", (uintptr_t)rpc_req, rpc_req->state);
+	return;
+}
+
+void srv_rpc_request_free(struct spdk_srv_rpc_request *rpc_req)
+{
+	memset(rpc_req, 0, offsetof(struct spdk_srv_rpc_request, wait_rpc_handle_queue));
+	rpc_req->state = FREE;
+}
+
+bool srv_rdma_request_process(struct spdk_srv_rdma_transport *rtransport,
+							  struct spdk_srv_rdma_request *rdma_req)
+{
+	struct spdk_srv_rdma_conn *rconn;
+	struct spdk_srv_rdma_device *device;
+	struct spdk_srv_rdma_poll_group *rgroup;
+	struct spdk_req_cpl *rsp = rdma_req->req.rsp;
+	int rc;
+	struct spdk_srv_rdma_recv *rdma_recv;
+	enum spdk_srv_rdma_request_state prev_state;
+	bool progress = false;
+	int data_posted;
+	uint32_t num_blocks;
+	struct spdk_srv_rpc_request *rpc_req;
+	uint32_t rpc_index;
+
+	rconn = SPDK_CONTAINEROF(rdma_req->req.conn, struct spdk_srv_rdma_conn, conn);
+	device = rconn->device;
+	rgroup = rconn->poller->group;
+
+	assert(rdma_req->state != RDMA_REQUEST_STATE_FREE);
+
+	/* If the queue pair is in an error state, force the request to the completed state
+	 * to release resources. */
+	if (rconn->ibv_state == IBV_QPS_ERR || rconn->conn.state != SPDK_SRV_CONN_ACTIVE)
+	{
+		if (rdma_req->state == RDMA_REQUEST_STATE_NEED_BUFFER)
+		{
+			STAILQ_REMOVE(&rgroup->group.pending_buf_queue, &rdma_req->req, spdk_srv_request, buf_link);
+		}
+		else if (rdma_req->state == RDMA_REQUEST_STATE_DATA_TRANSFER_TO_CONTROLLER_PENDING)
+		{
+			STAILQ_REMOVE(&rconn->pending_rdma_read_queue, rdma_req, spdk_srv_rdma_request, state_link);
+		}
+		else if (rdma_req->state == RDMA_REQUEST_STATE_DATA_TRANSFER_TO_HOST_PENDING)
+		{
+			STAILQ_REMOVE(&rconn->pending_rdma_write_queue, rdma_req, spdk_srv_rdma_request, state_link);
+		}
+		rdma_req->state = RDMA_REQUEST_STATE_COMPLETED;
+	}
+
+	/* The loop here is to allow for several back-to-back state changes. */
+	do
+	{
+		prev_state = rdma_req->state;
+
+		SPDK_DEBUGLOG(rdma, "Request %p entering state %d\n", rdma_req, prev_state);
+
+		switch (rdma_req->state)
+		{
+		case RDMA_REQUEST_STATE_FREE:
+			/* Some external code must kick a request into RDMA_REQUEST_STATE_NEW
+			 * to escape this state. */
+			break;
+		case RDMA_REQUEST_STATE_NEW:
+			spdk_trace_record(TRACE_RDMA_REQUEST_STATE_NEW, 0, 0,
+							  (uintptr_t)rdma_req, (uintptr_t)rconn);
+			rdma_recv = rdma_req->recv;
+
+			/* The first element of the SGL is the Srv command */
+			rdma_req->req.cmd = (struct spdk_req_cmd *)rdma_recv->sgl[0].addr;
+			memset(rdma_req->req.rsp, 0, sizeof(*rdma_req->req.rsp));
+
+			if (rconn->ibv_state == IBV_QPS_ERR || rconn->conn.state != SPDK_SRV_CONN_ACTIVE)
+			{
+				rdma_req->state = RDMA_REQUEST_STATE_COMPLETED;
+				break;
+			}
+			SPDK_DEBUGLOG(rdma, "enter RDMA_REQUEST_STATE_NEW\n");
+
+#ifdef SPDK_CONFIG_RDMA_SEND_WITH_INVAL
+			rdma_req->rsp.wr.opcode = IBV_WR_SEND;
+			rdma_req->rsp.wr.imm_data = 0;
+#endif
+
+			/* The next state transition depends on the data transfer needs of this request. */
+			rdma_req->req.xfer = spdk_srv_req_get_xfer(&rdma_req->req);
+
+			/* If no data to transfer, ready to execute. */
+			if (rdma_req->req.xfer == SPDK_SRV_DATA_NONE)
+			{
+				rdma_req->state = RDMA_REQUEST_STATE_READY_TO_EXECUTE;
+				break;
+			}
+
+			rdma_req->state = RDMA_REQUEST_STATE_NEED_BUFFER;
+			STAILQ_INSERT_TAIL(&rgroup->group.pending_buf_queue, &rdma_req->req, buf_link);
+			break;
+		case RDMA_REQUEST_STATE_NEED_BUFFER:
+			SPDK_DEBUGLOG(rdma, "enter RDMA_REQUEST_STATE_NEED_BUFFER\n");
+			spdk_trace_record(TRACE_RDMA_REQUEST_STATE_NEED_BUFFER, 0, 0,
+							  (uintptr_t)rdma_req, (uintptr_t)rconn);
+
+			assert(rdma_req->req.xfer != SPDK_SRV_DATA_NONE);
+
+			if (&rdma_req->req != STAILQ_FIRST(&rgroup->group.pending_buf_queue))
+			{
+				/* This request needs to wait in line to obtain a buffer */
+				break;
+			}
+
+			/* Try to get a data buffer */
+			rc = srv_rdma_request_parse_sgl(rtransport, device, rdma_req);
+			if (rc < 0)
+			{
+				STAILQ_REMOVE_HEAD(&rgroup->group.pending_buf_queue, buf_link);
+				rdma_req->state = RDMA_REQUEST_STATE_READY_TO_COMPLETE;
+				break;
+			}
+
+			if (!rdma_req->req.data)
+			{
+				/* No buffers available. */
+				rgroup->stat.pending_data_buffer++;
+				break;
+			}
+
+			STAILQ_REMOVE_HEAD(&rgroup->group.pending_buf_queue, buf_link);
+
+			/* If data is transferring from host to controller and the data didn't
+			 * arrive using in capsule data, we need to do a transfer from the host.
+			 */
+			if (rdma_req->req.xfer == SPDK_SRV_DATA_HOST_TO_CONTROLLER &&
+				rdma_req->req.data_from_pool)
+			{
+				STAILQ_INSERT_TAIL(&rconn->pending_rdma_read_queue, rdma_req, state_link);
+				rdma_req->state = RDMA_REQUEST_STATE_DATA_TRANSFER_TO_CONTROLLER_PENDING;
+				break;
+			}
+
+			rdma_req->state = RDMA_REQUEST_STATE_READY_TO_EXECUTE;
+			break;
+		case RDMA_REQUEST_STATE_DATA_TRANSFER_TO_CONTROLLER_PENDING:
+			SPDK_DEBUGLOG(rdma, "enter RDMA_REQUEST_STATE_DATA_TRANSFER_TO_CONTROLLER_PENDING\n");
+			spdk_trace_record(TRACE_RDMA_REQUEST_STATE_DATA_TRANSFER_TO_CONTROLLER_PENDING, 0, 0,
+							  (uintptr_t)rdma_req, (uintptr_t)rconn);
+
+			if (rdma_req != STAILQ_FIRST(&rconn->pending_rdma_read_queue))
+			{
+				/* This request needs to wait in line to perform RDMA */
+				break;
+			}
+			if (rconn->current_send_depth + rdma_req->num_outstanding_data_wr > rconn->max_send_depth || rconn->current_read_depth + rdma_req->num_outstanding_data_wr > rconn->max_read_depth)
+			{
+				/* We can only have so many WRs outstanding. we have to wait until some finish. */
+				rconn->poller->stat.pending_rdma_read++;
+				break;
+			}
+
+			/* We have already verified that this request is the head of the queue. */
+			STAILQ_REMOVE_HEAD(&rconn->pending_rdma_read_queue, state_link);
+
+			rc = request_transfer_in(&rdma_req->req);
+			if (!rc)
+			{
+				rdma_req->state = RDMA_REQUEST_STATE_TRANSFERRING_HOST_TO_CONTROLLER;
+			}
+			else
+			{
+				rsp->status.sc = SPDK_SRV_SC_INTERNAL_DEVICE_ERROR;
+				rdma_req->state = RDMA_REQUEST_STATE_READY_TO_COMPLETE;
+			}
+			break;
+		case RDMA_REQUEST_STATE_TRANSFERRING_HOST_TO_CONTROLLER:
+			SPDK_DEBUGLOG(rdma, "enter RDMA_REQUEST_STATE_TRANSFERRING_HOST_TO_CONTROLLER\n");
+			spdk_trace_record(TRACE_RDMA_REQUEST_STATE_TRANSFERRING_HOST_TO_CONTROLLER, 0, 0,
+							  (uintptr_t)rdma_req, (uintptr_t)rconn);
+			/* Some external code must kick a request into RDMA_REQUEST_STATE_READY_TO_EXECUTE
+			 * to escape this state. */
+			break;
+		case RDMA_REQUEST_STATE_READY_TO_EXECUTE:
+			spdk_trace_record(TRACE_RDMA_REQUEST_STATE_READY_TO_EXECUTE, 0, 0,
+							  (uintptr_t)rdma_req, (uintptr_t)rconn);
+
+			SPDK_DEBUGLOG(rdma, "enter RDMA_REQUEST_STATE_READY_TO_EXECUTE\n");
+			rdma_req->state = RDMA_REQUEST_STATE_EXECUTING;
+			if (rdma_req->req.cmd->opc == SPDK_CLIENT_OPC_RPC_WRITE)
+			{
+				srv_rpc_write_request_exec(rdma_req);
+			}
+			else if (rdma_req->req.cmd->opc == SPDK_CLIENT_OPC_RPC_READ)
+			{
+				srv_rpc_read_request_exec(rdma_req);
+			}
+			else
+			{
+				spdk_srv_request_exec(&rdma_req->req);
+			}
+			break;
+		case RDMA_REQUEST_STATE_EXECUTING:
+			spdk_trace_record(TRACE_RDMA_REQUEST_STATE_EXECUTING, 0, 0,
+							  (uintptr_t)rdma_req, (uintptr_t)rconn);
+			/* Some external code must kick a request into RDMA_REQUEST_STATE_EXECUTED
+			 * to escape this state. */
+			SPDK_DEBUGLOG(rdma, "enter RDMA_REQUEST_STATE_EXECUTING\n");
+
+			if (rdma_req->req.cmd->opc == SPDK_CLIENT_OPC_RPC_READ)
+			{
+				spdk_srv_request_exec(&rdma_req->req);
+			}
+			break;
+		case RDMA_REQUEST_STATE_EXECUTED:
+			SPDK_DEBUGLOG(rdma, "enter RDMA_REQUEST_STATE_EXECUTED\n");
+			spdk_trace_record(TRACE_RDMA_REQUEST_STATE_EXECUTED, 0, 0,
+							  (uintptr_t)rdma_req, (uintptr_t)rconn);
+
+			if (rdma_req->req.cmd->opc == SPDK_CLIENT_OPC_RPC_WRITE)
+			{
+				if (rdma_req != STAILQ_FIRST(&rconn->pending_complete_queue))
+				{
+					/* This request needs to wait in line to perform RDMA */
+					break;
+				}
+				STAILQ_REMOVE_HEAD(&rconn->pending_complete_queue, state_link);
+			}
+
+			if (rsp->status.sc == SPDK_SRV_SC_SUCCESS &&
+				rdma_req->req.xfer == SPDK_SRV_DATA_CONTROLLER_TO_HOST)
+			{
+				STAILQ_INSERT_TAIL(&rconn->pending_rdma_write_queue, rdma_req, state_link);
+				rdma_req->state = RDMA_REQUEST_STATE_DATA_TRANSFER_TO_HOST_PENDING;
+			}
+			else
+			{
+				rdma_req->state = RDMA_REQUEST_STATE_READY_TO_COMPLETE;
+			}
+
+			break;
+		case RDMA_REQUEST_STATE_DATA_TRANSFER_TO_HOST_PENDING:
+			SPDK_DEBUGLOG(rdma, "enter RDMA_REQUEST_STATE_DATA_TRANSFER_TO_HOST_PENDING\n");
+			spdk_trace_record(TRACE_RDMA_REQUEST_STATE_DATA_TRANSFER_TO_HOST_PENDING, 0, 0,
+							  (uintptr_t)rdma_req, (uintptr_t)rconn);
+
+			if (rdma_req != STAILQ_FIRST(&rconn->pending_rdma_write_queue))
+			{
+				/* This request needs to wait in line to perform RDMA */
+				break;
+			}
+			if ((rconn->current_send_depth + rdma_req->num_outstanding_data_wr + 1) >
+				rconn->max_send_depth)
+			{
+				/* We can only have so many WRs outstanding. we have to wait until some finish.
+				 * +1 since each request has an additional wr in the resp. */
+				rconn->poller->stat.pending_rdma_write++;
+				break;
+			}
+
+			/* We have already verified that this request is the head of the queue. */
+			STAILQ_REMOVE_HEAD(&rconn->pending_rdma_write_queue, state_link);
+
+			/* The data transfer will be kicked off from
+			 * RDMA_REQUEST_STATE_READY_TO_COMPLETE state.
+			 */
+			rdma_req->state = RDMA_REQUEST_STATE_READY_TO_COMPLETE;
+			break;
+		case RDMA_REQUEST_STATE_READY_TO_COMPLETE:
+			SPDK_DEBUGLOG(rdma, "enter RDMA_REQUEST_STATE_READY_TO_COMPLETE\n");
+			spdk_trace_record(TRACE_RDMA_REQUEST_STATE_READY_TO_COMPLETE, 0, 0,
+							  (uintptr_t)rdma_req, (uintptr_t)rconn);
+			rc = request_transfer_out(&rdma_req->req, &data_posted);
+			assert(rc == 0); /* No good way to handle this currently */
+			if (rc)
+			{
+				rdma_req->state = RDMA_REQUEST_STATE_COMPLETED;
+			}
+			else
+			{
+				rdma_req->state = data_posted ? RDMA_REQUEST_STATE_TRANSFERRING_CONTROLLER_TO_HOST : RDMA_REQUEST_STATE_COMPLETING;
+			}
+			break;
+		case RDMA_REQUEST_STATE_TRANSFERRING_CONTROLLER_TO_HOST:
+			spdk_trace_record(TRACE_RDMA_REQUEST_STATE_TRANSFERRING_CONTROLLER_TO_HOST, 0, 0,
+							  (uintptr_t)rdma_req, (uintptr_t)rconn);
+			/* Some external code must kick a request into RDMA_REQUEST_STATE_COMPLETED
+			 * to escape this state. */
+			break;
+		case RDMA_REQUEST_STATE_COMPLETING:
+			SPDK_DEBUGLOG(rdma, "enter RDMA_REQUEST_STATE_COMPLETING\n");
+			spdk_trace_record(TRACE_RDMA_REQUEST_STATE_COMPLETING, 0, 0,
+							  (uintptr_t)rdma_req, (uintptr_t)rconn);
+			/* Some external code must kick a request into RDMA_REQUEST_STATE_COMPLETED
+			 * to escape this state. */
+			break;
+		case RDMA_REQUEST_STATE_COMPLETED:
+			SPDK_DEBUGLOG(rdma, "enter RDMA_REQUEST_STATE_COMPLETED\n");
+			spdk_trace_record(TRACE_RDMA_REQUEST_STATE_COMPLETED, 0, 0,
+							  (uintptr_t)rdma_req, (uintptr_t)rconn);
+
+			rconn->poller->stat.request_latency += spdk_get_ticks() - rdma_req->receive_tsc;
+			_srv_rdma_request_free(rdma_req, rtransport);
+			break;
+		case RDMA_REQUEST_NUM_STATES:
+		default:
+			assert(0);
+			break;
+		}
+
+		if (rdma_req->state != prev_state)
+		{
+			progress = true;
+		}
+	} while (rdma_req->state != prev_state);
+
+	return progress;
+}
+
+/* Public API callbacks begin here */
+
+#define SPDK_SRV_RDMA_DEFAULT_MAX_QUEUE_DEPTH 4096
+#define SPDK_SRV_RDMA_DEFAULT_AQ_DEPTH 4096
+#define SPDK_SRV_RDMA_DEFAULT_SRQ_DEPTH 4096
+#define SPDK_SRV_RDMA_DEFAULT_MAX_CONNS_PER_TGT 65535
+#define SPDK_SRV_RDMA_DEFAULT_IN_CAPSULE_DATA_SIZE 8192
+#define SPDK_SRV_RDMA_DEFAULT_MAX_IO_SIZE 131072
+#define SPDK_SRV_RDMA_MIN_IO_BUFFER_SIZE (SPDK_SRV_RDMA_DEFAULT_MAX_IO_SIZE / SPDK_SRV_MAX_SGL_ENTRIES)
+#define SPDK_SRV_RDMA_DEFAULT_NUM_SHARED_BUFFERS 4095
+#define SPDK_SRV_RDMA_DEFAULT_BUFFER_CACHE_SIZE 32
+#define SPDK_SRV_RDMA_DEFAULT_NO_SRQ false
+#define SPDK_SRV_RDMA_DIF_INSERT_OR_STRIP false
+#define SPDK_SRV_RDMA_ACCEPTOR_BACKLOG 100
+#define SPDK_SRV_RDMA_DEFAULT_ABORT_TIMEOUT_SEC 1
+#define SPDK_SRV_RDMA_DEFAULT_NO_WR_BATCHING true // try set this
+
+static void
+srv_rdma_opts_init(struct spdk_srv_transport_opts *opts)
+{
+	opts->max_queue_depth = SPDK_SRV_RDMA_DEFAULT_MAX_QUEUE_DEPTH;
+	opts->max_conns_per_tgt = SPDK_SRV_RDMA_DEFAULT_MAX_CONNS_PER_TGT;
+	opts->in_capsule_data_size = SPDK_SRV_RDMA_DEFAULT_IN_CAPSULE_DATA_SIZE;
+	opts->max_io_size = SPDK_SRV_RDMA_DEFAULT_MAX_IO_SIZE;
+	opts->io_unit_size = SPDK_SRV_RDMA_MIN_IO_BUFFER_SIZE;
+	opts->max_aq_depth = SPDK_SRV_RDMA_DEFAULT_AQ_DEPTH;
+	opts->num_shared_buffers = SPDK_SRV_RDMA_DEFAULT_NUM_SHARED_BUFFERS;
+	opts->buf_cache_size = SPDK_SRV_RDMA_DEFAULT_BUFFER_CACHE_SIZE;
+	opts->dif_insert_or_strip = SPDK_SRV_RDMA_DIF_INSERT_OR_STRIP;
+	opts->abort_timeout_sec = SPDK_SRV_RDMA_DEFAULT_ABORT_TIMEOUT_SEC;
+	opts->transport_specific = NULL;
+}
+
+static int srv_rdma_destroy(struct spdk_srv_transport *transport,
+							spdk_srv_transport_destroy_done_cb cb_fn, void *cb_arg);
+
+static inline bool
+srv_rdma_is_rxe_device(struct spdk_srv_rdma_device *device)
+{
+	return device->attr.vendor_id == SPDK_RDMA_RXE_VENDOR_ID_OLD ||
+		   device->attr.vendor_id == SPDK_RDMA_RXE_VENDOR_ID_NEW;
+}
+
+static int
+srv_rdma_accept(void *ctx);
+
+static struct spdk_srv_transport *
+srv_rdma_create(struct spdk_srv_transport_opts *opts)
+{
+	int rc;
+	struct spdk_srv_rdma_transport *rtransport;
+	struct spdk_srv_rdma_device *device, *tmp;
+	struct ibv_context **contexts;
+	uint32_t i;
+	int flag;
+	uint32_t sge_count;
+	uint32_t min_shared_buffers;
+	uint32_t min_in_capsule_data_size;
+	int max_device_sge = SPDK_SRV_MAX_SGL_ENTRIES;
+	pthread_mutexattr_t attr;
+
+	rtransport = calloc(1, sizeof(*rtransport));
+	if (!rtransport)
+	{
+		return NULL;
+	}
+
+	if (pthread_mutexattr_init(&attr))
+	{
+		SPDK_ERRLOG("pthread_mutexattr_init() failed\n");
+		free(rtransport);
+		return NULL;
+	}
+
+	if (pthread_mutexattr_settype(&attr, PTHREAD_MUTEX_RECURSIVE))
+	{
+		SPDK_ERRLOG("pthread_mutexattr_settype() failed\n");
+		pthread_mutexattr_destroy(&attr);
+		free(rtransport);
+		return NULL;
+	}
+
+	if (pthread_mutex_init(&rtransport->lock, &attr))
+	{
+		SPDK_ERRLOG("pthread_mutex_init() failed\n");
+		pthread_mutexattr_destroy(&attr);
+		free(rtransport);
+		return NULL;
+	}
+
+	pthread_mutexattr_destroy(&attr);
+
+	TAILQ_INIT(&rtransport->devices);
+	TAILQ_INIT(&rtransport->ports);
+	TAILQ_INIT(&rtransport->poll_groups);
+
+	rtransport->transport.ops = &spdk_srv_transport_rdma;
+	rtransport->rdma_opts.num_cqe = DEFAULT_SRV_RDMA_CQ_SIZE;
+	rtransport->rdma_opts.max_srq_depth = SPDK_SRV_RDMA_DEFAULT_SRQ_DEPTH;
+	rtransport->rdma_opts.no_srq = SPDK_SRV_RDMA_DEFAULT_NO_SRQ;
+	rtransport->rdma_opts.acceptor_backlog = SPDK_SRV_RDMA_ACCEPTOR_BACKLOG;
+	rtransport->rdma_opts.no_wr_batching = SPDK_SRV_RDMA_DEFAULT_NO_WR_BATCHING;
+	if (opts->transport_specific != NULL &&
+		spdk_json_decode_object_relaxed(opts->transport_specific, rdma_transport_opts_decoder,
+										SPDK_COUNTOF(rdma_transport_opts_decoder),
+										&rtransport->rdma_opts))
+	{
+		SPDK_ERRLOG("spdk_json_decode_object_relaxed failed\n");
+		srv_rdma_destroy(&rtransport->transport, NULL, NULL);
+		return NULL;
+	}
+
+	SPDK_INFOLOG(rdma, "*** RDMA Transport Init ***\n"
+					   "  Transport opts:  max_ioq_depth=%d, max_io_size=%d,\n"
+					   "  max_io_conns_per_ctrlr=%d, io_unit_size=%d,\n"
+					   "  in_capsule_data_size=%d, max_aq_depth=%d,\n"
+					   "  num_shared_buffers=%d, num_cqe=%d, max_srq_depth=%d, no_srq=%d,"
+					   "  acceptor_backlog=%d, no_wr_batching=%d abort_timeout_sec=%d\n",
+				 opts->max_queue_depth,
+				 opts->max_io_size,
+				 opts->max_conns_per_tgt - 1,
+				 opts->io_unit_size,
+				 opts->in_capsule_data_size,
+				 opts->max_aq_depth,
+				 opts->num_shared_buffers,
+				 rtransport->rdma_opts.num_cqe,
+				 rtransport->rdma_opts.max_srq_depth,
+				 rtransport->rdma_opts.no_srq,
+				 rtransport->rdma_opts.acceptor_backlog,
+				 rtransport->rdma_opts.no_wr_batching,
+				 opts->abort_timeout_sec);
+
+	/* I/O unit size cannot be larger than max I/O size */
+	if (opts->io_unit_size > opts->max_io_size)
+	{
+		opts->io_unit_size = opts->max_io_size;
+	}
+
+	if (rtransport->rdma_opts.acceptor_backlog <= 0)
+	{
+		SPDK_ERRLOG("The acceptor backlog cannot be less than 1, setting to the default value of (%d).\n",
+					SPDK_SRV_RDMA_ACCEPTOR_BACKLOG);
+		rtransport->rdma_opts.acceptor_backlog = SPDK_SRV_RDMA_ACCEPTOR_BACKLOG;
+	}
+
+	if (opts->num_shared_buffers < (SPDK_SRV_MAX_SGL_ENTRIES * 2))
+	{
+		SPDK_ERRLOG("The number of shared data buffers (%d) is less than"
+					"the minimum number required to guarantee that forward progress can be made (%d)\n",
+					opts->num_shared_buffers, (SPDK_SRV_MAX_SGL_ENTRIES * 2));
+		srv_rdma_destroy(&rtransport->transport, NULL, NULL);
+		return NULL;
+	}
+
+	min_shared_buffers = spdk_env_get_core_count() * opts->buf_cache_size;
+	if (min_shared_buffers > opts->num_shared_buffers)
+	{
+		SPDK_ERRLOG("There are not enough buffers to satisfy"
+					"per-poll group caches for each thread. (%" PRIu32 ")"
+					"supplied. (%" PRIu32 ") required\n",
+					opts->num_shared_buffers, min_shared_buffers);
+		SPDK_ERRLOG("Please specify a larger number of shared buffers\n");
+		srv_rdma_destroy(&rtransport->transport, NULL, NULL);
+		return NULL;
+	}
+
+	sge_count = opts->max_io_size / opts->io_unit_size;
+	if (sge_count > SRV_DEFAULT_TX_SGE)
+	{
+		SPDK_ERRLOG("Unsupported IO Unit size specified, %d bytes\n", opts->io_unit_size);
+		srv_rdma_destroy(&rtransport->transport, NULL, NULL);
+		return NULL;
+	}
+
+	min_in_capsule_data_size = sizeof(struct spdk_req_sgl_descriptor) * SPDK_SRV_MAX_SGL_ENTRIES;
+	if (opts->in_capsule_data_size < min_in_capsule_data_size)
+	{
+		SPDK_WARNLOG("In capsule data size is set to %u, this is minimum size required to support msdbd=16\n",
+					 min_in_capsule_data_size);
+		opts->in_capsule_data_size = min_in_capsule_data_size;
+	}
+
+	rtransport->event_channel = rdma_create_event_channel();
+	if (rtransport->event_channel == NULL)
+	{
+		SPDK_ERRLOG("rdma_create_event_channel() failed, %s\n", spdk_strerror(errno));
+		srv_rdma_destroy(&rtransport->transport, NULL, NULL);
+		return NULL;
+	}
+
+	flag = fcntl(rtransport->event_channel->fd, F_GETFL);
+	if (fcntl(rtransport->event_channel->fd, F_SETFL, flag | O_NONBLOCK) < 0)
+	{
+		SPDK_ERRLOG("fcntl can't set nonblocking mode for socket, fd: %d (%s)\n",
+					rtransport->event_channel->fd, spdk_strerror(errno));
+		srv_rdma_destroy(&rtransport->transport, NULL, NULL);
+		return NULL;
+	}
+
+	rtransport->data_wr_pool = spdk_mempool_create("spdk_srv_rdma_wr_data",
+												   opts->max_queue_depth * SPDK_SRV_MAX_SGL_ENTRIES,
+												   sizeof(struct spdk_srv_rdma_request_data),
+												   SPDK_MEMPOOL_DEFAULT_CACHE_SIZE,
+												   SPDK_ENV_SOCKET_ID_ANY);
+	if (!rtransport->data_wr_pool)
+	{
+		SPDK_ERRLOG("Unable to allocate work request pool for poll group\n");
+		srv_rdma_destroy(&rtransport->transport, NULL, NULL);
+		return NULL;
+	}
+
+	contexts = rdma_get_devices(NULL);
+	if (contexts == NULL)
+	{
+		SPDK_ERRLOG("rdma_get_devices() failed: %s (%d)\n", spdk_strerror(errno), errno);
+		srv_rdma_destroy(&rtransport->transport, NULL, NULL);
+		return NULL;
+	}
+
+	i = 0;
+	rc = 0;
+	while (contexts[i] != NULL)
+	{
+		device = calloc(1, sizeof(*device));
+		if (!device)
+		{
+			SPDK_ERRLOG("Unable to allocate memory for RDMA devices.\n");
+			rc = -ENOMEM;
+			break;
+		}
+		device->context = contexts[i];
+		rc = ibv_query_device(device->context, &device->attr);
+		if (rc < 0)
+		{
+			SPDK_ERRLOG("Failed to query RDMA device attributes.\n");
+			free(device);
+			break;
+		}
+
+		max_device_sge = spdk_min(max_device_sge, device->attr.max_sge);
+
+#ifdef SPDK_CONFIG_RDMA_SEND_WITH_INVAL
+		if ((device->attr.device_cap_flags & IBV_DEVICE_MEM_MGT_EXTENSIONS) == 0)
+		{
+			SPDK_WARNLOG("The libibverbs on this system supports SEND_WITH_INVALIDATE,");
+			SPDK_WARNLOG("but the device with vendor ID %u does not.\n", device->attr.vendor_id);
+		}
+
+		/**
+		 * The vendor ID is assigned by the IEEE and an ID of 0 implies Soft-RoCE.
+		 * The Soft-RoCE RXE driver does not currently support send with invalidate,
+		 * but incorrectly reports that it does. There are changes making their way
+		 * through the kernel now that will enable this feature. When they are merged,
+		 * we can conditionally enable this feature.
+		 *
+		 * TODO: enable this for versions of the kernel rxe driver that support it.
+		 */
+		if (srv_rdma_is_rxe_device(device))
+		{
+			device->attr.device_cap_flags &= ~(IBV_DEVICE_MEM_MGT_EXTENSIONS);
+		}
+#endif
+
+		/* set up device context async ev fd as NON_BLOCKING */
+		flag = fcntl(device->context->async_fd, F_GETFL);
+		rc = fcntl(device->context->async_fd, F_SETFL, flag | O_NONBLOCK);
+		if (rc < 0)
+		{
+			SPDK_ERRLOG("Failed to set context async fd to NONBLOCK.\n");
+			free(device);
+			break;
+		}
+
+		TAILQ_INSERT_TAIL(&rtransport->devices, device, link);
+		i++;
+
+		device->pd = ibv_alloc_pd(device->context);
+
+		if (!device->pd)
+		{
+			SPDK_ERRLOG("Unable to allocate protection domain.\n");
+			rc = -ENOMEM;
+			break;
+		}
+
+		assert(device->map == NULL);
+
+		device->map = spdk_rdma_create_mem_map(device->pd, 0, SPDK_RDMA_MEMORY_MAP_ROLE_TARGET);
+		if (!device->map)
+		{
+			SPDK_ERRLOG("Unable to allocate memory map for listen address\n");
+			rc = -ENOMEM;
+			break;
+		}
+
+		assert(device->map != NULL);
+		assert(device->pd != NULL);
+	}
+	rdma_free_devices(contexts);
+
+	if (opts->io_unit_size * max_device_sge < opts->max_io_size)
+	{
+		/* divide and round up. */
+		opts->io_unit_size = (opts->max_io_size + max_device_sge - 1) / max_device_sge;
+
+		/* round up to the nearest 4k. */
+		opts->io_unit_size = (opts->io_unit_size + SRV_DATA_BUFFER_ALIGNMENT - 1) & ~SRV_DATA_BUFFER_MASK;
+
+		opts->io_unit_size = spdk_max(opts->io_unit_size, SPDK_SRV_RDMA_MIN_IO_BUFFER_SIZE);
+		SPDK_NOTICELOG("Adjusting the io unit size to fit the device's maximum I/O size. New I/O unit size %u\n",
+					   opts->io_unit_size);
+	}
+
+	if (rc < 0)
+	{
+		srv_rdma_destroy(&rtransport->transport, NULL, NULL);
+		return NULL;
+	}
+
+	/* Set up poll descriptor array to monitor events from RDMA and IB
+	 * in a single poll syscall
+	 */
+	rtransport->npoll_fds = i + 1;
+	i = 0;
+	rtransport->poll_fds = calloc(rtransport->npoll_fds, sizeof(struct pollfd));
+	if (rtransport->poll_fds == NULL)
+	{
+		SPDK_ERRLOG("poll_fds allocation failed\n");
+		srv_rdma_destroy(&rtransport->transport, NULL, NULL);
+		return NULL;
+	}
+
+	rtransport->poll_fds[i].fd = rtransport->event_channel->fd;
+	rtransport->poll_fds[i++].events = POLLIN;
+
+	TAILQ_FOREACH_SAFE(device, &rtransport->devices, link, tmp)
+	{
+		rtransport->poll_fds[i].fd = device->context->async_fd;
+		rtransport->poll_fds[i++].events = POLLIN;
+	}
+
+	rtransport->accept_poller = SPDK_POLLER_REGISTER(srv_rdma_accept, &rtransport->transport,
+													 opts->acceptor_poll_rate);
+	if (!rtransport->accept_poller)
+	{
+		srv_rdma_destroy(&rtransport->transport, NULL, NULL);
+		return NULL;
+	}
+
+	return &rtransport->transport;
+}
+
+static void
+srv_rdma_dump_opts(struct spdk_srv_transport *transport, struct spdk_json_write_ctx *w)
+{
+	struct spdk_srv_rdma_transport *rtransport;
+	assert(w != NULL);
+
+	rtransport = SPDK_CONTAINEROF(transport, struct spdk_srv_rdma_transport, transport);
+	spdk_json_write_named_uint32(w, "max_srq_depth", rtransport->rdma_opts.max_srq_depth);
+	spdk_json_write_named_bool(w, "no_srq", rtransport->rdma_opts.no_srq);
+	if (rtransport->rdma_opts.no_srq == true)
+	{
+		spdk_json_write_named_int32(w, "num_cqe", rtransport->rdma_opts.num_cqe);
+	}
+	spdk_json_write_named_int32(w, "acceptor_backlog", rtransport->rdma_opts.acceptor_backlog);
+	spdk_json_write_named_bool(w, "no_wr_batching", rtransport->rdma_opts.no_wr_batching);
+}
+
+static int
+srv_rdma_destroy(struct spdk_srv_transport *transport,
+				 spdk_srv_transport_destroy_done_cb cb_fn, void *cb_arg)
+{
+	struct spdk_srv_rdma_transport *rtransport;
+	struct spdk_srv_rdma_port *port, *port_tmp;
+	struct spdk_srv_rdma_device *device, *device_tmp;
+
+	rtransport = SPDK_CONTAINEROF(transport, struct spdk_srv_rdma_transport, transport);
+
+	TAILQ_FOREACH_SAFE(port, &rtransport->ports, link, port_tmp)
+	{
+		TAILQ_REMOVE(&rtransport->ports, port, link);
+		rdma_destroy_id(port->id);
+		free(port);
+	}
+
+	if (rtransport->poll_fds != NULL)
+	{
+		free(rtransport->poll_fds);
+	}
+
+	if (rtransport->event_channel != NULL)
+	{
+		rdma_destroy_event_channel(rtransport->event_channel);
+	}
+
+	TAILQ_FOREACH_SAFE(device, &rtransport->devices, link, device_tmp)
+	{
+		TAILQ_REMOVE(&rtransport->devices, device, link);
+		spdk_rdma_free_mem_map(&device->map);
+		if (device->pd)
+		{
+			ibv_dealloc_pd(device->pd);
+		}
+		free(device);
+	}
+
+	if (rtransport->data_wr_pool != NULL)
+	{
+		if (spdk_mempool_count(rtransport->data_wr_pool) !=
+			(transport->opts.max_queue_depth * SPDK_SRV_MAX_SGL_ENTRIES))
+		{
+			SPDK_ERRLOG("transport wr pool count is %zu but should be %u\n",
+						spdk_mempool_count(rtransport->data_wr_pool),
+						transport->opts.max_queue_depth * SPDK_SRV_MAX_SGL_ENTRIES);
+		}
+	}
+
+	spdk_mempool_free(rtransport->data_wr_pool);
+
+	spdk_poller_unregister(&rtransport->accept_poller);
+	pthread_mutex_destroy(&rtransport->lock);
+	free(rtransport);
+
+	if (cb_fn)
+	{
+		cb_fn(cb_arg);
+	}
+	return 0;
+}
+
+static int
+srv_rdma_trid_from_cm_id(struct rdma_cm_id *id,
+						 struct spdk_srv_transport_id *trid,
+						 bool peer);
+
+static int
+srv_rdma_listen(struct spdk_srv_transport *transport, const struct spdk_srv_transport_id *trid,
+				struct spdk_srv_listen_opts *listen_opts)
+{
+	struct spdk_srv_rdma_transport *rtransport;
+	struct spdk_srv_rdma_device *device;
+	struct spdk_srv_rdma_port *port;
+	struct addrinfo *res;
+	struct addrinfo hints;
+	int family;
+	int rc;
+
+	if (!strlen(trid->trsvcid))
+	{
+		SPDK_ERRLOG("Service id is required\n");
+		return -EINVAL;
+	}
+
+	rtransport = SPDK_CONTAINEROF(transport, struct spdk_srv_rdma_transport, transport);
+	assert(rtransport->event_channel != NULL);
+
+	pthread_mutex_lock(&rtransport->lock);
+	port = calloc(1, sizeof(*port));
+	if (!port)
+	{
+		SPDK_ERRLOG("Port allocation failed\n");
+		pthread_mutex_unlock(&rtransport->lock);
+		return -ENOMEM;
+	}
+
+	port->trid = trid;
+
+	switch (trid->adrfam)
+	{
+	case SPDK_SRV_ADRFAM_IPV4:
+		family = AF_INET;
+		break;
+	case SPDK_SRV_ADRFAM_IPV6:
+		family = AF_INET6;
+		break;
+	default:
+		SPDK_ERRLOG("Unhandled ADRFAM %d\n", trid->adrfam);
+		free(port);
+		pthread_mutex_unlock(&rtransport->lock);
+		return -EINVAL;
+	}
+
+	memset(&hints, 0, sizeof(hints));
+	hints.ai_family = family;
+	hints.ai_flags = AI_NUMERICSERV;
+	hints.ai_socktype = SOCK_STREAM;
+	hints.ai_protocol = 0;
+
+	rc = getaddrinfo(trid->traddr, trid->trsvcid, &hints, &res);
+	if (rc)
+	{
+		SPDK_ERRLOG("getaddrinfo failed: %s (%d)\n", gai_strerror(rc), rc);
+		free(port);
+		pthread_mutex_unlock(&rtransport->lock);
+		return -EINVAL;
+	}
+
+	rc = rdma_create_id(rtransport->event_channel, &port->id, port, RDMA_PS_TCP);
+	if (rc < 0)
+	{
+		SPDK_ERRLOG("rdma_create_id() failed\n");
+		freeaddrinfo(res);
+		free(port);
+		pthread_mutex_unlock(&rtransport->lock);
+		return rc;
+	}
+
+	rc = rdma_bind_addr(port->id, res->ai_addr);
+	freeaddrinfo(res);
+
+	if (rc < 0)
+	{
+		SPDK_ERRLOG("rdma_bind_addr() failed\n");
+		rdma_destroy_id(port->id);
+		free(port);
+		pthread_mutex_unlock(&rtransport->lock);
+		return rc;
+	}
+
+	if (!port->id->verbs)
+	{
+		SPDK_ERRLOG("ibv_context is null\n");
+		rdma_destroy_id(port->id);
+		free(port);
+		pthread_mutex_unlock(&rtransport->lock);
+		return -1;
+	}
+
+	rc = rdma_listen(port->id, rtransport->rdma_opts.acceptor_backlog);
+	if (rc < 0)
+	{
+		SPDK_ERRLOG("rdma_listen() failed\n");
+		rdma_destroy_id(port->id);
+		free(port);
+		pthread_mutex_unlock(&rtransport->lock);
+		return rc;
+	}
+
+	TAILQ_FOREACH(device, &rtransport->devices, link)
+	{
+		if (device->context == port->id->verbs)
+		{
+			port->device = device;
+			break;
+		}
+	}
+	if (!port->device)
+	{
+		SPDK_ERRLOG("Accepted a connection with verbs %p, but unable to find a corresponding device.\n",
+					port->id->verbs);
+		rdma_destroy_id(port->id);
+		free(port);
+		pthread_mutex_unlock(&rtransport->lock);
+		return -EINVAL;
+	}
+
+	SPDK_NOTICELOG("*** Srv/RDMA Target Listening on %s port %s ***\n",
+				   trid->traddr, trid->trsvcid);
+
+	TAILQ_INSERT_TAIL(&rtransport->ports, port, link);
+	pthread_mutex_unlock(&rtransport->lock);
+	return 0;
+}
+
+static void
+srv_rdma_stop_listen(struct spdk_srv_transport *transport,
+					 const struct spdk_srv_transport_id *trid)
+{
+	struct spdk_srv_rdma_transport *rtransport;
+	struct spdk_srv_rdma_port *port, *tmp;
+
+	rtransport = SPDK_CONTAINEROF(transport, struct spdk_srv_rdma_transport, transport);
+
+	pthread_mutex_lock(&rtransport->lock);
+	TAILQ_FOREACH_SAFE(port, &rtransport->ports, link, tmp)
+	{
+		if (spdk_srv_transport_id_compare(port->trid, trid) == 0)
+		{
+			TAILQ_REMOVE(&rtransport->ports, port, link);
+			rdma_destroy_id(port->id);
+			free(port);
+			break;
+		}
+	}
+
+	pthread_mutex_unlock(&rtransport->lock);
+}
+
+static void
+srv_rdma_conn_process_pending(struct spdk_srv_rdma_transport *rtransport,
+							  struct spdk_srv_rdma_conn *rconn, bool drain)
+{
+	struct spdk_srv_request *req, *tmp;
+	struct spdk_srv_rdma_request *rdma_req, *req_tmp;
+	struct spdk_srv_rdma_resources *resources;
+	struct spdk_req_cpl *rsp;
+
+	/* We process I/O in the data transfer pending queue at the highest priority. RDMA reads first */
+	STAILQ_FOREACH_SAFE(rdma_req, &rconn->pending_rdma_read_queue, state_link, req_tmp)
+	{
+		if (srv_rdma_request_process(rtransport, rdma_req) == false && drain == false)
+		{
+			break;
+		}
+	}
+
+	/* Then RDMA writes since reads have stronger restrictions than writes */
+	STAILQ_FOREACH_SAFE(rdma_req, &rconn->pending_rdma_write_queue, state_link, req_tmp)
+	{
+		if (srv_rdma_request_process(rtransport, rdma_req) == false && drain == false)
+		{
+			break;
+		}
+	}
+
+	/* Then we handle request waiting on memory buffers. */
+	STAILQ_FOREACH_SAFE(req, &rconn->poller->group->group.pending_buf_queue, buf_link, tmp)
+	{
+		rdma_req = SPDK_CONTAINEROF(req, struct spdk_srv_rdma_request, req);
+		if (srv_rdma_request_process(rtransport, rdma_req) == false && drain == false)
+		{
+			break;
+		}
+	}
+
+	resources = rconn->resources;
+	while (!STAILQ_EMPTY(&resources->free_queue) && !STAILQ_EMPTY(&resources->incoming_queue))
+	{
+		rdma_req = STAILQ_FIRST(&resources->free_queue);
+		STAILQ_REMOVE_HEAD(&resources->free_queue, state_link);
+		rdma_req->recv = STAILQ_FIRST(&resources->incoming_queue);
+		STAILQ_REMOVE_HEAD(&resources->incoming_queue, link);
+
+		if (rconn->srq != NULL)
+		{
+			rdma_req->req.conn = &rdma_req->recv->conn->conn;
+			rdma_req->recv->conn->qd++;
+		}
+		else
+		{
+			rconn->qd++;
+		}
+
+		rdma_req->receive_tsc = rdma_req->recv->receive_tsc;
+		rdma_req->state = RDMA_REQUEST_STATE_NEW;
+		if (srv_rdma_request_process(rtransport, rdma_req) == false)
+		{
+			break;
+		}
+	}
+	if (!STAILQ_EMPTY(&resources->incoming_queue) && STAILQ_EMPTY(&resources->free_queue))
+	{
+		rconn->poller->stat.pending_free_request++;
+	}
+}
+
+static inline bool
+srv_rdma_can_ignore_last_wqe_reached(struct spdk_srv_rdma_device *device)
+{
+	/* iWARP transport and SoftRoCE driver don't support LAST_WQE_REACHED ibv async event */
+	return srv_rdma_is_rxe_device(device) ||
+		   device->context->device->transport_type == IBV_TRANSPORT_IWARP;
+}
+
+static void
+srv_rdma_destroy_drained_conn(struct spdk_srv_rdma_conn *rconn)
+{
+	struct spdk_srv_rdma_transport *rtransport = SPDK_CONTAINEROF(rconn->conn.transport,
+																  struct spdk_srv_rdma_transport, transport);
+
+	srv_rdma_conn_process_pending(rtransport, rconn, true);
+
+	/* nvmr_rdma_close_conn is not called */
+	if (!rconn->to_close)
+	{
+		return;
+	}
+
+	/* In non SRQ path, we will reach rconn->max_queue_depth. In SRQ path, we will get the last_wqe event. */
+	if (rconn->current_send_depth != 0)
+	{
+		return;
+	}
+
+	if (rconn->srq == NULL && rconn->current_recv_depth != rconn->max_queue_depth)
+	{
+		return;
+	}
+
+	if (rconn->srq != NULL && rconn->last_wqe_reached == false &&
+		!srv_rdma_can_ignore_last_wqe_reached(rconn->device))
+	{
+		return;
+	}
+
+	assert(rconn->conn.state == SPDK_SRV_CONN_ERROR);
+
+	srv_rdma_conn_destroy(rconn);
+}
+
+static int
+srv_rdma_disconnect(struct rdma_cm_event *evt)
+{
+	struct spdk_srv_conn *conn;
+	struct spdk_srv_rdma_conn *rconn;
+
+	if (evt->id == NULL)
+	{
+		SPDK_ERRLOG("disconnect request: missing cm_id\n");
+		return -1;
+	}
+
+	conn = evt->id->context;
+	if (conn == NULL)
+	{
+		SPDK_ERRLOG("disconnect request: no active connection\n");
+		return -1;
+	}
+
+	rconn = SPDK_CONTAINEROF(conn, struct spdk_srv_rdma_conn, conn);
+
+	spdk_trace_record(TRACE_RDMA_QP_DISCONNECT, 0, 0, (uintptr_t)rconn);
+
+	spdk_srv_conn_disconnect(&rconn->conn, NULL, NULL);
+
+	return 0;
+}
+
+#ifdef DEBUG
+static const char *CM_EVENT_STR[] = {
+	"RDMA_CM_EVENT_ADDR_RESOLVED",
+	"RDMA_CM_EVENT_ADDR_ERROR",
+	"RDMA_CM_EVENT_ROUTE_RESOLVED",
+	"RDMA_CM_EVENT_ROUTE_ERROR",
+	"RDMA_CM_EVENT_CONNECT_REQUEST",
+	"RDMA_CM_EVENT_CONNECT_RESPONSE",
+	"RDMA_CM_EVENT_CONNECT_ERROR",
+	"RDMA_CM_EVENT_UNREACHABLE",
+	"RDMA_CM_EVENT_REJECTED",
+	"RDMA_CM_EVENT_ESTABLISHED",
+	"RDMA_CM_EVENT_DISCONNECTED",
+	"RDMA_CM_EVENT_DEVICE_REMOVAL",
+	"RDMA_CM_EVENT_MULTICAST_JOIN",
+	"RDMA_CM_EVENT_MULTICAST_ERROR",
+	"RDMA_CM_EVENT_ADDR_CHANGE",
+	"RDMA_CM_EVENT_TIMEWAIT_EXIT"};
+#endif /* DEBUG */
+
+static void
+srv_rdma_disconnect_conns_on_port(struct spdk_srv_rdma_transport *rtransport,
+								  struct spdk_srv_rdma_port *port)
+{
+	struct spdk_srv_rdma_poll_group *rgroup;
+	struct spdk_srv_rdma_poller *rpoller;
+	struct spdk_srv_rdma_conn *rconn;
+
+	TAILQ_FOREACH(rgroup, &rtransport->poll_groups, link)
+	{
+		TAILQ_FOREACH(rpoller, &rgroup->pollers, link)
+		{
+			TAILQ_FOREACH(rconn, &rpoller->conns, link)
+			{
+				if (rconn->listen_id == port->id)
+				{
+					spdk_srv_conn_disconnect(&rconn->conn, NULL, NULL);
+				}
+			}
+		}
+	}
+}
+
+static bool
+srv_rdma_handle_cm_event_addr_change(struct spdk_srv_transport *transport,
+									 struct rdma_cm_event *event)
+{
+	const struct spdk_srv_transport_id *trid;
+	struct spdk_srv_rdma_port *port;
+	struct spdk_srv_rdma_transport *rtransport;
+	bool event_acked = false;
+
+	rtransport = SPDK_CONTAINEROF(transport, struct spdk_srv_rdma_transport, transport);
+	TAILQ_FOREACH(port, &rtransport->ports, link)
+	{
+		if (port->id == event->id)
+		{
+			SPDK_ERRLOG("ADDR_CHANGE: IP %s:%s migrated\n", port->trid->traddr, port->trid->trsvcid);
+			rdma_ack_cm_event(event);
+			event_acked = true;
+			trid = port->trid;
+			break;
+		}
+	}
+
+	if (event_acked)
+	{
+		srv_rdma_disconnect_conns_on_port(rtransport, port);
+
+		srv_rdma_stop_listen(transport, trid);
+		srv_rdma_listen(transport, trid, NULL);
+	}
+
+	return event_acked;
+}
+
+static void
+srv_rdma_handle_cm_event_port_removal(struct spdk_srv_transport *transport,
+									  struct rdma_cm_event *event)
+{
+	struct spdk_srv_rdma_port *port;
+	struct spdk_srv_rdma_transport *rtransport;
+
+	port = event->id->context;
+	rtransport = SPDK_CONTAINEROF(transport, struct spdk_srv_rdma_transport, transport);
+
+	SPDK_NOTICELOG("Port %s:%s is being removed\n", port->trid->traddr, port->trid->trsvcid);
+
+	srv_rdma_disconnect_conns_on_port(rtransport, port);
+
+	rdma_ack_cm_event(event);
+
+	while (spdk_srv_transport_stop_listen(transport, port->trid) == 0)
+	{
+		;
+	}
+}
+
+static void
+srv_process_cm_event(struct spdk_srv_transport *transport)
+{
+	struct spdk_srv_rdma_transport *rtransport;
+	struct rdma_cm_event *event;
+	int rc;
+	bool event_acked;
+
+	rtransport = SPDK_CONTAINEROF(transport, struct spdk_srv_rdma_transport, transport);
+
+	if (rtransport->event_channel == NULL)
+	{
+		return;
+	}
+
+	while (1)
+	{
+		event_acked = false;
+		rc = rdma_get_cm_event(rtransport->event_channel, &event);
+		if (rc)
+		{
+			if (errno != EAGAIN && errno != EWOULDBLOCK)
+			{
+				SPDK_ERRLOG("Acceptor Event Error: %s\n", spdk_strerror(errno));
+			}
+			break;
+		}
+
+		SPDK_DEBUGLOG(rdma, "Acceptor Event: %s\n", CM_EVENT_STR[event->event]);
+
+		spdk_trace_record(TRACE_RDMA_CM_ASYNC_EVENT, 0, 0, 0, event->event);
+
+		switch (event->event)
+		{
+		case RDMA_CM_EVENT_ADDR_RESOLVED:
+		case RDMA_CM_EVENT_ADDR_ERROR:
+		case RDMA_CM_EVENT_ROUTE_RESOLVED:
+		case RDMA_CM_EVENT_ROUTE_ERROR:
+			/* No action required. The target never attempts to resolve routes. */
+			break;
+		case RDMA_CM_EVENT_CONNECT_REQUEST:
+			rc = srv_rdma_connect(transport, event);
+			if (rc < 0)
+			{
+				SPDK_ERRLOG("Unable to process connect event. rc: %d\n", rc);
+				break;
+			}
+			break;
+		case RDMA_CM_EVENT_CONNECT_RESPONSE:
+			/* The target never initiates a new connection. So this will not occur. */
+			break;
+		case RDMA_CM_EVENT_CONNECT_ERROR:
+			/* Can this happen? The docs say it can, but not sure what causes it. */
+			break;
+		case RDMA_CM_EVENT_UNREACHABLE:
+		case RDMA_CM_EVENT_REJECTED:
+			/* These only occur on the client side. */
+			break;
+		case RDMA_CM_EVENT_ESTABLISHED:
+			/* TODO: Should we be waiting for this event anywhere? */
+			break;
+		case RDMA_CM_EVENT_DISCONNECTED:
+			rc = srv_rdma_disconnect(event);
+			if (rc < 0)
+			{
+				SPDK_ERRLOG("Unable to process disconnect event. rc: %d\n", rc);
+				break;
+			}
+			break;
+		case RDMA_CM_EVENT_DEVICE_REMOVAL:
+			/* In case of device removal, kernel IB part triggers IBV_EVENT_DEVICE_FATAL
+			 * which triggers RDMA_CM_EVENT_DEVICE_REMOVAL on all cma_id’s.
+			 * Once these events are sent to SPDK, we should release all IB resources and
+			 * don't make attempts to call any ibv_query/modify/create functions. We can only call
+			 * ibv_destroy* functions to release user space memory allocated by IB. All kernel
+			 * resources are already cleaned. */
+			if (event->id->qp)
+			{
+				/* If rdma_cm event has a valid `qp` pointer then the event refers to the
+				 * corresponding conn. Otherwise the event refers to a listening device */
+				rc = srv_rdma_disconnect(event);
+				if (rc < 0)
+				{
+					SPDK_ERRLOG("Unable to process disconnect event. rc: %d\n", rc);
+					break;
+				}
+			}
+			else
+			{
+				srv_rdma_handle_cm_event_port_removal(transport, event);
+				event_acked = true;
+			}
+			break;
+		case RDMA_CM_EVENT_MULTICAST_JOIN:
+		case RDMA_CM_EVENT_MULTICAST_ERROR:
+			/* Multicast is not used */
+			break;
+		case RDMA_CM_EVENT_ADDR_CHANGE:
+			event_acked = srv_rdma_handle_cm_event_addr_change(transport, event);
+			break;
+		case RDMA_CM_EVENT_TIMEWAIT_EXIT:
+			/* For now, do nothing. The target never re-uses queue pairs. */
+			break;
+		default:
+			SPDK_ERRLOG("Unexpected Acceptor Event [%d]\n", event->event);
+			break;
+		}
+		if (!event_acked)
+		{
+			rdma_ack_cm_event(event);
+		}
+	}
+}
+
+static void
+srv_rdma_handle_last_wqe_reached(struct spdk_srv_rdma_conn *rconn)
+{
+	rconn->last_wqe_reached = true;
+	srv_rdma_destroy_drained_conn(rconn);
+}
+
+static void
+srv_rdma_conn_process_ibv_event(void *ctx)
+{
+	struct spdk_srv_rdma_ibv_event_ctx *event_ctx = ctx;
+
+	if (event_ctx->rconn)
+	{
+		STAILQ_REMOVE(&event_ctx->rconn->ibv_events, event_ctx, spdk_srv_rdma_ibv_event_ctx, link);
+		if (event_ctx->cb_fn)
+		{
+			event_ctx->cb_fn(event_ctx->rconn);
+		}
+	}
+	free(event_ctx);
+}
+
+static int
+srv_rdma_send_conn_async_event(struct spdk_srv_rdma_conn *rconn,
+							   spdk_srv_rdma_conn_ibv_event fn)
+{
+	struct spdk_srv_rdma_ibv_event_ctx *ctx;
+	struct spdk_thread *thr = NULL;
+	int rc;
+
+	if (rconn->conn.group)
+	{
+		thr = rconn->conn.group->thread;
+	}
+	else if (rconn->destruct_channel)
+	{
+		thr = spdk_io_channel_get_thread(rconn->destruct_channel);
+	}
+
+	if (!thr)
+	{
+		SPDK_DEBUGLOG(rdma, "rconn %p has no thread\n", rconn);
+		return -EINVAL;
+	}
+
+	ctx = calloc(1, sizeof(*ctx));
+	if (!ctx)
+	{
+		return -ENOMEM;
+	}
+
+	ctx->rconn = rconn;
+	ctx->cb_fn = fn;
+	STAILQ_INSERT_TAIL(&rconn->ibv_events, ctx, link);
+
+	rc = spdk_thread_send_msg(thr, srv_rdma_conn_process_ibv_event, ctx);
+	if (rc)
+	{
+		STAILQ_REMOVE(&rconn->ibv_events, ctx, spdk_srv_rdma_ibv_event_ctx, link);
+		free(ctx);
+	}
+
+	return rc;
+}
+
+static int
+srv_process_ib_event(struct spdk_srv_rdma_device *device)
+{
+	int rc;
+	struct spdk_srv_rdma_conn *rconn = NULL;
+	struct ibv_async_event event;
+
+	rc = ibv_get_async_event(device->context, &event);
+
+	if (rc)
+	{
+		/* In non-blocking mode -1 means there are no events available */
+		return rc;
+	}
+
+	switch (event.event_type)
+	{
+	case IBV_EVENT_QP_FATAL:
+		rconn = event.element.qp->qp_context;
+		SPDK_ERRLOG("Fatal event received for rconn %p\n", rconn);
+		spdk_trace_record(TRACE_RDMA_IBV_ASYNC_EVENT, 0, 0,
+						  (uintptr_t)rconn, event.event_type);
+		srv_rdma_update_ibv_state(rconn);
+		spdk_srv_conn_disconnect(&rconn->conn, NULL, NULL);
+		break;
+	case IBV_EVENT_QP_LAST_WQE_REACHED:
+		/* This event only occurs for shared receive queues. */
+		rconn = event.element.qp->qp_context;
+		SPDK_DEBUGLOG(rdma, "Last WQE reached event received for rconn %p\n", rconn);
+		rc = srv_rdma_send_conn_async_event(rconn, srv_rdma_handle_last_wqe_reached);
+		if (rc)
+		{
+			SPDK_WARNLOG("Failed to send LAST_WQE_REACHED event. rconn %p, err %d\n", rconn, rc);
+			rconn->last_wqe_reached = true;
+		}
+		break;
+	case IBV_EVENT_SQ_DRAINED:
+		/* This event occurs frequently in both error and non-error states.
+		 * Check if the conn is in an error state before sending a message. */
+		rconn = event.element.qp->qp_context;
+		SPDK_DEBUGLOG(rdma, "Last sq drained event received for rconn %p\n", rconn);
+		spdk_trace_record(TRACE_RDMA_IBV_ASYNC_EVENT, 0, 0,
+						  (uintptr_t)rconn, event.event_type);
+		if (srv_rdma_update_ibv_state(rconn) == IBV_QPS_ERR)
+		{
+			spdk_srv_conn_disconnect(&rconn->conn, NULL, NULL);
+		}
+		break;
+	case IBV_EVENT_QP_REQ_ERR:
+	case IBV_EVENT_QP_ACCESS_ERR:
+	case IBV_EVENT_COMM_EST:
+	case IBV_EVENT_PATH_MIG:
+	case IBV_EVENT_PATH_MIG_ERR:
+		SPDK_NOTICELOG("Async event: %s\n",
+					   ibv_event_type_str(event.event_type));
+		rconn = event.element.qp->qp_context;
+		spdk_trace_record(TRACE_RDMA_IBV_ASYNC_EVENT, 0, 0,
+						  (uintptr_t)rconn, event.event_type);
+		srv_rdma_update_ibv_state(rconn);
+		break;
+	case IBV_EVENT_CQ_ERR:
+	case IBV_EVENT_DEVICE_FATAL:
+	case IBV_EVENT_PORT_ACTIVE:
+	case IBV_EVENT_PORT_ERR:
+	case IBV_EVENT_LID_CHANGE:
+	case IBV_EVENT_PKEY_CHANGE:
+	case IBV_EVENT_SM_CHANGE:
+	case IBV_EVENT_SRQ_ERR:
+	case IBV_EVENT_SRQ_LIMIT_REACHED:
+	case IBV_EVENT_CLIENT_REREGISTER:
+	case IBV_EVENT_GID_CHANGE:
+	default:
+		SPDK_NOTICELOG("Async event: %s\n",
+					   ibv_event_type_str(event.event_type));
+		spdk_trace_record(TRACE_RDMA_IBV_ASYNC_EVENT, 0, 0, 0, event.event_type);
+		break;
+	}
+	ibv_ack_async_event(&event);
+
+	return 0;
+}
+
+static void
+srv_process_ib_events(struct spdk_srv_rdma_device *device, uint32_t max_events)
+{
+	int rc = 0;
+	uint32_t i = 0;
+
+	for (i = 0; i < max_events; i++)
+	{
+		rc = srv_process_ib_event(device);
+		if (rc)
+		{
+			break;
+		}
+	}
+
+	SPDK_DEBUGLOG(rdma, "Device %s: %u events processed\n", device->context->device->name, i);
+}
+
+static int
+srv_rdma_accept(void *ctx)
+{
+	int nfds, i = 0;
+	struct spdk_srv_transport *transport = ctx;
+	struct spdk_srv_rdma_transport *rtransport;
+	struct spdk_srv_rdma_device *device, *tmp;
+	uint32_t count;
+
+	rtransport = SPDK_CONTAINEROF(transport, struct spdk_srv_rdma_transport, transport);
+	count = nfds = poll(rtransport->poll_fds, rtransport->npoll_fds, 0);
+
+	if (nfds <= 0)
+	{
+		return SPDK_POLLER_IDLE;
+	}
+
+	/* The first poll descriptor is RDMA CM event */
+	if (rtransport->poll_fds[i++].revents & POLLIN)
+	{
+		srv_process_cm_event(transport);
+		nfds--;
+	}
+
+	if (nfds == 0)
+	{
+		return SPDK_POLLER_BUSY;
+	}
+
+	/* Second and subsequent poll descriptors are IB async events */
+	TAILQ_FOREACH_SAFE(device, &rtransport->devices, link, tmp)
+	{
+		if (rtransport->poll_fds[i++].revents & POLLIN)
+		{
+			srv_process_ib_events(device, 32);
+			nfds--;
+		}
+	}
+	/* check all flagged fd's have been served */
+	assert(nfds == 0);
+
+	return count > 0 ? SPDK_POLLER_BUSY : SPDK_POLLER_IDLE;
+}
+
+static void
+srv_rdma_poll_group_destroy(struct spdk_srv_transport_poll_group *group);
+
+static struct spdk_srv_transport_poll_group *
+srv_rdma_poll_group_create(struct spdk_srv_transport *transport)
+{
+	struct spdk_srv_rdma_transport *rtransport;
+	struct spdk_srv_rdma_poll_group *rgroup;
+	struct spdk_srv_rdma_poller *poller;
+	struct spdk_srv_rdma_device *device;
+	struct spdk_rdma_srq_init_attr srq_init_attr;
+	struct spdk_srv_rdma_resource_opts opts;
+	int num_cqe;
+
+	rtransport = SPDK_CONTAINEROF(transport, struct spdk_srv_rdma_transport, transport);
+
+	rgroup = calloc(1, sizeof(*rgroup));
+	if (!rgroup)
+	{
+		return NULL;
+	}
+
+	TAILQ_INIT(&rgroup->pollers);
+
+	pthread_mutex_lock(&rtransport->lock);
+	TAILQ_FOREACH(device, &rtransport->devices, link)
+	{
+		poller = calloc(1, sizeof(*poller));
+		if (!poller)
+		{
+			SPDK_ERRLOG("Unable to allocate memory for new RDMA poller\n");
+			srv_rdma_poll_group_destroy(&rgroup->group);
+			pthread_mutex_unlock(&rtransport->lock);
+			return NULL;
+		}
+
+		poller->device = device;
+		poller->group = rgroup;
+
+		TAILQ_INIT(&poller->conns);
+		STAILQ_INIT(&poller->conns_pending_send);
+		STAILQ_INIT(&poller->conns_pending_recv);
+
+		TAILQ_INSERT_TAIL(&rgroup->pollers, poller, link);
+
+		/*
+		 * When using an srq, we can limit the completion queue at startup.
+		 * The following formula represents the calculation:
+		 * num_cqe = num_recv + num_data_wr + num_send_wr.
+		 * where num_recv=num_data_wr=and num_send_wr=poller->max_srq_depth
+		 */
+		if (poller->srq)
+		{
+			num_cqe = poller->max_srq_depth * 3;
+		}
+		else
+		{
+			num_cqe = rtransport->rdma_opts.num_cqe;
+		}
+
+		poller->cq = ibv_create_cq(device->context, num_cqe, poller, NULL, 0);
+		if (!poller->cq)
+		{
+			SPDK_ERRLOG("Unable to create completion queue\n");
+			srv_rdma_poll_group_destroy(&rgroup->group);
+			pthread_mutex_unlock(&rtransport->lock);
+			return NULL;
+		}
+		poller->num_cqe = num_cqe;
+	}
+
+	TAILQ_INSERT_TAIL(&rtransport->poll_groups, rgroup, link);
+	if (rtransport->conn_sched.next_admin_pg == NULL)
+	{
+		rtransport->conn_sched.next_admin_pg = rgroup;
+		rtransport->conn_sched.next_io_pg = rgroup;
+	}
+
+	pthread_mutex_unlock(&rtransport->lock);
+	return &rgroup->group;
+}
+
+static struct spdk_srv_transport_poll_group *
+srv_rdma_get_optimal_poll_group(struct spdk_srv_conn *conn)
+{
+	struct spdk_srv_rdma_transport *rtransport;
+	struct spdk_srv_rdma_poll_group **pg;
+	struct spdk_srv_transport_poll_group *result;
+
+	rtransport = SPDK_CONTAINEROF(conn->transport, struct spdk_srv_rdma_transport, transport);
+
+	pthread_mutex_lock(&rtransport->lock);
+
+	if (TAILQ_EMPTY(&rtransport->poll_groups))
+	{
+		pthread_mutex_unlock(&rtransport->lock);
+		return NULL;
+	}
+
+	if (conn->qid == 0)
+	{
+		pg = &rtransport->conn_sched.next_admin_pg;
+	}
+	else
+	{
+		pg = &rtransport->conn_sched.next_io_pg;
+	}
+
+	assert(*pg != NULL);
+
+	result = &(*pg)->group;
+
+	*pg = TAILQ_NEXT(*pg, link);
+	if (*pg == NULL)
+	{
+		*pg = TAILQ_FIRST(&rtransport->poll_groups);
+	}
+
+	pthread_mutex_unlock(&rtransport->lock);
+
+	return result;
+}
+
+static void
+srv_rdma_poll_group_destroy(struct spdk_srv_transport_poll_group *group)
+{
+	struct spdk_srv_rdma_poll_group *rgroup, *next_rgroup;
+	struct spdk_srv_rdma_poller *poller, *tmp;
+	struct spdk_srv_rdma_conn *conn, *tmp_conn;
+	struct spdk_srv_rdma_transport *rtransport;
+
+	rgroup = SPDK_CONTAINEROF(group, struct spdk_srv_rdma_poll_group, group);
+	if (!rgroup)
+	{
+		return;
+	}
+
+	TAILQ_FOREACH_SAFE(poller, &rgroup->pollers, link, tmp)
+	{
+		TAILQ_REMOVE(&rgroup->pollers, poller, link);
+
+		TAILQ_FOREACH_SAFE(conn, &poller->conns, link, tmp_conn)
+		{
+			srv_rdma_conn_destroy(conn);
+		}
+
+		if (poller->srq)
+		{
+			if (poller->resources)
+			{
+				srv_rdma_resources_destroy(poller->resources);
+			}
+			spdk_rdma_srq_destroy(poller->srq);
+			SPDK_DEBUGLOG(rdma, "Destroyed RDMA shared queue %p\n", poller->srq);
+		}
+
+		if (poller->cq)
+		{
+			ibv_destroy_cq(poller->cq);
+		}
+
+		free(poller);
+	}
+
+	if (rgroup->group.transport == NULL)
+	{
+		/* Transport can be NULL when srv_rdma_poll_group_create()
+		 * calls this function directly in a failure path. */
+		free(rgroup);
+		return;
+	}
+
+	rtransport = SPDK_CONTAINEROF(rgroup->group.transport, struct spdk_srv_rdma_transport, transport);
+
+	pthread_mutex_lock(&rtransport->lock);
+	next_rgroup = TAILQ_NEXT(rgroup, link);
+	TAILQ_REMOVE(&rtransport->poll_groups, rgroup, link);
+	if (next_rgroup == NULL)
+	{
+		next_rgroup = TAILQ_FIRST(&rtransport->poll_groups);
+	}
+	if (rtransport->conn_sched.next_admin_pg == rgroup)
+	{
+		rtransport->conn_sched.next_admin_pg = next_rgroup;
+	}
+	if (rtransport->conn_sched.next_io_pg == rgroup)
+	{
+		rtransport->conn_sched.next_io_pg = next_rgroup;
+	}
+	pthread_mutex_unlock(&rtransport->lock);
+
+	free(rgroup);
+}
+
+static void
+srv_rdma_conn_reject_connection(struct spdk_srv_rdma_conn *rconn)
+{
+	if (rconn->cm_id != NULL)
+	{
+		srv_rdma_event_reject(rconn->cm_id, SPDK_SRV_RDMA_ERROR_NO_RESOURCES);
+	}
+}
+
+static int
+srv_rdma_poll_group_add(struct spdk_srv_transport_poll_group *group,
+						struct spdk_srv_conn *conn)
+{
+	struct spdk_srv_rdma_poll_group *rgroup;
+	struct spdk_srv_rdma_conn *rconn;
+	struct spdk_srv_rdma_device *device;
+	struct spdk_srv_rdma_poller *poller;
+	int rc;
+
+	rgroup = SPDK_CONTAINEROF(group, struct spdk_srv_rdma_poll_group, group);
+	rconn = SPDK_CONTAINEROF(conn, struct spdk_srv_rdma_conn, conn);
+
+	device = rconn->device;
+
+	TAILQ_FOREACH(poller, &rgroup->pollers, link)
+	{
+		if (poller->device == device)
+		{
+			break;
+		}
+	}
+
+	if (!poller)
+	{
+		SPDK_ERRLOG("No poller found for device.\n");
+		return -1;
+	}
+
+	TAILQ_INSERT_TAIL(&poller->conns, rconn, link);
+	rconn->poller = poller;
+	rconn->srq = rconn->poller->srq;
+
+	rc = srv_rdma_conn_initialize(conn);
+	if (rc < 0)
+	{
+		SPDK_ERRLOG("Failed to initialize srv_rdma_conn with conn=%p\n", conn);
+		return -1;
+	}
+
+	rc = srv_rdma_event_accept(rconn->cm_id, rconn);
+	if (rc)
+	{
+		/* Try to reject, but we probably can't */
+		srv_rdma_conn_reject_connection(rconn);
+		return -1;
+	}
+
+	srv_rdma_update_ibv_state(rconn);
+
+	return 0;
+}
+
+// FIXME:
+static int
+srv_rdma_poll_group_remove(struct spdk_srv_transport_poll_group *group,
+						   struct spdk_srv_conn *conn)
+{
+	return 0;
+}
+
+static int
+srv_rdma_request_free(struct spdk_srv_request *req)
+{
+	struct spdk_srv_rdma_request *rdma_req = SPDK_CONTAINEROF(req, struct spdk_srv_rdma_request, req);
+	struct spdk_srv_rdma_transport *rtransport = SPDK_CONTAINEROF(req->conn->transport,
+																  struct spdk_srv_rdma_transport, transport);
+	struct spdk_srv_rdma_conn *rconn = SPDK_CONTAINEROF(rdma_req->req.conn,
+														struct spdk_srv_rdma_conn, conn);
+
+	/*
+	 * AER requests are freed when a conn is destroyed. The recv corresponding to that request
+	 * needs to be returned to the shared receive queue or the poll group will eventually be
+	 * starved of RECV structures.
+	 */
+	if (rconn->srq && rdma_req->recv)
+	{
+		int rc;
+		struct ibv_recv_wr *bad_recv_wr;
+
+		spdk_rdma_srq_queue_recv_wrs(rconn->srq, &rdma_req->recv->wr);
+		rc = spdk_rdma_srq_flush_recv_wrs(rconn->srq, &bad_recv_wr);
+		if (rc)
+		{
+			SPDK_ERRLOG("Unable to re-post rx descriptor\n");
+		}
+	}
+
+	_srv_rdma_request_free(rdma_req, rtransport);
+	return 0;
+}
+
+static int
+srv_rdma_request_complete(struct spdk_srv_request *req)
+{
+	struct spdk_srv_rdma_transport *rtransport = SPDK_CONTAINEROF(req->conn->transport,
+																  struct spdk_srv_rdma_transport, transport);
+	struct spdk_srv_rdma_request *rdma_req = SPDK_CONTAINEROF(req,
+															  struct spdk_srv_rdma_request, req);
+	struct spdk_srv_rdma_conn *rconn = SPDK_CONTAINEROF(rdma_req->req.conn,
+														struct spdk_srv_rdma_conn, conn);
+
+	if (rconn->ibv_state != IBV_QPS_ERR)
+	{
+		/* The connection is alive, so process the request as normal */
+		rdma_req->state = RDMA_REQUEST_STATE_EXECUTED;
+	}
+	else
+	{
+		/* The connection is dead. Move the request directly to the completed state. */
+		rdma_req->state = RDMA_REQUEST_STATE_COMPLETED;
+	}
+
+	srv_rdma_request_process(rtransport, rdma_req);
+
+	return 0;
+}
+
+static void
+srv_rdma_close_conn(struct spdk_srv_conn *conn,
+					spdk_srv_transport_conn_fini_cb cb_fn, void *cb_arg)
+{
+	struct spdk_srv_rdma_conn *rconn = SPDK_CONTAINEROF(conn, struct spdk_srv_rdma_conn, conn);
+
+	rconn->to_close = true;
+
+	/* This happens only when the conn is disconnected before
+	 * it is added to the poll group. Since there is no poll group,
+	 * the RDMA qp has not been initialized yet and the RDMA CM
+	 * event has not yet been acknowledged, so we need to reject it.
+	 */
+	if (rconn->conn.state == SPDK_SRV_CONN_UNINITIALIZED)
+	{
+		srv_rdma_conn_reject_connection(rconn);
+		srv_rdma_conn_destroy(rconn);
+		return;
+	}
+
+	if (rconn->rdma_qp)
+	{
+		spdk_rdma_qp_disconnect(rconn->rdma_qp);
+	}
+
+	srv_rdma_destroy_drained_conn(rconn);
+
+	if (cb_fn)
+	{
+		cb_fn(cb_arg);
+	}
+}
+
+static struct spdk_srv_rdma_conn *
+get_rdma_conn_from_wc(struct spdk_srv_rdma_poller *rpoller, struct ibv_wc *wc)
+{
+	struct spdk_srv_rdma_conn *rconn;
+	/* @todo: improve QP search */
+	TAILQ_FOREACH(rconn, &rpoller->conns, link)
+	{
+		if (wc->qp_num == rconn->rdma_qp->qp->qp_num)
+		{
+			return rconn;
+		}
+	}
+	SPDK_ERRLOG("Didn't find QP with qp_num %u\n", wc->qp_num);
+	return NULL;
+}
+
+#ifdef DEBUG
+static int
+srv_rdma_req_is_completing(struct spdk_srv_rdma_request *rdma_req)
+{
+	return rdma_req->state == RDMA_REQUEST_STATE_TRANSFERRING_CONTROLLER_TO_HOST ||
+		   rdma_req->state == RDMA_REQUEST_STATE_COMPLETING;
+}
+#endif
+
+static void
+_poller_reset_failed_recvs(struct spdk_srv_rdma_poller *rpoller, struct ibv_recv_wr *bad_recv_wr,
+						   int rc)
+{
+	struct spdk_srv_rdma_recv *rdma_recv;
+	struct spdk_srv_rdma_wr *bad_rdma_wr;
+
+	SPDK_ERRLOG("Failed to post a recv for the poller %p with errno %d\n", rpoller, -rc);
+	while (bad_recv_wr != NULL)
+	{
+		bad_rdma_wr = (struct spdk_srv_rdma_wr *)bad_recv_wr->wr_id;
+		rdma_recv = SPDK_CONTAINEROF(bad_rdma_wr, struct spdk_srv_rdma_recv, rdma_wr);
+
+		rdma_recv->conn->current_recv_depth++;
+		bad_recv_wr = bad_recv_wr->next;
+		SPDK_ERRLOG("Failed to post a recv for the conn %p with errno %d\n", rdma_recv->conn, -rc);
+		spdk_srv_conn_disconnect(&rdma_recv->conn->conn, NULL, NULL);
+	}
+}
+
+static void
+_qp_reset_failed_recvs(struct spdk_srv_rdma_conn *rconn, struct ibv_recv_wr *bad_recv_wr, int rc)
+{
+	SPDK_ERRLOG("Failed to post a recv for the conn %p with errno %d\n", rconn, -rc);
+	while (bad_recv_wr != NULL)
+	{
+		bad_recv_wr = bad_recv_wr->next;
+		rconn->current_recv_depth++;
+	}
+	spdk_srv_conn_disconnect(&rconn->conn, NULL, NULL);
+}
+
+static void
+_poller_submit_recvs(struct spdk_srv_rdma_transport *rtransport,
+					 struct spdk_srv_rdma_poller *rpoller)
+{
+	struct spdk_srv_rdma_conn *rconn;
+	struct ibv_recv_wr *bad_recv_wr;
+	int rc;
+
+	if (rpoller->srq)
+	{
+		rc = spdk_rdma_srq_flush_recv_wrs(rpoller->srq, &bad_recv_wr);
+		if (rc)
+		{
+			_poller_reset_failed_recvs(rpoller, bad_recv_wr, rc);
+		}
+	}
+	else
+	{
+		while (!STAILQ_EMPTY(&rpoller->conns_pending_recv))
+		{
+			rconn = STAILQ_FIRST(&rpoller->conns_pending_recv);
+			rc = spdk_rdma_qp_flush_recv_wrs(rconn->rdma_qp, &bad_recv_wr);
+			if (rc)
+			{
+				_qp_reset_failed_recvs(rconn, bad_recv_wr, rc);
+			}
+			STAILQ_REMOVE_HEAD(&rpoller->conns_pending_recv, recv_link);
+		}
+	}
+}
+
+static void
+_poller_comsume_pending_rpc_rdma_request(struct spdk_srv_rdma_transport *rtransport,
+										 struct spdk_srv_rdma_poller *rpoller)
+{
+	struct spdk_srv_rdma_conn *rconn, *rconn_tmp;
+	struct spdk_srv_rdma_request *rdma_req, *req_tmp;
+	struct ibv_recv_wr *bad_recv_wr;
+	int rc;
+	TAILQ_FOREACH_SAFE(rconn, &rpoller->conns, link, rconn_tmp)
+	{
+		STAILQ_FOREACH_SAFE(rdma_req, &rconn->pending_complete_queue, state_link, req_tmp)
+		{
+			if (rconn->ibv_state != IBV_QPS_ERR)
+			{
+				/* The connection is alive, so process the request as normal */
+				rdma_req->state = RDMA_REQUEST_STATE_EXECUTED;
+			}
+			else
+			{
+				/* The connection is dead. Move the request directly to the completed state. */
+				rdma_req->state = RDMA_REQUEST_STATE_COMPLETED;
+			}
+			SPDK_DEBUGLOG(rdma, "rdma_req addr: %p\n", rdma_req);
+			assert(rdma_req->recv != NULL);
+			if (srv_rdma_request_process(rtransport, rdma_req) == false)
+			{
+				break;
+			}
+		}
+	}
+}
+
+static void
+_qp_reset_failed_sends(struct spdk_srv_rdma_transport *rtransport,
+					   struct spdk_srv_rdma_conn *rconn, struct ibv_send_wr *bad_wr, int rc)
+{
+	struct spdk_srv_rdma_wr *bad_rdma_wr;
+	struct spdk_srv_rdma_request *prev_rdma_req = NULL, *cur_rdma_req = NULL;
+
+	SPDK_ERRLOG("Failed to post a send for the conn %p with errno %d\n", rconn, -rc);
+	for (; bad_wr != NULL; bad_wr = bad_wr->next)
+	{
+		bad_rdma_wr = (struct spdk_srv_rdma_wr *)bad_wr->wr_id;
+		assert(rconn->current_send_depth > 0);
+		rconn->current_send_depth--;
+		switch (bad_rdma_wr->type)
+		{
+		case RDMA_WR_TYPE_DATA:
+			cur_rdma_req = SPDK_CONTAINEROF(bad_rdma_wr, struct spdk_srv_rdma_request, data.rdma_wr);
+			if (bad_wr->opcode == IBV_WR_RDMA_READ)
+			{
+				assert(rconn->current_read_depth > 0);
+				rconn->current_read_depth--;
+			}
+			break;
+		case RDMA_WR_TYPE_SEND:
+			cur_rdma_req = SPDK_CONTAINEROF(bad_rdma_wr, struct spdk_srv_rdma_request, rsp.rdma_wr);
+			break;
+		default:
+			SPDK_ERRLOG("Found a RECV in the list of pending SEND requests for conn %p\n", rconn);
+			prev_rdma_req = cur_rdma_req;
+			continue;
+		}
+
+		if (prev_rdma_req == cur_rdma_req)
+		{
+			/* this request was handled by an earlier wr. i.e. we were performing an srv read. */
+			/* We only have to check against prev_wr since each requests wrs are contiguous in this list. */
+			continue;
+		}
+
+		switch (cur_rdma_req->state)
+		{
+		case RDMA_REQUEST_STATE_TRANSFERRING_HOST_TO_CONTROLLER:
+			cur_rdma_req->req.rsp->status.sc = SPDK_SRV_SC_INTERNAL_DEVICE_ERROR;
+			cur_rdma_req->state = RDMA_REQUEST_STATE_READY_TO_COMPLETE;
+			break;
+		case RDMA_REQUEST_STATE_TRANSFERRING_CONTROLLER_TO_HOST:
+		case RDMA_REQUEST_STATE_COMPLETING:
+			cur_rdma_req->state = RDMA_REQUEST_STATE_COMPLETED;
+			break;
+		default:
+			SPDK_ERRLOG("Found a request in a bad state %d when draining pending SEND requests for conn %p\n",
+						cur_rdma_req->state, rconn);
+			continue;
+		}
+
+		srv_rdma_request_process(rtransport, cur_rdma_req);
+		prev_rdma_req = cur_rdma_req;
+	}
+
+	if (rconn->conn.state == SPDK_SRV_CONN_ACTIVE)
+	{
+		/* Disconnect the connection. */
+		spdk_srv_conn_disconnect(&rconn->conn, NULL, NULL);
+	}
+}
+
+static void
+_poller_submit_sends(struct spdk_srv_rdma_transport *rtransport,
+					 struct spdk_srv_rdma_poller *rpoller)
+{
+	struct spdk_srv_rdma_conn *rconn;
+	struct ibv_send_wr *bad_wr = NULL;
+	int rc;
+	while (!STAILQ_EMPTY(&rpoller->conns_pending_send))
+	{
+		rconn = STAILQ_FIRST(&rpoller->conns_pending_send);
+		rc = spdk_rdma_qp_flush_send_wrs(rconn->rdma_qp, &bad_wr);
+
+		/* bad wr always points to the first wr that failed. */
+		if (rc)
+		{
+			_qp_reset_failed_sends(rtransport, rconn, bad_wr, rc);
+		}
+		STAILQ_REMOVE_HEAD(&rpoller->conns_pending_send, send_link);
+	}
+}
+
+static const char *
+srv_rdma_wr_type_str(enum spdk_srv_rdma_wr_type wr_type)
+{
+	switch (wr_type)
+	{
+	case RDMA_WR_TYPE_RECV:
+		return "RECV";
+	case RDMA_WR_TYPE_SEND:
+		return "SEND";
+	case RDMA_WR_TYPE_DATA:
+		return "DATA";
+	default:
+		SPDK_ERRLOG("Unknown WR type %d\n", wr_type);
+		SPDK_UNREACHABLE();
+	}
+}
+
+static inline void
+srv_rdma_log_wc_status(struct spdk_srv_rdma_conn *rconn, struct ibv_wc *wc)
+{
+	enum spdk_srv_rdma_wr_type wr_type = ((struct spdk_srv_rdma_wr *)wc->wr_id)->type;
+
+	if (wc->status == IBV_WC_WR_FLUSH_ERR)
+	{
+		/* If conn is in ERR state, we will receive completions for all posted and not completed
+		 * Work Requests with IBV_WC_WR_FLUSH_ERR status. Don't log an error in that case */
+		SPDK_DEBUGLOG(rdma,
+					  "Error on CQ %p, (qp state %d ibv_state %d) request 0x%lu, type %s, status: (%d): %s\n",
+					  rconn->poller->cq, rconn->conn.state, rconn->ibv_state, wc->wr_id,
+					  srv_rdma_wr_type_str(wr_type), wc->status, ibv_wc_status_str(wc->status));
+	}
+	else
+	{
+		SPDK_ERRLOG("Error on CQ %p, (qp state %d ibv_state %d) request 0x%lu, type %s, status: (%d): %s\n",
+					rconn->poller->cq, rconn->conn.state, rconn->ibv_state, wc->wr_id,
+					srv_rdma_wr_type_str(wr_type), wc->status, ibv_wc_status_str(wc->status));
+	}
+}
+
+static int
+srv_rdma_poller_poll(struct spdk_srv_rdma_transport *rtransport,
+					 struct spdk_srv_rdma_poller *rpoller)
+{
+	struct ibv_wc wc[32];
+	struct spdk_srv_rdma_wr *rdma_wr;
+	struct spdk_srv_rdma_request *rdma_req, *req_tmp;
+	struct spdk_srv_rdma_recv *rdma_recv;
+	struct spdk_srv_rdma_conn *rconn;
+	int reaped, i;
+	int count = 0;
+	bool error = false;
+	uint64_t poll_tsc = spdk_get_ticks();
+
+	/* Poll for completing operations. */
+	reaped = ibv_poll_cq(rpoller->cq, 32, wc);
+	if (reaped < 0)
+	{
+		SPDK_ERRLOG("Error polling CQ! (%d): %s\n",
+					errno, spdk_strerror(errno));
+		return -1;
+	}
+	else if (reaped == 0)
+	{
+		rpoller->stat.idle_polls++;
+	}
+
+	rpoller->stat.polls++;
+	rpoller->stat.completions += reaped;
+
+	for (i = 0; i < reaped; i++)
+	{
+
+		rdma_wr = (struct spdk_srv_rdma_wr *)wc[i].wr_id;
+		SPDK_DEBUGLOG(rdma, "enter srv_rdma_poller_poll reaped %d current %d\n", reaped, i);
+		switch (rdma_wr->type)
+		{
+		case RDMA_WR_TYPE_SEND:
+			rdma_req = SPDK_CONTAINEROF(rdma_wr, struct spdk_srv_rdma_request, rsp.rdma_wr);
+			rconn = SPDK_CONTAINEROF(rdma_req->req.conn, struct spdk_srv_rdma_conn, conn);
+
+			if (!wc[i].status)
+			{
+				count++;
+				assert(wc[i].opcode == IBV_WC_SEND);
+				assert(srv_rdma_req_is_completing(rdma_req));
+			}
+
+			rdma_req->state = RDMA_REQUEST_STATE_COMPLETED;
+			/* RDMA_WRITE operation completed. +1 since it was chained with rsp WR */
+			rconn->current_send_depth -= rdma_req->num_outstanding_data_wr + 1;
+			rdma_req->num_outstanding_data_wr = 0;
+
+			srv_rdma_request_process(rtransport, rdma_req);
+			break;
+		case RDMA_WR_TYPE_RECV:
+			/* rdma_recv->conn will be invalid if using an SRQ.  In that case we have to get the conn from the wc. */
+			rdma_recv = SPDK_CONTAINEROF(rdma_wr, struct spdk_srv_rdma_recv, rdma_wr);
+			if (rpoller->srq != NULL)
+			{
+				rdma_recv->conn = get_rdma_conn_from_wc(rpoller, &wc[i]);
+				/* It is possible that there are still some completions for destroyed QP
+				 * associated with SRQ. We just ignore these late completions and re-post
+				 * receive WRs back to SRQ.
+				 */
+				if (spdk_unlikely(NULL == rdma_recv->conn))
+				{
+					struct ibv_recv_wr *bad_wr;
+					int rc;
+
+					rdma_recv->wr.next = NULL;
+					spdk_rdma_srq_queue_recv_wrs(rpoller->srq, &rdma_recv->wr);
+					rc = spdk_rdma_srq_flush_recv_wrs(rpoller->srq, &bad_wr);
+					if (rc)
+					{
+						SPDK_ERRLOG("Failed to re-post recv WR to SRQ, err %d\n", rc);
+					}
+					continue;
+				}
+			}
+			rconn = rdma_recv->conn;
+
+			assert(rconn != NULL);
+			if (!wc[i].status)
+			{
+				assert(wc[i].opcode == IBV_WC_RECV);
+				if (rconn->current_recv_depth >= rconn->max_queue_depth)
+				{
+					spdk_srv_conn_disconnect(&rconn->conn, NULL, NULL);
+					break;
+				}
+			}
+
+			rdma_recv->wr.next = NULL;
+			rconn->current_recv_depth++;
+			rdma_recv->receive_tsc = poll_tsc;
+			rpoller->stat.requests++;
+			STAILQ_INSERT_HEAD(&rconn->resources->incoming_queue, rdma_recv, link);
+			break;
+		case RDMA_WR_TYPE_DATA:
+			rdma_req = SPDK_CONTAINEROF(rdma_wr, struct spdk_srv_rdma_request, data.rdma_wr);
+			rconn = SPDK_CONTAINEROF(rdma_req->req.conn, struct spdk_srv_rdma_conn, conn);
+
+			assert(rdma_req->num_outstanding_data_wr > 0);
+
+			rconn->current_send_depth--;
+			rdma_req->num_outstanding_data_wr--;
+			if (!wc[i].status)
+			{
+				assert(wc[i].opcode == IBV_WC_RDMA_READ);
+				rconn->current_read_depth--;
+				/* wait for all outstanding reads associated with the same rdma_req to complete before proceeding. */
+				if (rdma_req->num_outstanding_data_wr == 0)
+				{
+					rdma_req->state = RDMA_REQUEST_STATE_READY_TO_EXECUTE;
+					srv_rdma_request_process(rtransport, rdma_req);
+				}
+			}
+			else
+			{
+				/* If the data transfer fails still force the queue into the error state,
+				 * if we were performing an RDMA_READ, we need to force the request into a
+				 * completed state since it wasn't linked to a send. However, in the RDMA_WRITE
+				 * case, we should wait for the SEND to complete. */
+				if (rdma_req->data.wr.opcode == IBV_WR_RDMA_READ)
+				{
+					rconn->current_read_depth--;
+					if (rdma_req->num_outstanding_data_wr == 0)
+					{
+						rdma_req->state = RDMA_REQUEST_STATE_COMPLETED;
+					}
+				}
+			}
+			break;
+		default:
+			SPDK_ERRLOG("Received an unknown opcode on the CQ: %d\n", wc[i].opcode);
+			continue;
+		}
+
+		/* Handle error conditions */
+		if (wc[i].status)
+		{
+			srv_rdma_update_ibv_state(rconn);
+			srv_rdma_log_wc_status(rconn, &wc[i]);
+
+			error = true;
+
+			if (rconn->conn.state == SPDK_SRV_CONN_ACTIVE)
+			{
+				/* Disconnect the connection. */
+				spdk_srv_conn_disconnect(&rconn->conn, NULL, NULL);
+			}
+			else
+			{
+				srv_rdma_destroy_drained_conn(rconn);
+			}
+			continue;
+		}
+
+		srv_rdma_conn_process_pending(rtransport, rconn, false);
+
+		if (rconn->conn.state != SPDK_SRV_CONN_ACTIVE)
+		{
+			srv_rdma_destroy_drained_conn(rconn);
+		}
+	}
+
+	if (error == true)
+	{
+		return -1;
+	}
+
+	/* submit outstanding work requests. */
+	_poller_submit_recvs(rtransport, rpoller);
+	_poller_submit_sends(rtransport, rpoller);
+	_poller_comsume_pending_rpc_rdma_request(rtransport, rpoller);
+
+	return count;
+}
+
+static int
+srv_rdma_poll_group_poll(struct spdk_srv_transport_poll_group *group)
+{
+	struct spdk_srv_rdma_transport *rtransport;
+	struct spdk_srv_rdma_poll_group *rgroup;
+	struct spdk_srv_rdma_poller *rpoller;
+	int count, rc;
+
+	rtransport = SPDK_CONTAINEROF(group->transport, struct spdk_srv_rdma_transport, transport);
+	rgroup = SPDK_CONTAINEROF(group, struct spdk_srv_rdma_poll_group, group);
+
+	count = 0;
+	TAILQ_FOREACH(rpoller, &rgroup->pollers, link)
+	{
+		rc = srv_rdma_poller_poll(rtransport, rpoller);
+		if (rc < 0)
+		{
+			return rc;
+		}
+		count += rc;
+	}
+
+	return count;
+}
+
+static int
+srv_rdma_trid_from_cm_id(struct rdma_cm_id *id,
+						 struct spdk_srv_transport_id *trid,
+						 bool peer)
+{
+	struct sockaddr *saddr;
+	uint16_t port;
+
+	spdk_srv_trid_populate_transport(trid, SPDK_SRV_TRANSPORT_RDMA);
+
+	if (peer)
+	{
+		saddr = rdma_get_peer_addr(id);
+	}
+	else
+	{
+		saddr = rdma_get_local_addr(id);
+	}
+	switch (saddr->sa_family)
+	{
+	case AF_INET:
+	{
+		struct sockaddr_in *saddr_in = (struct sockaddr_in *)saddr;
+
+		trid->adrfam = SPDK_SRV_ADRFAM_IPV4;
+		inet_ntop(AF_INET, &saddr_in->sin_addr,
+				  trid->traddr, sizeof(trid->traddr));
+		if (peer)
+		{
+			port = ntohs(rdma_get_dst_port(id));
+		}
+		else
+		{
+			port = ntohs(rdma_get_src_port(id));
+		}
+		snprintf(trid->trsvcid, sizeof(trid->trsvcid), "%u", port);
+		break;
+	}
+	case AF_INET6:
+	{
+		struct sockaddr_in6 *saddr_in = (struct sockaddr_in6 *)saddr;
+		trid->adrfam = SPDK_SRV_ADRFAM_IPV6;
+		inet_ntop(AF_INET6, &saddr_in->sin6_addr,
+				  trid->traddr, sizeof(trid->traddr));
+		if (peer)
+		{
+			port = ntohs(rdma_get_dst_port(id));
+		}
+		else
+		{
+			port = ntohs(rdma_get_src_port(id));
+		}
+		snprintf(trid->trsvcid, sizeof(trid->trsvcid), "%u", port);
+		break;
+	}
+	default:
+		return -1;
+	}
+
+	return 0;
+}
+
+static int
+srv_rdma_conn_get_peer_trid(struct spdk_srv_conn *conn,
+							struct spdk_srv_transport_id *trid)
+{
+	struct spdk_srv_rdma_conn *rconn;
+
+	rconn = SPDK_CONTAINEROF(conn, struct spdk_srv_rdma_conn, conn);
+
+	return srv_rdma_trid_from_cm_id(rconn->cm_id, trid, true);
+}
+
+static int
+srv_rdma_conn_get_local_trid(struct spdk_srv_conn *conn,
+							 struct spdk_srv_transport_id *trid)
+{
+	struct spdk_srv_rdma_conn *rconn;
+
+	rconn = SPDK_CONTAINEROF(conn, struct spdk_srv_rdma_conn, conn);
+
+	return srv_rdma_trid_from_cm_id(rconn->cm_id, trid, false);
+}
+
+static int
+srv_rdma_conn_get_listen_trid(struct spdk_srv_conn *conn,
+							  struct spdk_srv_transport_id *trid)
+{
+	struct spdk_srv_rdma_conn *rconn;
+
+	rconn = SPDK_CONTAINEROF(conn, struct spdk_srv_rdma_conn, conn);
+
+	return srv_rdma_trid_from_cm_id(rconn->listen_id, trid, false);
+}
+
+static void
+srv_rdma_request_set_abort_status(struct spdk_srv_request *req,
+								  struct spdk_srv_rdma_request *rdma_req_to_abort)
+{
+	rdma_req_to_abort->req.rsp->status.sc = SPDK_SRV_SC_ABORTED_BY_REQUEST;
+
+	rdma_req_to_abort->state = RDMA_REQUEST_STATE_READY_TO_COMPLETE;
+
+	req->rsp->cdw0 &= ~1U; /* Command was successfully aborted. */
+}
+
+static int
+_srv_rdma_conn_abort_request(void *ctx)
+{
+	struct spdk_srv_request *req = ctx;
+	struct spdk_srv_rdma_request *rdma_req_to_abort = SPDK_CONTAINEROF(
+		req->req_to_abort, struct spdk_srv_rdma_request, req);
+	struct spdk_srv_rdma_conn *rconn = SPDK_CONTAINEROF(req->req_to_abort->conn,
+														struct spdk_srv_rdma_conn, conn);
+	int rc;
+
+	spdk_poller_unregister(&req->poller);
+
+	switch (rdma_req_to_abort->state)
+	{
+	case RDMA_REQUEST_STATE_EXECUTING:
+
+		break;
+
+	case RDMA_REQUEST_STATE_NEED_BUFFER:
+		STAILQ_REMOVE(&rconn->poller->group->group.pending_buf_queue,
+					  &rdma_req_to_abort->req, spdk_srv_request, buf_link);
+
+		srv_rdma_request_set_abort_status(req, rdma_req_to_abort);
+		break;
+
+	case RDMA_REQUEST_STATE_DATA_TRANSFER_TO_CONTROLLER_PENDING:
+		STAILQ_REMOVE(&rconn->pending_rdma_read_queue, rdma_req_to_abort,
+					  spdk_srv_rdma_request, state_link);
+
+		srv_rdma_request_set_abort_status(req, rdma_req_to_abort);
+		break;
+
+	case RDMA_REQUEST_STATE_DATA_TRANSFER_TO_HOST_PENDING:
+		STAILQ_REMOVE(&rconn->pending_rdma_write_queue, rdma_req_to_abort,
+					  spdk_srv_rdma_request, state_link);
+
+		srv_rdma_request_set_abort_status(req, rdma_req_to_abort);
+		break;
+
+	case RDMA_REQUEST_STATE_TRANSFERRING_HOST_TO_CONTROLLER:
+		if (spdk_get_ticks() < req->timeout_tsc)
+		{
+			req->poller = SPDK_POLLER_REGISTER(_srv_rdma_conn_abort_request, req, 0);
+			return SPDK_POLLER_BUSY;
+		}
+		break;
+
+	default:
+		break;
+	}
+
+	spdk_srv_request_complete(req);
+	return SPDK_POLLER_BUSY;
+}
+
+static void
+srv_rdma_conn_abort_request(struct spdk_srv_conn *conn,
+							struct spdk_srv_request *req)
+{
+	struct spdk_srv_rdma_conn *rconn;
+	struct spdk_srv_rdma_transport *rtransport;
+	struct spdk_srv_transport *transport;
+	uint16_t cid;
+	uint32_t i, max_req_count;
+	struct spdk_srv_rdma_request *rdma_req_to_abort = NULL, *rdma_req;
+
+	rconn = SPDK_CONTAINEROF(conn, struct spdk_srv_rdma_conn, conn);
+	rtransport = SPDK_CONTAINEROF(conn->transport, struct spdk_srv_rdma_transport, transport);
+	transport = &rtransport->transport;
+	cid = req->cmd->cid;
+	max_req_count = rconn->srq == NULL ? rconn->max_queue_depth : rconn->poller->max_srq_depth;
+
+	for (i = 0; i < max_req_count; i++)
+	{
+		rdma_req = &rconn->resources->reqs[i];
+		/* When SRQ == NULL, rconn has its own requests and req.conn pointer always points to the conn
+		 * When SRQ != NULL all rconns share common requests and conn pointer is assigned when we start to
+		 * process a request. So in both cases all requests which are not in FREE state have valid conn ptr */
+		if (rdma_req->state != RDMA_REQUEST_STATE_FREE && rdma_req->req.cmd->cid == cid &&
+			rdma_req->req.conn == conn)
+		{
+			rdma_req_to_abort = rdma_req;
+			break;
+		}
+	}
+
+	if (rdma_req_to_abort == NULL)
+	{
+		spdk_srv_request_complete(req);
+		return;
+	}
+
+	req->req_to_abort = &rdma_req_to_abort->req;
+	req->timeout_tsc = spdk_get_ticks() +
+					   transport->opts.abort_timeout_sec * spdk_get_ticks_hz();
+	req->poller = NULL;
+
+	_srv_rdma_conn_abort_request(req);
+}
+
+static void
+srv_rdma_poll_group_dump_stat(struct spdk_srv_transport_poll_group *group,
+							  struct spdk_json_write_ctx *w)
+{
+	struct spdk_srv_rdma_poll_group *rgroup;
+	struct spdk_srv_rdma_poller *rpoller;
+
+	assert(w != NULL);
+
+	rgroup = SPDK_CONTAINEROF(group, struct spdk_srv_rdma_poll_group, group);
+
+	spdk_json_write_named_uint64(w, "pending_data_buffer", rgroup->stat.pending_data_buffer);
+
+	spdk_json_write_named_array_begin(w, "devices");
+
+	TAILQ_FOREACH(rpoller, &rgroup->pollers, link)
+	{
+		spdk_json_write_object_begin(w);
+		spdk_json_write_named_string(w, "name",
+									 ibv_get_device_name(rpoller->device->context->device));
+		spdk_json_write_named_uint64(w, "polls",
+									 rpoller->stat.polls);
+		spdk_json_write_named_uint64(w, "idle_polls",
+									 rpoller->stat.idle_polls);
+		spdk_json_write_named_uint64(w, "completions",
+									 rpoller->stat.completions);
+		spdk_json_write_named_uint64(w, "requests",
+									 rpoller->stat.requests);
+		spdk_json_write_named_uint64(w, "request_latency",
+									 rpoller->stat.request_latency);
+		spdk_json_write_named_uint64(w, "pending_free_request",
+									 rpoller->stat.pending_free_request);
+		spdk_json_write_named_uint64(w, "pending_rdma_read",
+									 rpoller->stat.pending_rdma_read);
+		spdk_json_write_named_uint64(w, "pending_rdma_write",
+									 rpoller->stat.pending_rdma_write);
+		spdk_json_write_named_uint64(w, "total_send_wrs",
+									 rpoller->stat.qp_stats.send.num_submitted_wrs);
+		spdk_json_write_named_uint64(w, "send_doorbell_updates",
+									 rpoller->stat.qp_stats.send.doorbell_updates);
+		spdk_json_write_named_uint64(w, "total_recv_wrs",
+									 rpoller->stat.qp_stats.recv.num_submitted_wrs);
+		spdk_json_write_named_uint64(w, "recv_doorbell_updates",
+									 rpoller->stat.qp_stats.recv.doorbell_updates);
+		spdk_json_write_object_end(w);
+	}
+
+	spdk_json_write_array_end(w);
+}
+
+const struct spdk_srv_transport_ops spdk_srv_transport_rdma = {
+	.name = "RDMA",
+	.type = SPDK_SRV_TRANSPORT_RDMA,
+	.opts_init = srv_rdma_opts_init,
+	.create = srv_rdma_create,
+	.dump_opts = srv_rdma_dump_opts,
+	.destroy = srv_rdma_destroy,
+
+	.listen = srv_rdma_listen,
+	.stop_listen = srv_rdma_stop_listen,
+
+	.poll_group_create = srv_rdma_poll_group_create,
+	.get_optimal_poll_group = srv_rdma_get_optimal_poll_group,
+	.poll_group_destroy = srv_rdma_poll_group_destroy,
+	.poll_group_add = srv_rdma_poll_group_add,
+	.poll_group_remove = srv_rdma_poll_group_remove,
+	.poll_group_poll = srv_rdma_poll_group_poll,
+
+	.req_free = srv_rdma_request_free,
+	.req_complete = srv_rdma_request_complete,
+
+	.conn_fini = srv_rdma_close_conn,
+	.conn_get_peer_trid = srv_rdma_conn_get_peer_trid,
+	.conn_get_local_trid = srv_rdma_conn_get_local_trid,
+	.conn_get_listen_trid = srv_rdma_conn_get_listen_trid,
+	.conn_abort_request = srv_rdma_conn_abort_request,
+
+	.poll_group_dump_stat = srv_rdma_poll_group_dump_stat,
+};
+
+SPDK_SRV_TRANSPORT_REGISTER(rdma, &spdk_srv_transport_rdma);
+SPDK_LOG_REGISTER_COMPONENT(rdma)
diff --git a/lib/rdma_server/server.c b/lib/rdma_server/server.c
new file mode 100644
index 000000000..a7ad1fb6d
--- /dev/null
+++ b/lib/rdma_server/server.c
@@ -0,0 +1,836 @@
+#include "spdk/stdinc.h"
+
+#include "spdk/bdev.h"
+#include "spdk/bit_array.h"
+#include "spdk/thread.h"
+#include "spdk/endian.h"
+#include "spdk/string.h"
+#include "spdk/util.h"
+#include "spdk/log.h"
+#include "spdk_internal/usdt.h"
+#include "spdk_internal/rdma_server.h"
+
+SPDK_LOG_REGISTER_COMPONENT(srv)
+
+static TAILQ_HEAD(, spdk_srv_tgt) g_srv_tgts = TAILQ_HEAD_INITIALIZER(g_srv_tgts);
+static struct spdk_thread *g_init_thread = NULL;
+static struct spdk_srv_tgt *g_srv_tgt = NULL;
+typedef void (*srv_conn_disconnect_cpl)(void *ctx, int status);
+static void srv_tgt_destroy_poll_group(void *io_device, void *ctx_buf);
+int srv_poll_group_add_transport(struct spdk_srv_poll_group *group,
+								 struct spdk_srv_transport *transport);
+
+/*
+ * There are several times when we need to iterate through the list of all conns and selectively delete them.
+ * In order to do this sequentially without overlap, we must provide a context to recover the next conn from
+ * to enable calling srv_conn_disconnect on the next desired conn.
+ */
+struct srv_conn_disconnect_many_ctx
+{
+	struct spdk_srv_subsystem *subsystem;
+	struct spdk_srv_poll_group *group;
+	spdk_srv_poll_group_mod_done cpl_fn;
+	void *cpl_ctx;
+	uint32_t count;
+};
+
+struct spdk_srv_tgt_add_transport_ctx
+{
+	struct spdk_srv_tgt *tgt;
+	struct spdk_srv_transport *transport;
+	spdk_srv_tgt_add_transport_done_fn cb_fn;
+	void *cb_arg;
+	int status;
+};
+
+struct spdk_srv_tgt_add_transport_cb_ctx
+{
+	struct spdk_srv_tgt_add_transport_ctx *ctx;
+	struct spdk_srv_poll_group *new_group;
+};
+
+static int
+srv_poll_group_poll(void *ctx)
+{
+	struct spdk_srv_poll_group *group = ctx;
+	int rc;
+	int count = 0;
+	struct spdk_srv_transport_poll_group *tgroup;
+
+	TAILQ_FOREACH(tgroup, &group->tgroups, link)
+	{
+		rc = srv_transport_poll_group_poll(tgroup);
+		if (rc < 0)
+		{
+			return SPDK_POLLER_BUSY;
+		}
+		count += rc;
+	}
+
+	return count > 0 ? SPDK_POLLER_BUSY : SPDK_POLLER_IDLE;
+}
+// TODO: 需要取消io_device的概念
+
+static void
+srv_tgt_create_poll_groups_done(void *args)
+{
+	int pool_groups_cnt = 0;
+	struct spdk_srv_poll_group *tmp;
+	struct spdk_srv_tgt_add_transport_cb_ctx *wapper = (struct spdk_srv_tgt_add_transport_cb_ctx *)args;
+
+	TAILQ_FOREACH(tmp, &wapper->ctx->tgt->poll_groups, link)
+	{
+		pool_groups_cnt++;
+	}
+	assert(pool_groups_cnt < spdk_env_get_core_count());
+
+	TAILQ_INSERT_TAIL(&wapper->ctx->tgt->poll_groups, wapper->new_group, link);
+
+	if (++pool_groups_cnt == spdk_env_get_core_count())
+	{
+		fprintf(stdout, "create targets's poll groups done\n");
+		(*wapper->ctx->cb_fn)(wapper->ctx->cb_arg, 0);
+		free(wapper->ctx);
+	}
+	free(wapper);
+	return;
+}
+
+static int
+srv_tgt_create_poll_group(void *args)
+{
+	struct spdk_srv_tgt_add_transport_ctx *ctx = (struct spdk_srv_tgt_add_transport_ctx *)args;
+	struct spdk_srv_tgt_add_transport_cb_ctx *ctx_wapper;
+	struct spdk_srv_tgt *tgt = ctx->tgt;
+	struct spdk_srv_poll_group *group;
+	struct spdk_srv_transport *transport = ctx->transport;
+	struct spdk_thread *thread = spdk_get_thread();
+	int rc;
+
+	group = calloc(1, sizeof(*group));
+	if (!group)
+	{
+		SPDK_ERRLOG("Failed to allocate memory for srv_tgt_create_poll_group\n");
+		exit(-1);
+		return;
+	}
+
+	TAILQ_INIT(&group->tgroups);
+	TAILQ_INIT(&group->conns);
+	group->thread = thread;
+
+	rc = srv_poll_group_add_transport(group, transport);
+	if (rc != 0)
+	{
+		return rc;
+	}
+
+	group->poller = SPDK_POLLER_REGISTER(srv_poll_group_poll, group, 0);
+
+	SPDK_DTRACE_PROBE1(srv_create_poll_group, spdk_thread_get_id(thread));
+
+	ctx_wapper = calloc(1, sizeof(*ctx_wapper));
+	if (!ctx_wapper)
+	{
+		SPDK_ERRLOG("Failed to allocate memory for srv_tgt_create_poll_group\n");
+		exit(-1);
+		return;
+	}
+
+	ctx_wapper->ctx = ctx;
+	ctx_wapper->new_group = group;
+	spdk_thread_send_msg(g_init_thread, srv_tgt_create_poll_groups_done, ctx_wapper);
+
+	return 0;
+}
+
+static void
+srv_tgt_destroy_poll_group(void *io_device, void *ctx_buf)
+{
+	struct spdk_srv_tgt *tgt = io_device;
+	struct spdk_srv_poll_group *group = ctx_buf;
+	struct spdk_srv_transport_poll_group *tgroup, *tmp;
+	struct spdk_srv_subsystem_poll_group *sgroup;
+	uint32_t sid, nsid;
+
+	SPDK_DTRACE_PROBE1(srv_destroy_poll_group, spdk_thread_get_id(group->thread));
+
+	pthread_mutex_lock(&tgt->mutex);
+	TAILQ_REMOVE(&tgt->poll_groups, group, link);
+	pthread_mutex_unlock(&tgt->mutex);
+
+	TAILQ_FOREACH_SAFE(tgroup, &group->tgroups, link, tmp)
+	{
+		TAILQ_REMOVE(&group->tgroups, tgroup, link);
+		srv_transport_poll_group_destroy(tgroup);
+	}
+
+	spdk_poller_unregister(&group->poller);
+
+	if (group->destroy_cb_fn)
+	{
+		group->destroy_cb_fn(group->destroy_cb_arg, 0);
+	}
+}
+
+static void
+_srv_tgt_disconnect_next_conn(void *ctx)
+{
+	struct spdk_srv_conn *conn;
+	struct srv_conn_disconnect_many_ctx *conn_ctx = ctx;
+	struct spdk_srv_poll_group *group = conn_ctx->group;
+	struct spdk_io_channel *ch;
+	int rc = 0;
+
+	conn = TAILQ_FIRST(&group->conns);
+
+	if (conn)
+	{
+		rc = spdk_srv_conn_disconnect(conn, _srv_tgt_disconnect_next_conn, ctx);
+	}
+
+	if (!conn || rc != 0)
+	{
+		/* When the refcount from the channels reaches 0, srv_tgt_destroy_poll_group will be called. */
+		ch = spdk_io_channel_from_ctx(group);
+		spdk_put_io_channel(ch);
+		free(conn_ctx);
+	}
+}
+
+static void
+srv_tgt_destroy_poll_group_conns(struct spdk_srv_poll_group *group)
+{
+	struct srv_conn_disconnect_many_ctx *ctx;
+
+	SPDK_DTRACE_PROBE1(srv_destroy_poll_group_conns, spdk_thread_get_id(group->thread));
+
+	ctx = calloc(1, sizeof(struct srv_conn_disconnect_many_ctx));
+	if (!ctx)
+	{
+		SPDK_ERRLOG("Failed to allocate memory for destroy poll group ctx\n");
+		return;
+	}
+
+	ctx->group = group;
+	_srv_tgt_disconnect_next_conn(ctx);
+}
+
+struct spdk_srv_tgt *
+spdk_srv_tgt_create(struct spdk_srv_target_opts *opts)
+{
+	struct spdk_srv_tgt *tgt, *tmp_tgt;
+	if (strnlen(opts->name, SRV_TGT_NAME_MAX_LENGTH) == SRV_TGT_NAME_MAX_LENGTH)
+	{
+		SPDK_ERRLOG("Provided target name exceeds the max length of %u.\n", SRV_TGT_NAME_MAX_LENGTH);
+		return NULL;
+	}
+
+	if (g_srv_tgt != NULL)
+	{
+		SPDK_ERRLOG("tgt already created \n");
+		return NULL;
+	}
+
+	TAILQ_FOREACH(tmp_tgt, &g_srv_tgts, link)
+	{
+		if (!strncmp(opts->name, tmp_tgt->name, SRV_TGT_NAME_MAX_LENGTH))
+		{
+			SPDK_ERRLOG("Provided target name must be unique.\n");
+			return NULL;
+		}
+	}
+
+	tgt = calloc(1, sizeof(*tgt));
+	if (!tgt)
+	{
+		return NULL;
+	}
+
+	snprintf(tgt->name, SRV_TGT_NAME_MAX_LENGTH, "%s", opts->name);
+
+	TAILQ_INIT(&tgt->transports);
+	TAILQ_INIT(&tgt->poll_groups);
+
+	pthread_mutex_init(&tgt->mutex, NULL);
+	// TODO:不需要注册IODEVICE，但是怎么去触发创建srv_tgt_create_poll_group呢？
+
+	// spdk_io_device_register(tgt,
+	// 			srv_tgt_create_poll_group,
+	// 			srv_tgt_destroy_poll_group,
+	// 			sizeof(struct spdk_srv_poll_group),
+	// 			tgt->name);
+
+	// tmp_pg = calloc(1, sizeof(*tmp_pg));
+	// if (!tmp_pg) {
+	// 	return NULL;
+	// }
+	// TODO:什么时候销毁呢？
+	g_init_thread = spdk_get_thread();
+	g_srv_tgt = tgt;
+	TAILQ_INSERT_HEAD(&g_srv_tgts, tgt, link);
+
+	return tgt;
+}
+
+static void
+_srv_tgt_destroy_next_transport(void *ctx)
+{
+	struct spdk_srv_tgt *tgt = ctx;
+	struct spdk_srv_transport *transport;
+
+	if (!TAILQ_EMPTY(&tgt->transports))
+	{
+		transport = TAILQ_FIRST(&tgt->transports);
+		TAILQ_REMOVE(&tgt->transports, transport, link);
+		spdk_srv_transport_destroy(transport, _srv_tgt_destroy_next_transport, tgt);
+	}
+	else
+	{
+		spdk_srv_tgt_destroy_done_fn *destroy_cb_fn = tgt->destroy_cb_fn;
+		void *destroy_cb_arg = tgt->destroy_cb_arg;
+
+		pthread_mutex_destroy(&tgt->mutex);
+		free(tgt);
+
+		if (destroy_cb_fn)
+		{
+			destroy_cb_fn(destroy_cb_arg, 0);
+		}
+	}
+}
+
+static void
+srv_tgt_destroy_cb(void *io_device)
+{
+	struct spdk_srv_tgt *tgt = io_device;
+	uint32_t i;
+	int rc;
+
+	_srv_tgt_destroy_next_transport(tgt);
+}
+
+void spdk_srv_tgt_destroy(struct spdk_srv_tgt *tgt,
+						  spdk_srv_tgt_destroy_done_fn cb_fn,
+						  void *cb_arg)
+{
+	tgt->destroy_cb_fn = cb_fn;
+	tgt->destroy_cb_arg = cb_arg;
+
+	TAILQ_REMOVE(&g_srv_tgts, tgt, link);
+
+	spdk_io_device_unregister(tgt, srv_tgt_destroy_cb);
+}
+
+static const char *
+spdk_srv_tgt_get_name(struct spdk_srv_tgt *tgt)
+{
+	return tgt->name;
+}
+
+struct spdk_srv_tgt *
+spdk_srv_get_tgt(const char *name)
+{
+	struct spdk_srv_tgt *tgt;
+	uint32_t num_targets = 0;
+
+	TAILQ_FOREACH(tgt, &g_srv_tgts, link)
+	{
+		if (name)
+		{
+			if (!strncmp(tgt->name, name, SRV_TGT_NAME_MAX_LENGTH))
+			{
+				return tgt;
+			}
+		}
+		num_targets++;
+	}
+
+	/*
+	 * special case. If there is only one target and
+	 * no name was specified, return the only available
+	 * target. If there is more than one target, name must
+	 * be specified.
+	 */
+	if (!name && num_targets == 1)
+	{
+		return TAILQ_FIRST(&g_srv_tgts);
+	}
+
+	return NULL;
+}
+
+static struct spdk_srv_tgt *
+spdk_srv_get_first_tgt(void)
+{
+	return TAILQ_FIRST(&g_srv_tgts);
+}
+
+static struct spdk_srv_tgt *
+spdk_srv_get_next_tgt(struct spdk_srv_tgt *prev)
+{
+	return TAILQ_NEXT(prev, link);
+}
+
+static void
+srv_listen_opts_copy(struct spdk_srv_listen_opts *opts,
+					 const struct spdk_srv_listen_opts *opts_src, size_t opts_size)
+{
+	assert(opts);
+	assert(opts_src);
+
+	opts->opts_size = opts_size;
+
+#define SET_FIELD(field)                                                                 \
+	if (offsetof(struct spdk_srv_listen_opts, field) + sizeof(opts->field) <= opts_size) \
+	{                                                                                    \
+		opts->field = opts_src->field;                                                   \
+	}
+
+	SET_FIELD(transport_specific);
+#undef SET_FIELD
+
+	/* Do not remove this statement, you should always update this statement when you adding a new field,
+	 * and do not forget to add the SET_FIELD statement for your added field. */
+	SPDK_STATIC_ASSERT(sizeof(struct spdk_srv_listen_opts) == 16, "Incorrect size");
+}
+
+static void
+spdk_srv_listen_opts_init(struct spdk_srv_listen_opts *opts, size_t opts_size)
+{
+	struct spdk_srv_listen_opts opts_local = {};
+
+	/* local version of opts should have defaults set here */
+
+	srv_listen_opts_copy(opts, &opts_local, opts_size);
+}
+
+static int
+spdk_srv_tgt_listen_ext(struct spdk_srv_tgt *tgt, const struct spdk_srv_transport_id *trid,
+						struct spdk_srv_listen_opts *opts)
+{
+	struct spdk_srv_transport *transport;
+	int rc;
+	struct spdk_srv_listen_opts opts_local = {};
+
+	if (!opts)
+	{
+		SPDK_ERRLOG("opts should not be NULL\n");
+		return -EINVAL;
+	}
+
+	if (!opts->opts_size)
+	{
+		SPDK_ERRLOG("The opts_size in opts structure should not be zero\n");
+		return -EINVAL;
+	}
+
+	transport = spdk_srv_tgt_get_transport(tgt, trid->trstring);
+	if (!transport)
+	{
+		SPDK_ERRLOG("Unable to find %s transport. The transport must be created first also make sure it is properly registered.\n",
+					trid->trstring);
+		return -EINVAL;
+	}
+
+	srv_listen_opts_copy(&opts_local, opts, opts->opts_size);
+	rc = spdk_srv_transport_listen(transport, trid, &opts_local);
+	if (rc < 0)
+	{
+		SPDK_ERRLOG("Unable to listen on address '%s'\n", trid->traddr);
+	}
+
+	return rc;
+}
+
+int spdk_srv_tgt_stop_listen(struct spdk_srv_tgt *tgt,
+							 struct spdk_srv_transport_id *trid)
+{
+	struct spdk_srv_transport *transport;
+	int rc;
+
+	transport = spdk_srv_tgt_get_transport(tgt, trid->trstring);
+	if (!transport)
+	{
+		SPDK_ERRLOG("Unable to find %s transport. The transport must be created first also make sure it is properly registered.\n",
+					trid->trstring);
+		return -EINVAL;
+	}
+
+	rc = spdk_srv_transport_stop_listen(transport, trid);
+	if (rc < 0)
+	{
+		SPDK_ERRLOG("Failed to stop listening on address '%s'\n", trid->traddr);
+		return rc;
+	}
+	return 0;
+}
+
+static void
+_srv_tgt_remove_transport_done(struct spdk_io_channel_iter *i, int status)
+{
+	struct spdk_srv_tgt_add_transport_ctx *ctx = spdk_io_channel_iter_get_ctx(i);
+
+	ctx->cb_fn(ctx->cb_arg, ctx->status);
+	free(ctx);
+}
+
+static void
+_srv_tgt_remove_transport(struct spdk_io_channel_iter *i)
+{
+	struct spdk_srv_tgt_add_transport_ctx *ctx = spdk_io_channel_iter_get_ctx(i);
+	struct spdk_io_channel *ch = spdk_io_channel_iter_get_channel(i);
+	struct spdk_srv_poll_group *group = spdk_io_channel_get_ctx(ch);
+	struct spdk_srv_transport_poll_group *tgroup, *tmp;
+
+	TAILQ_FOREACH_SAFE(tgroup, &group->tgroups, link, tmp)
+	{
+		if (tgroup->transport == ctx->transport)
+		{
+			TAILQ_REMOVE(&group->tgroups, tgroup, link);
+			srv_transport_poll_group_destroy(tgroup);
+		}
+	}
+
+	spdk_for_each_channel_continue(i, 0);
+}
+
+static void
+_srv_tgt_add_transport_done(struct spdk_io_channel_iter *i, int status)
+{
+	struct spdk_srv_tgt_add_transport_ctx *ctx = spdk_io_channel_iter_get_ctx(i);
+
+	if (status)
+	{
+		ctx->status = status;
+		spdk_for_each_channel(ctx->tgt,
+							  _srv_tgt_remove_transport,
+							  ctx,
+							  _srv_tgt_remove_transport_done);
+		return;
+	}
+
+	ctx->transport->tgt = ctx->tgt;
+	TAILQ_INSERT_TAIL(&ctx->tgt->transports, ctx->transport, link);
+	ctx->cb_fn(ctx->cb_arg, status);
+	free(ctx);
+}
+
+static void
+_srv_tgt_add_transport(struct spdk_io_channel_iter *i)
+{
+	struct spdk_srv_tgt_add_transport_ctx *ctx = spdk_io_channel_iter_get_ctx(i);
+	struct spdk_io_channel *ch = spdk_io_channel_iter_get_channel(i);
+	struct spdk_srv_poll_group *group = spdk_io_channel_get_ctx(ch);
+	int rc;
+
+	rc = srv_poll_group_add_transport(group, ctx->transport);
+	spdk_for_each_channel_continue(i, rc);
+}
+
+void spdk_srv_tgt_add_transport(struct spdk_srv_tgt *tgt,
+								struct spdk_srv_transport *transport,
+								spdk_srv_tgt_add_transport_done_fn cb_fn,
+								void *cb_arg)
+{
+	struct spdk_srv_tgt_add_transport_ctx *ctx;
+	struct spdk_srv_poll_group *group;
+	struct spdk_cpuset tmp_cpumask = {};
+	uint32_t i;
+	char thread_name[32];
+	struct spdk_thread *thread;
+	SPDK_DTRACE_PROBE2(srv_tgt_add_transport, transport, tgt->name);
+
+	if (spdk_srv_tgt_get_transport(tgt, transport->ops->name))
+	{
+		cb_fn(cb_arg, -EEXIST);
+		return; /* transport already created */
+	}
+
+	TAILQ_INSERT_TAIL(&tgt->transports, transport, link);
+
+	assert(g_init_thread != NULL);
+
+	ctx = calloc(1, sizeof(*ctx));
+	if (!ctx)
+	{
+		cb_fn(cb_arg, -ENOMEM);
+		return;
+	}
+
+	ctx->tgt = tgt;
+	ctx->transport = transport;
+	ctx->cb_fn = cb_fn;
+	ctx->cb_arg = cb_arg;
+
+	SPDK_ENV_FOREACH_CORE(i)
+	{
+		spdk_cpuset_zero(&tmp_cpumask);
+		spdk_cpuset_set_cpu(&tmp_cpumask, i, true);
+		snprintf(thread_name, sizeof(thread_name), "srv_tgt_poll_group_%u", i);
+
+		thread = spdk_thread_create(thread_name, &tmp_cpumask);
+		assert(thread != NULL);
+
+		spdk_thread_send_msg(thread, srv_tgt_create_poll_group, ctx);
+	}
+
+	return;
+}
+
+struct spdk_srv_transport *
+spdk_srv_tgt_get_transport(struct spdk_srv_tgt *tgt, const char *transport_name)
+{
+	struct spdk_srv_transport *transport;
+
+	TAILQ_FOREACH(transport, &tgt->transports, link)
+	{
+		if (!strncasecmp(transport->ops->name, transport_name, SPDK_SRV_TRSTRING_MAX_LEN))
+		{
+			return transport;
+		}
+	}
+	return NULL;
+}
+
+struct srv_new_conn_ctx
+{
+	struct spdk_srv_conn *conn;
+	struct spdk_srv_poll_group *group;
+};
+
+static int
+spdk_srv_poll_group_add(struct spdk_srv_poll_group *group,
+						struct spdk_srv_conn *conn)
+{
+	int rc = -1;
+	struct spdk_srv_transport_poll_group *tgroup;
+
+	TAILQ_INIT(&conn->outstanding);
+	conn->group = group;
+	conn->disconnect_started = false;
+
+	TAILQ_FOREACH(tgroup, &group->tgroups, link)
+	{
+		if (tgroup->transport == conn->transport)
+		{
+			rc = srv_transport_poll_group_add(tgroup, conn);
+			break;
+		}
+	}
+
+	/* We add the conn to the group only it is successfully added into the tgroup */
+	if (rc == 0)
+	{
+		SPDK_DTRACE_PROBE2(srv_poll_group_add_conn, conn, spdk_thread_get_id(group->thread));
+		TAILQ_INSERT_TAIL(&group->conns, conn, link);
+		srv_conn_set_state(conn, SPDK_SRV_CONN_ACTIVE);
+	}
+
+	return rc;
+}
+
+static void
+_srv_poll_group_add(void *_ctx)
+{
+	struct srv_new_conn_ctx *ctx = _ctx;
+	struct spdk_srv_conn *conn = ctx->conn;
+	struct spdk_srv_poll_group *group = ctx->group;
+
+	free(_ctx);
+
+	if (spdk_srv_poll_group_add(group, conn) != 0)
+	{
+		SPDK_ERRLOG("Unable to add the conn to a poll group.\n");
+		spdk_srv_conn_disconnect(conn, NULL, NULL);
+	}
+}
+
+static struct spdk_srv_poll_group *
+spdk_srv_get_optimal_poll_group(struct spdk_srv_conn *conn)
+{
+	struct spdk_srv_transport_poll_group *tgroup;
+
+	tgroup = srv_transport_get_optimal_poll_group(conn->transport, conn);
+
+	if (tgroup == NULL)
+	{
+		return NULL;
+	}
+
+	return tgroup->group;
+}
+
+void spdk_srv_tgt_new_conn(struct spdk_srv_tgt *tgt, struct spdk_srv_conn *conn)
+{
+	struct spdk_srv_poll_group *group;
+	struct srv_new_conn_ctx *ctx;
+
+	group = spdk_srv_get_optimal_poll_group(conn);
+	if (group == NULL)
+	{
+		if (tgt->next_poll_group == NULL)
+		{
+			tgt->next_poll_group = TAILQ_FIRST(&tgt->poll_groups);
+			if (tgt->next_poll_group == NULL)
+			{
+				SPDK_ERRLOG("No poll groups exist.\n");
+				spdk_srv_conn_disconnect(conn, NULL, NULL);
+				return;
+			}
+		}
+		group = tgt->next_poll_group;
+		tgt->next_poll_group = TAILQ_NEXT(group, link);
+	}
+
+	ctx = calloc(1, sizeof(*ctx));
+	if (!ctx)
+	{
+		SPDK_ERRLOG("Unable to send message to poll group.\n");
+		spdk_srv_conn_disconnect(conn, NULL, NULL);
+		return;
+	}
+
+	ctx->conn = conn;
+	ctx->group = group;
+
+	spdk_thread_send_msg(group->thread, _srv_poll_group_add, ctx);
+}
+
+struct spdk_srv_poll_group *
+spdk_srv_poll_group_create(struct spdk_srv_tgt *tgt)
+{
+	struct spdk_io_channel *ch;
+
+	ch = spdk_get_io_channel(tgt);
+	if (!ch)
+	{
+		SPDK_ERRLOG("Unable to get I/O channel for target\n");
+		return NULL;
+	}
+
+	return spdk_io_channel_get_ctx(ch);
+}
+
+void spdk_srv_poll_group_destroy(struct spdk_srv_poll_group *group,
+								 spdk_srv_poll_group_destroy_done_fn cb_fn,
+								 void *cb_arg)
+{
+	assert(group->destroy_cb_fn == NULL);
+	group->destroy_cb_fn = cb_fn;
+	group->destroy_cb_arg = cb_arg;
+
+	/* This function will put the io_channel associated with this poll group */
+	srv_tgt_destroy_poll_group_conns(group);
+}
+
+void spdk_srv_poll_group_remove(struct spdk_srv_conn *conn)
+{
+	struct spdk_srv_transport_poll_group *tgroup;
+	int rc;
+
+	SPDK_DTRACE_PROBE2(srv_poll_group_remove_conn, conn,
+					   spdk_thread_get_id(conn->group->thread));
+	srv_conn_set_state(conn, SPDK_SRV_CONN_ERROR);
+
+	/* Find the tgroup and remove the conn from the tgroup */
+	TAILQ_FOREACH(tgroup, &conn->group->tgroups, link)
+	{
+		if (tgroup->transport == conn->transport)
+		{
+			rc = srv_transport_poll_group_remove(tgroup, conn);
+			if (rc && (rc != ENOTSUP))
+			{
+				SPDK_ERRLOG("Cannot remove conn=%p from transport group=%p\n",
+							conn, tgroup);
+			}
+			break;
+		}
+	}
+
+	TAILQ_REMOVE(&conn->group->conns, conn, link);
+	conn->group = NULL;
+}
+
+static int
+spdk_srv_conn_get_peer_trid(struct spdk_srv_conn *conn,
+							struct spdk_srv_transport_id *trid)
+{
+	return srv_transport_conn_get_peer_trid(conn, trid);
+}
+
+static int
+spdk_srv_conn_get_local_trid(struct spdk_srv_conn *conn,
+							 struct spdk_srv_transport_id *trid)
+{
+	return srv_transport_conn_get_local_trid(conn, trid);
+}
+
+static int
+spdk_srv_conn_get_listen_trid(struct spdk_srv_conn *conn,
+							  struct spdk_srv_transport_id *trid)
+{
+	return srv_transport_conn_get_listen_trid(conn, trid);
+}
+
+int srv_poll_group_add_transport(struct spdk_srv_poll_group *group,
+								 struct spdk_srv_transport *transport)
+{
+	struct spdk_srv_transport_poll_group *tgroup;
+
+	TAILQ_FOREACH(tgroup, &group->tgroups, link)
+	{
+		if (tgroup->transport == transport)
+		{
+			/* Transport already in the poll group */
+			return 0;
+		}
+	}
+
+	tgroup = srv_transport_poll_group_create(transport);
+	if (!tgroup)
+	{
+		SPDK_ERRLOG("Unable to create poll group for transport\n");
+		return -1;
+	}
+	SPDK_DTRACE_PROBE2(srv_transport_poll_group_create, transport, spdk_thread_get_id(group->thread));
+
+	tgroup->group = group;
+	TAILQ_INSERT_TAIL(&group->tgroups, tgroup, link);
+
+	return 0;
+}
+
+void spdk_srv_poll_group_dump_stat(struct spdk_srv_poll_group *group, struct spdk_json_write_ctx *w)
+{
+	struct spdk_srv_transport_poll_group *tgroup;
+
+	spdk_json_write_object_begin(w);
+
+	spdk_json_write_named_string(w, "name", spdk_thread_get_name(spdk_get_thread()));
+	spdk_json_write_named_uint32(w, "conns", group->stat.conns);
+	spdk_json_write_named_uint32(w, "current_conns", group->stat.current_conns);
+	spdk_json_write_named_uint64(w, "pending_bdev_io", group->stat.pending_bdev_io);
+
+	spdk_json_write_named_array_begin(w, "transports");
+
+	TAILQ_FOREACH(tgroup, &group->tgroups, link)
+	{
+		spdk_json_write_object_begin(w);
+		/*
+		 * The trtype field intentionally contains a transport name as this is more informative.
+		 * The field has not been renamed for backward compatibility.
+		 */
+		spdk_json_write_named_string(w, "trtype", spdk_srv_get_transport_name(tgroup->transport));
+
+		if (tgroup->transport->ops->poll_group_dump_stat)
+		{
+			tgroup->transport->ops->poll_group_dump_stat(tgroup, w);
+		}
+
+		spdk_json_write_object_end(w);
+	}
+
+	spdk_json_write_array_end(w);
+	spdk_json_write_object_end(w);
+}
diff --git a/lib/rdma_server/transport_c.c b/lib/rdma_server/transport_c.c
new file mode 100644
index 000000000..03f7ff1c5
--- /dev/null
+++ b/lib/rdma_server/transport_c.c
@@ -0,0 +1,764 @@
+/*-
+ *   BSD LICENSE
+ *
+ *   Copyright (c) Intel Corporation.
+ *   All rights reserved.
+ *   Copyright (c) 2021 Mellanox Technologies LTD. All rights reserved.
+ *   Copyright (c) 2021, 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ *
+ *   Redistribution and use in source and binary forms, with or without
+ *   modification, are permitted provided that the following conditions
+ *   are met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in
+ *       the documentation and/or other materials provided with the
+ *       distribution.
+ *     * Neither the name of Intel Corporation nor the names of its
+ *       contributors may be used to endorse or promote products derived
+ *       from this software without specific prior written permission.
+ *
+ *   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ *   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ *   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ *   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ *   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ *   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ *   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ *   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ *   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ *   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ *   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/*
+ * Client transport abstraction
+ */
+
+#include "spdk_internal/rdma_client.h"
+#include "spdk/queue.h"
+
+#define SPDK_MAX_NUM_OF_TRANSPORTS 16
+
+struct spdk_client_transport
+{
+	struct spdk_client_transport_ops ops;
+	TAILQ_ENTRY(spdk_client_transport)
+	link;
+};
+
+TAILQ_HEAD(client_transport_list, spdk_client_transport)
+g_spdk_client_transports =
+	TAILQ_HEAD_INITIALIZER(g_spdk_client_transports);
+
+struct spdk_client_transport g_spdk_transports_dif[SPDK_MAX_NUM_OF_TRANSPORTS] = {};
+int g_current_transport_index_dif = 0;
+
+const struct spdk_client_transport *
+client_get_first_transport(void)
+{
+	return TAILQ_FIRST(&g_spdk_client_transports);
+}
+
+const struct spdk_client_transport *
+client_get_next_transport(const struct spdk_client_transport *transport)
+{
+	return TAILQ_NEXT(transport, link);
+}
+
+/*
+ * Unfortunately, due to Client PCIe multiprocess support, we cannot store the
+ * transport object in either the controller struct or the admin qpair. THis means
+ * that a lot of admin related transport calls will have to call client_get_transport
+ * in order to knwo which functions to call.
+ * In the I/O path, we have the ability to store the transport struct in the I/O
+ * qpairs to avoid taking a performance hit.
+ */
+const struct spdk_client_transport *
+client_get_transport(const char *transport_name)
+{
+	struct spdk_client_transport *registered_transport;
+
+	TAILQ_FOREACH(registered_transport, &g_spdk_client_transports, link)
+	{
+		if (strcasecmp(transport_name, registered_transport->ops.name) == 0)
+		{
+			return registered_transport;
+		}
+	}
+
+	return NULL;
+}
+
+bool spdk_client_transport_available(enum spdk_client_transport_type trtype)
+{
+	return client_get_transport(spdk_client_transport_id_trtype_str(trtype)) == NULL ? false : true;
+}
+
+bool spdk_client_transport_available_by_name(const char *transport_name)
+{
+	return client_get_transport(transport_name) == NULL ? false : true;
+}
+
+void spdk_client_transport_register(const struct spdk_client_transport_ops *ops)
+{
+	struct spdk_client_transport *new_transport;
+
+	if (client_get_transport(ops->name))
+	{
+		SPDK_ERRLOG("Double registering Client transport %s is prohibited.\n", ops->name);
+		assert(false);
+	}
+
+	if (g_current_transport_index_dif == SPDK_MAX_NUM_OF_TRANSPORTS)
+	{
+		SPDK_ERRLOG("Unable to register new Client transport.\n");
+		assert(false);
+		return;
+	}
+	new_transport = &g_spdk_transports_dif[g_current_transport_index_dif++];
+
+	new_transport->ops = *ops;
+	TAILQ_INSERT_TAIL(&g_spdk_client_transports, new_transport, link);
+}
+
+int get_random_str(char *random_str, const int random_len)
+{
+	int i, random_num, seed_str_len;
+	char seed_str[] = "abcdefghijklmnopqrstuvwxyz"
+					  "ABCDEFGHIJKLMNOPQRSTUVWXYZ";
+
+	seed_str_len = strlen(seed_str);
+
+	for (i = 0; i < random_len; i++)
+	{
+		random_num = rand() % seed_str_len;
+		random_str[i] = seed_str[random_num];
+	}
+
+	return 0;
+}
+
+struct spdk_client_ctrlr *client_transport_ctrlr_construct(const char *trstring,
+														   const struct spdk_client_ctrlr_opts *opts,
+														   void *devhandle)
+{
+	const struct spdk_client_transport *transport = client_get_transport(trstring);
+	struct spdk_client_ctrlr *ctrlr;
+
+	if (transport == NULL)
+	{
+		SPDK_ERRLOG("Transport %s doesn't exist.", trstring);
+		return NULL;
+	}
+	ctrlr = transport->ops.ctrlr_construct(opts, devhandle);
+	ctrlr->trtype = transport->ops.type;
+	strncpy(ctrlr->trstring, trstring, sizeof(ctrlr->trstring));
+	ctrlr->ioccsz_bytes = 8192;
+	ctrlr->icdoff = 0;
+	ctrlr->max_sges = client_transport_ctrlr_get_max_sges(ctrlr);
+	ctrlr->io_unit_size = ctrlr->opts.sector_size * ctrlr->opts.sectors_per_max_io / ctrlr->max_sges;
+
+	char str[8];
+	get_random_str(str, 8);
+    str[7] = '\0';
+
+	ctrlr->rpc_data_mp = spdk_mempool_create(str,
+											 SPDK_SRV_MEMORY_POOL_ELEMENT_SIZE, /* src + dst */
+											 ctrlr->io_unit_size,
+											 SPDK_MEMPOOL_DEFAULT_CACHE_SIZE,
+											 SPDK_ENV_SOCKET_ID_ANY);
+	assert(ctrlr->rpc_data_mp != NULL);
+	SPDK_INFOLOG(rdma, "create rpc data mem pool, item size = %d\n", ctrlr->io_unit_size);
+	return ctrlr;
+}
+
+int client_transport_ctrlr_destruct(struct spdk_client_ctrlr *ctrlr)
+{
+	const struct spdk_client_transport *transport = client_get_transport(ctrlr->trstring);
+
+	assert(transport != NULL);
+	return transport->ops.ctrlr_destruct(ctrlr);
+}
+
+int client_transport_ctrlr_enable(struct spdk_client_ctrlr *ctrlr)
+{
+	const struct spdk_client_transport *transport = client_get_transport(ctrlr->trstring);
+
+	assert(transport != NULL);
+	return transport->ops.ctrlr_enable(ctrlr);
+}
+
+uint32_t
+client_transport_ctrlr_get_max_xfer_size(struct spdk_client_ctrlr *ctrlr)
+{
+	const struct spdk_client_transport *transport = client_get_transport(ctrlr->trstring);
+
+	assert(transport != NULL);
+	return transport->ops.ctrlr_get_max_xfer_size(ctrlr);
+}
+
+uint16_t
+client_transport_ctrlr_get_max_sges(struct spdk_client_ctrlr *ctrlr)
+{
+	const struct spdk_client_transport *transport = client_get_transport(ctrlr->trstring);
+
+	assert(transport != NULL);
+	return transport->ops.ctrlr_get_max_sges(ctrlr);
+}
+
+int client_transport_ctrlr_reserve_cmb(struct spdk_client_ctrlr *ctrlr)
+{
+	const struct spdk_client_transport *transport = client_get_transport(ctrlr->trstring);
+
+	assert(transport != NULL);
+	if (transport->ops.ctrlr_reserve_cmb != NULL)
+	{
+		return transport->ops.ctrlr_reserve_cmb(ctrlr);
+	}
+
+	return -ENOTSUP;
+}
+
+void *
+client_transport_ctrlr_map_cmb(struct spdk_client_ctrlr *ctrlr, size_t *size)
+{
+	const struct spdk_client_transport *transport = client_get_transport(ctrlr->trstring);
+
+	assert(transport != NULL);
+	if (transport->ops.ctrlr_map_cmb != NULL)
+	{
+		return transport->ops.ctrlr_map_cmb(ctrlr, size);
+	}
+
+	return NULL;
+}
+
+int client_transport_ctrlr_unmap_cmb(struct spdk_client_ctrlr *ctrlr)
+{
+	const struct spdk_client_transport *transport = client_get_transport(ctrlr->trstring);
+
+	assert(transport != NULL);
+	if (transport->ops.ctrlr_unmap_cmb != NULL)
+	{
+		return transport->ops.ctrlr_unmap_cmb(ctrlr);
+	}
+
+	return 0;
+}
+
+int client_transport_ctrlr_enable_pmr(struct spdk_client_ctrlr *ctrlr)
+{
+	const struct spdk_client_transport *transport = client_get_transport(ctrlr->trstring);
+
+	assert(transport != NULL);
+	if (transport->ops.ctrlr_enable_pmr != NULL)
+	{
+		return transport->ops.ctrlr_enable_pmr(ctrlr);
+	}
+
+	return -ENOSYS;
+}
+
+int client_transport_ctrlr_disable_pmr(struct spdk_client_ctrlr *ctrlr)
+{
+	const struct spdk_client_transport *transport = client_get_transport(ctrlr->trstring);
+
+	assert(transport != NULL);
+	if (transport->ops.ctrlr_disable_pmr != NULL)
+	{
+		return transport->ops.ctrlr_disable_pmr(ctrlr);
+	}
+
+	return -ENOSYS;
+}
+
+void *
+client_transport_ctrlr_map_pmr(struct spdk_client_ctrlr *ctrlr, size_t *size)
+{
+	const struct spdk_client_transport *transport = client_get_transport(ctrlr->trstring);
+
+	assert(transport != NULL);
+	if (transport->ops.ctrlr_map_pmr != NULL)
+	{
+		return transport->ops.ctrlr_map_pmr(ctrlr, size);
+	}
+
+	return NULL;
+}
+
+int client_transport_ctrlr_unmap_pmr(struct spdk_client_ctrlr *ctrlr)
+{
+	const struct spdk_client_transport *transport = client_get_transport(ctrlr->trstring);
+
+	assert(transport != NULL);
+	if (transport->ops.ctrlr_unmap_pmr != NULL)
+	{
+		return transport->ops.ctrlr_unmap_pmr(ctrlr);
+	}
+
+	return -ENOSYS;
+}
+
+struct spdk_client_qpair *
+client_transport_ctrlr_create_io_qpair(struct spdk_client_ctrlr *ctrlr, uint16_t qid,
+									   const struct spdk_client_io_qpair_opts *opts)
+{
+	struct spdk_client_qpair *qpair;
+	const struct spdk_client_transport *transport = client_get_transport(ctrlr->trstring);
+
+	assert(transport != NULL);
+	qpair = transport->ops.ctrlr_create_io_qpair(ctrlr, qid, opts);
+
+	if (qpair != NULL)
+	{
+		qpair->transport = transport;
+	}
+
+	return qpair;
+}
+
+void client_transport_ctrlr_delete_io_qpair(struct spdk_client_ctrlr *ctrlr, struct spdk_client_qpair *qpair)
+{
+	const struct spdk_client_transport *transport = client_get_transport(ctrlr->trstring);
+	int rc;
+
+	assert(transport != NULL);
+
+	/* Do not rely on qpair->transport.  For multi-process cases, a foreign process may delete
+	 * the IO qpair, in which case the transport object would be invalid (each process has their
+	 * own unique transport objects since they contain function pointers).  So we look up the
+	 * transport object in the delete_io_qpair case.
+	 */
+	rc = transport->ops.ctrlr_delete_io_qpair(ctrlr, qpair);
+	if (rc != 0)
+	{
+		SPDK_ERRLOG("transport %s returned non-zero for ctrlr_delete_io_qpair op\n",
+					transport->ops.name);
+		assert(false);
+	}
+}
+
+static void
+client_transport_connect_qpair_fail(struct spdk_client_qpair *qpair, void *unused)
+{
+	struct spdk_client_ctrlr *ctrlr = qpair->ctrlr;
+
+	/* If the qpair was unable to reconnect, restore the original failure reason */
+	qpair->transport_failure_reason = qpair->last_transport_failure_reason;
+	client_transport_ctrlr_disconnect_qpair(ctrlr, qpair);
+}
+
+int client_transport_ctrlr_connect_qpair(struct spdk_client_ctrlr *ctrlr, struct spdk_client_qpair *qpair)
+{
+	const struct spdk_client_transport *transport = client_get_transport(ctrlr->trstring);
+	int rc;
+
+	assert(transport != NULL);
+
+	qpair->transport = transport;
+	qpair->last_transport_failure_reason = qpair->transport_failure_reason;
+	qpair->transport_failure_reason = SPDK_CLIENT_QPAIR_FAILURE_NONE;
+
+	client_qpair_set_state(qpair, CLIENT_QPAIR_CONNECTING);
+	rc = transport->ops.ctrlr_connect_qpair(ctrlr, qpair);
+	if (rc != 0)
+	{
+		goto err;
+	}
+
+	if (qpair->poll_group)
+	{
+		rc = client_poll_group_connect_qpair(qpair);
+		if (rc)
+		{
+			goto err;
+		}
+	}
+
+	if (!qpair->async)
+	{
+		/* Busy wait until the qpair exits the connecting state */
+		while (client_qpair_get_state(qpair) == CLIENT_QPAIR_CONNECTING)
+		{
+			if (qpair->poll_group)
+			{
+				rc = spdk_client_poll_group_process_completions(
+					qpair->poll_group->group, 0,
+					client_transport_connect_qpair_fail);
+			}
+			else
+			{
+				rc = spdk_client_qpair_process_completions(qpair, 0);
+			}
+
+			if (rc < 0)
+			{
+				goto err;
+			}
+		}
+	}
+
+	return 0;
+err:
+	client_transport_connect_qpair_fail(qpair, NULL);
+	if (client_qpair_get_state(qpair) == CLIENT_QPAIR_DISCONNECTING)
+	{
+		assert(qpair->async == true);
+		/* Let the caller to poll the qpair until it is actually disconnected. */
+		return 0;
+	}
+
+	return rc;
+}
+
+int client_transport_ctrlr_connect_qpair_async(struct spdk_client_ctrlr *ctrlr, struct spdk_client_qpair *qpair)
+{
+	const struct spdk_client_transport *transport = client_get_transport(ctrlr->trstring);
+	int rc;
+
+	assert(transport != NULL);
+
+	qpair->transport = transport;
+	qpair->last_transport_failure_reason = qpair->transport_failure_reason;
+	qpair->transport_failure_reason = SPDK_CLIENT_QPAIR_FAILURE_NONE;
+
+	client_qpair_set_state(qpair, CLIENT_QPAIR_CONNECTING);
+	rc = transport->ops.ctrlr_connect_qpair(ctrlr, qpair);
+	if (rc != 0)
+	{
+		goto err;
+	}
+
+	if (qpair->poll_group)
+	{
+		rc = client_poll_group_connect_qpair(qpair);
+		if (rc)
+		{
+			goto err;
+		}
+	}
+
+	if (!qpair->async)
+	{
+		/* Busy wait until the qpair exits the connecting state */
+		if (client_qpair_get_state(qpair) == CLIENT_QPAIR_CONNECTING)
+		{
+			if (qpair->poll_group)
+			{
+				rc = spdk_client_poll_group_process_completions(
+					qpair->poll_group->group, 0,
+					client_transport_connect_qpair_fail);
+			}
+			else
+			{
+				rc = spdk_client_qpair_process_completions(qpair, 0);
+			}
+
+			if (rc < 0)
+			{
+				goto err;
+			}
+		}
+	}
+
+	return 0;
+err:
+	client_transport_connect_qpair_fail(qpair, NULL);
+	if (client_qpair_get_state(qpair) == CLIENT_QPAIR_DISCONNECTING)
+	{
+		assert(qpair->async == true);
+		/* Let the caller to poll the qpair until it is actually disconnected. */
+		return 0;
+	}
+
+	return rc;
+}
+
+void client_transport_ctrlr_disconnect_qpair(struct spdk_client_ctrlr *ctrlr, struct spdk_client_qpair *qpair)
+{
+	const struct spdk_client_transport *transport = client_get_transport(ctrlr->trstring);
+
+	if (client_qpair_get_state(qpair) == CLIENT_QPAIR_DISCONNECTING ||
+		client_qpair_get_state(qpair) == CLIENT_QPAIR_DISCONNECTED)
+	{
+		return;
+	}
+
+	client_qpair_set_state(qpair, CLIENT_QPAIR_DISCONNECTING);
+	assert(transport != NULL);
+	if (qpair->poll_group)
+	{
+		client_poll_group_disconnect_qpair(qpair);
+	}
+
+	transport->ops.ctrlr_disconnect_qpair(ctrlr, qpair);
+
+	client_qpair_abort_all_queued_reqs(qpair, 0);
+	client_transport_qpair_abort_reqs(qpair, 0);
+	client_qpair_set_state(qpair, CLIENT_QPAIR_DISCONNECTED);
+}
+
+int client_transport_ctrlr_get_memory_domains(const struct spdk_client_ctrlr *ctrlr,
+											  struct spdk_memory_domain **domains, int array_size)
+{
+	const struct spdk_client_transport *transport = client_get_transport(ctrlr->trstring);
+
+	assert(transport != NULL);
+	if (transport->ops.ctrlr_get_memory_domains)
+	{
+		return transport->ops.ctrlr_get_memory_domains(ctrlr, domains, array_size);
+	}
+
+	return 0;
+}
+
+void client_transport_qpair_abort_reqs(struct spdk_client_qpair *qpair, uint32_t dnr)
+{
+	const struct spdk_client_transport *transport;
+
+	assert(dnr <= 1);
+	if (spdk_likely(!client_qpair_is_admin_queue(qpair)))
+	{
+		qpair->transport->ops.qpair_abort_reqs(qpair, dnr);
+	}
+	else
+	{
+		transport = client_get_transport(qpair->ctrlr->trstring);
+		assert(transport != NULL);
+		transport->ops.qpair_abort_reqs(qpair, dnr);
+	}
+}
+
+int client_transport_qpair_reset(struct spdk_client_qpair *qpair)
+{
+	const struct spdk_client_transport *transport;
+
+	if (spdk_likely(!client_qpair_is_admin_queue(qpair)))
+	{
+		return qpair->transport->ops.qpair_reset(qpair);
+	}
+
+	transport = client_get_transport(qpair->ctrlr->trstring);
+	assert(transport != NULL);
+	return transport->ops.qpair_reset(qpair);
+}
+
+int client_transport_qpair_submit_request(struct spdk_client_qpair *qpair, struct client_request *req)
+{
+	const struct spdk_client_transport *transport;
+
+	if (spdk_likely(!client_qpair_is_admin_queue(qpair)))
+	{
+		return qpair->transport->ops.qpair_submit_request(qpair, req);
+	}
+
+	transport = client_get_transport(qpair->ctrlr->trstring);
+	assert(transport != NULL);
+	return transport->ops.qpair_submit_request(qpair, req);
+}
+
+int32_t
+client_transport_qpair_process_completions(struct spdk_client_qpair *qpair, uint32_t max_completions)
+{
+	const struct spdk_client_transport *transport;
+
+	if (spdk_likely(!client_qpair_is_admin_queue(qpair)))
+	{
+		return qpair->transport->ops.qpair_process_completions(qpair, max_completions);
+	}
+
+	transport = client_get_transport(qpair->ctrlr->trstring);
+	assert(transport != NULL);
+	return transport->ops.qpair_process_completions(qpair, max_completions);
+}
+
+int client_transport_qpair_iterate_requests(struct spdk_client_qpair *qpair,
+											int (*iter_fn)(struct client_request *req, void *arg),
+											void *arg)
+{
+	const struct spdk_client_transport *transport;
+
+	if (spdk_likely(!client_qpair_is_admin_queue(qpair)))
+	{
+		return qpair->transport->ops.qpair_iterate_requests(qpair, iter_fn, arg);
+	}
+
+	transport = client_get_transport(qpair->ctrlr->trstring);
+	assert(transport != NULL);
+	return transport->ops.qpair_iterate_requests(qpair, iter_fn, arg);
+}
+
+void client_transport_admin_qpair_abort_aers(struct spdk_client_qpair *qpair)
+{
+	const struct spdk_client_transport *transport = client_get_transport(qpair->ctrlr->trstring);
+
+	assert(transport != NULL);
+	transport->ops.admin_qpair_abort_aers(qpair);
+}
+
+struct spdk_client_transport_poll_group *
+client_transport_poll_group_create(const struct spdk_client_transport *transport)
+{
+	struct spdk_client_transport_poll_group *group = NULL;
+
+	group = transport->ops.poll_group_create();
+	if (group)
+	{
+		group->transport = transport;
+		STAILQ_INIT(&group->connected_qpairs);
+		STAILQ_INIT(&group->disconnected_qpairs);
+	}
+
+	return group;
+}
+
+struct spdk_client_transport_poll_group *
+client_transport_qpair_get_optimal_poll_group(const struct spdk_client_transport *transport,
+											  struct spdk_client_qpair *qpair)
+{
+	if (transport->ops.qpair_get_optimal_poll_group)
+	{
+		return transport->ops.qpair_get_optimal_poll_group(qpair);
+	}
+	else
+	{
+		return NULL;
+	}
+}
+
+int client_transport_poll_group_add(struct spdk_client_transport_poll_group *tgroup,
+									struct spdk_client_qpair *qpair)
+{
+	int rc;
+
+	rc = tgroup->transport->ops.poll_group_add(tgroup, qpair);
+	if (rc == 0)
+	{
+		qpair->poll_group = tgroup;
+		assert(client_qpair_get_state(qpair) < CLIENT_QPAIR_CONNECTED);
+		qpair->poll_group_tailq_head = &tgroup->disconnected_qpairs;
+		STAILQ_INSERT_TAIL(&tgroup->disconnected_qpairs, qpair, poll_group_stailq);
+	}
+
+	return rc;
+}
+
+int client_transport_poll_group_remove(struct spdk_client_transport_poll_group *tgroup,
+									   struct spdk_client_qpair *qpair)
+{
+	int rc __attribute__((unused));
+
+	if (qpair->poll_group_tailq_head == &tgroup->connected_qpairs)
+	{
+		return -EINVAL;
+	}
+	else if (qpair->poll_group_tailq_head != &tgroup->disconnected_qpairs)
+	{
+		return -ENOENT;
+	}
+
+	rc = tgroup->transport->ops.poll_group_remove(tgroup, qpair);
+	assert(rc == 0);
+
+	STAILQ_REMOVE(&tgroup->disconnected_qpairs, qpair, spdk_client_qpair, poll_group_stailq);
+
+	qpair->poll_group = NULL;
+	qpair->poll_group_tailq_head = NULL;
+
+	return 0;
+}
+
+int64_t
+client_transport_poll_group_process_completions(struct spdk_client_transport_poll_group *tgroup,
+												uint32_t completions_per_qpair, spdk_client_disconnected_qpair_cb disconnected_qpair_cb)
+{
+	return tgroup->transport->ops.poll_group_process_completions(tgroup, completions_per_qpair,
+																 disconnected_qpair_cb);
+}
+
+int client_transport_poll_group_destroy(struct spdk_client_transport_poll_group *tgroup)
+{
+	return tgroup->transport->ops.poll_group_destroy(tgroup);
+}
+
+int client_transport_poll_group_disconnect_qpair(struct spdk_client_qpair *qpair)
+{
+	struct spdk_client_transport_poll_group *tgroup;
+	int rc __attribute__((unused));
+
+	tgroup = qpair->poll_group;
+
+	if (qpair->poll_group_tailq_head == &tgroup->disconnected_qpairs)
+	{
+		return 0;
+	}
+
+	if (qpair->poll_group_tailq_head == &tgroup->connected_qpairs)
+	{
+		rc = tgroup->transport->ops.poll_group_disconnect_qpair(qpair);
+		assert(rc == 0);
+
+		qpair->poll_group_tailq_head = &tgroup->disconnected_qpairs;
+		STAILQ_REMOVE(&tgroup->connected_qpairs, qpair, spdk_client_qpair, poll_group_stailq);
+		STAILQ_INSERT_TAIL(&tgroup->disconnected_qpairs, qpair, poll_group_stailq);
+
+		return 0;
+	}
+
+	return -EINVAL;
+}
+
+int client_transport_poll_group_connect_qpair(struct spdk_client_qpair *qpair)
+{
+	struct spdk_client_transport_poll_group *tgroup;
+	int rc;
+
+	tgroup = qpair->poll_group;
+
+	if (qpair->poll_group_tailq_head == &tgroup->connected_qpairs)
+	{
+		return 0;
+	}
+
+	if (qpair->poll_group_tailq_head == &tgroup->disconnected_qpairs)
+	{
+		rc = tgroup->transport->ops.poll_group_connect_qpair(qpair);
+		if (rc == 0)
+		{
+			qpair->poll_group_tailq_head = &tgroup->connected_qpairs;
+			STAILQ_REMOVE(&tgroup->disconnected_qpairs, qpair, spdk_client_qpair, poll_group_stailq);
+			STAILQ_INSERT_TAIL(&tgroup->connected_qpairs, qpair, poll_group_stailq);
+		}
+
+		return rc == -EINPROGRESS ? 0 : rc;
+	}
+
+	return -EINVAL;
+}
+
+int client_transport_poll_group_get_stats(struct spdk_client_transport_poll_group *tgroup,
+										  struct spdk_client_transport_poll_group_stat **stats)
+{
+	if (tgroup->transport->ops.poll_group_get_stats)
+	{
+		return tgroup->transport->ops.poll_group_get_stats(tgroup, stats);
+	}
+	return -ENOTSUP;
+}
+
+void client_transport_poll_group_free_stats(struct spdk_client_transport_poll_group *tgroup,
+											struct spdk_client_transport_poll_group_stat *stats)
+{
+	if (tgroup->transport->ops.poll_group_free_stats)
+	{
+		tgroup->transport->ops.poll_group_free_stats(tgroup, stats);
+	}
+}
+
+enum spdk_client_transport_type client_transport_get_trtype(const struct spdk_client_transport *transport)
+{
+	return transport->ops.type;
+}
diff --git a/lib/rdma_server/transport_s.c b/lib/rdma_server/transport_s.c
new file mode 100644
index 000000000..d750e44fd
--- /dev/null
+++ b/lib/rdma_server/transport_s.c
@@ -0,0 +1,900 @@
+#include "spdk/stdinc.h"
+#include "spdk/config.h"
+#include "spdk/log.h"
+#include "spdk/queue.h"
+#include "spdk/util.h"
+#include "spdk/thread.h"
+#include "spdk_internal/usdt.h"
+#include "spdk_internal/rdma_server.h"
+
+#define MAX_MEMPOOL_NAME_LENGTH 40
+struct srv_transport_ops_list_element
+{
+	struct spdk_srv_transport_ops ops;
+	TAILQ_ENTRY(srv_transport_ops_list_element)
+	link;
+};
+
+TAILQ_HEAD(srv_transport_ops_list, srv_transport_ops_list_element)
+g_spdk_srv_transport_ops = TAILQ_HEAD_INITIALIZER(g_spdk_srv_transport_ops);
+
+static inline const struct spdk_srv_transport_ops *
+srv_get_transport_ops(const char *transport_name)
+{
+	struct srv_transport_ops_list_element *ops;
+	TAILQ_FOREACH(ops, &g_spdk_srv_transport_ops, link)
+	{
+		if (strcasecmp(transport_name, ops->ops.name) == 0)
+		{
+			return &ops->ops;
+		}
+	}
+	return NULL;
+}
+
+void spdk_srv_transport_register(const struct spdk_srv_transport_ops *ops)
+{
+	struct srv_transport_ops_list_element *new_ops;
+
+	if (srv_get_transport_ops(ops->name) != NULL)
+	{
+		SPDK_ERRLOG("Double registering srv transport type %s.\n", ops->name);
+		assert(false);
+		return;
+	}
+
+	new_ops = calloc(1, sizeof(*new_ops));
+	if (new_ops == NULL)
+	{
+		SPDK_ERRLOG("Unable to allocate memory to register new transport type %s.\n", ops->name);
+		assert(false);
+		return;
+	}
+
+	new_ops->ops = *ops;
+
+	TAILQ_INSERT_TAIL(&g_spdk_srv_transport_ops, new_ops, link);
+}
+
+const struct spdk_srv_transport_opts *
+spdk_srv_get_transport_opts(struct spdk_srv_transport *transport)
+{
+	return &transport->opts;
+}
+
+spdk_srv_transport_type_t spdk_srv_get_transport_type(struct spdk_srv_transport *transport)
+{
+	return transport->ops->type;
+}
+
+const char *
+spdk_srv_get_transport_name(struct spdk_srv_transport *transport)
+{
+	return transport->ops->name;
+}
+
+static void srv_transport_opts_copy(struct spdk_srv_transport_opts *opts,
+									struct spdk_srv_transport_opts *opts_src,
+									size_t opts_size)
+{
+	assert(opts);
+	assert(opts_src);
+
+	opts->opts_size = opts_size;
+
+#define SET_FIELD(field)                                                                    \
+	if (offsetof(struct spdk_srv_transport_opts, field) + sizeof(opts->field) <= opts_size) \
+	{                                                                                       \
+		opts->field = opts_src->field;                                                      \
+	}
+
+	SET_FIELD(max_queue_depth);
+	SET_FIELD(in_capsule_data_size);
+	SET_FIELD(max_io_size);
+	SET_FIELD(io_unit_size);
+	SET_FIELD(max_aq_depth);
+	SET_FIELD(buf_cache_size);
+	SET_FIELD(num_shared_buffers);
+	SET_FIELD(dif_insert_or_strip);
+	SET_FIELD(abort_timeout_sec);
+	SET_FIELD(association_timeout);
+	SET_FIELD(transport_specific);
+	SET_FIELD(acceptor_poll_rate);
+	SET_FIELD(zcopy);
+
+	/* Do not remove this statement, you should always update this statement when you adding a new field,
+	 * and do not forget to add the SET_FIELD statement for your added field. */
+	SPDK_STATIC_ASSERT(sizeof(struct spdk_srv_transport_opts) == 64, "Incorrect size");
+
+#undef SET_FIELD
+#undef FILED_CHECK
+}
+
+struct spdk_srv_transport *
+spdk_srv_transport_create(const char *transport_name, struct spdk_srv_transport_opts *opts)
+{
+	const struct spdk_srv_transport_ops *ops = NULL;
+	struct spdk_srv_transport *transport;
+	char spdk_mempool_name[MAX_MEMPOOL_NAME_LENGTH];
+	int chars_written;
+	struct spdk_srv_transport_opts opts_local = {};
+
+	if (!opts)
+	{
+		SPDK_ERRLOG("opts should not be NULL\n");
+		return NULL;
+	}
+
+	if (!opts->opts_size)
+	{
+		SPDK_ERRLOG("The opts_size in opts structure should not be zero\n");
+		return NULL;
+	}
+
+	ops = srv_get_transport_ops(transport_name);
+	if (!ops)
+	{
+		SPDK_ERRLOG("Transport type '%s' unavailable.\n", transport_name);
+		return NULL;
+	}
+	srv_transport_opts_copy(&opts_local, opts, opts->opts_size);
+
+	if (opts_local.max_io_size != 0 && (!spdk_u32_is_pow2(opts_local.max_io_size) ||
+										opts_local.max_io_size < 8192))
+	{
+		SPDK_ERRLOG("max_io_size %u must be a power of 2 and be greater than or equal 8KB\n",
+					opts_local.max_io_size);
+		return NULL;
+	}
+
+	transport = ops->create(&opts_local);
+	if (!transport)
+	{
+		SPDK_ERRLOG("Unable to create new transport of type %s\n", transport_name);
+		return NULL;
+	}
+
+	TAILQ_INIT(&transport->listeners);
+
+	transport->ops = ops;
+	transport->opts = opts_local;
+
+	chars_written = snprintf(spdk_mempool_name, MAX_MEMPOOL_NAME_LENGTH, "%s_%s_%s", "spdk_srv",
+							 transport_name, "data");
+	if (chars_written < 0)
+	{
+		SPDK_ERRLOG("Unable to generate transport data buffer pool name.\n");
+		ops->destroy(transport, NULL, NULL);
+		return NULL;
+	}
+
+	if (opts_local.num_shared_buffers)
+	{
+		transport->data_buf_pool = spdk_mempool_create(spdk_mempool_name,
+													   opts_local.num_shared_buffers,
+													   opts_local.io_unit_size + SRV_DATA_BUFFER_ALIGNMENT,
+													   SPDK_MEMPOOL_DEFAULT_CACHE_SIZE,
+													   SPDK_ENV_SOCKET_ID_ANY);
+
+		if (!transport->data_buf_pool)
+		{
+			SPDK_ERRLOG("Unable to allocate buffer pool for poll group\n");
+			ops->destroy(transport, NULL, NULL);
+			return NULL;
+		}
+	}
+
+	return transport;
+}
+
+struct spdk_srv_transport *
+spdk_srv_transport_get_next(struct spdk_srv_transport *transport)
+{
+	return TAILQ_NEXT(transport, link);
+}
+
+int spdk_srv_transport_destroy(struct spdk_srv_transport *transport,
+							   spdk_srv_transport_destroy_done_cb cb_fn, void *cb_arg)
+{
+	struct spdk_srv_listener *listener, *listener_tmp;
+
+	if (transport->data_buf_pool != NULL)
+	{
+		if (spdk_mempool_count(transport->data_buf_pool) !=
+			transport->opts.num_shared_buffers)
+		{
+			SPDK_ERRLOG("transport buffer pool count is %zu but should be %u\n",
+						spdk_mempool_count(transport->data_buf_pool),
+						transport->opts.num_shared_buffers);
+		}
+		spdk_mempool_free(transport->data_buf_pool);
+	}
+
+	TAILQ_FOREACH_SAFE(listener, &transport->listeners, link, listener_tmp)
+	{
+		TAILQ_REMOVE(&transport->listeners, listener, link);
+		transport->ops->stop_listen(transport, &listener->trid);
+		free(listener);
+	}
+
+	return transport->ops->destroy(transport, cb_fn, cb_arg);
+}
+
+struct spdk_srv_listener *
+srv_transport_find_listener(struct spdk_srv_transport *transport,
+							const struct spdk_srv_transport_id *trid)
+{
+	struct spdk_srv_listener *listener;
+
+	TAILQ_FOREACH(listener, &transport->listeners, link)
+	{
+		if (spdk_srv_transport_id_compare(&listener->trid, trid) == 0)
+		{
+			return listener;
+		}
+	}
+
+	return NULL;
+}
+
+int spdk_srv_transport_listen(struct spdk_srv_transport *transport,
+							  const struct spdk_srv_transport_id *trid, struct spdk_srv_listen_opts *opts)
+
+{
+	struct spdk_srv_listener *listener;
+	int rc;
+
+	listener = srv_transport_find_listener(transport, trid);
+	if (!listener)
+	{
+		listener = calloc(1, sizeof(*listener));
+		if (!listener)
+		{
+			return -ENOMEM;
+		}
+
+		listener->ref = 1;
+		listener->trid = *trid;
+		TAILQ_INSERT_TAIL(&transport->listeners, listener, link);
+		rc = transport->ops->listen(transport, &listener->trid, opts);
+		if (rc != 0)
+		{
+			TAILQ_REMOVE(&transport->listeners, listener, link);
+			free(listener);
+		}
+		return rc;
+	}
+
+	++listener->ref;
+
+	return 0;
+}
+
+int spdk_srv_transport_stop_listen(struct spdk_srv_transport *transport,
+								   const struct spdk_srv_transport_id *trid)
+{
+	struct spdk_srv_listener *listener;
+
+	listener = srv_transport_find_listener(transport, trid);
+	if (!listener)
+	{
+		return -ENOENT;
+	}
+
+	if (--listener->ref == 0)
+	{
+		TAILQ_REMOVE(&transport->listeners, listener, link);
+		transport->ops->stop_listen(transport, trid);
+		free(listener);
+	}
+
+	return 0;
+}
+
+struct spdk_srv_transport_poll_group *
+srv_transport_poll_group_create(struct spdk_srv_transport *transport)
+{
+	struct spdk_srv_transport_poll_group *group;
+	struct spdk_srv_transport_pg_cache_buf **bufs;
+	uint32_t i;
+
+	group = transport->ops->poll_group_create(transport);
+	if (!group)
+	{
+		return NULL;
+	}
+	group->transport = transport;
+
+	STAILQ_INIT(&group->pending_buf_queue);
+	STAILQ_INIT(&group->buf_cache);
+
+	if (transport->opts.buf_cache_size)
+	{
+		group->buf_cache_size = transport->opts.buf_cache_size;
+		bufs = calloc(group->buf_cache_size, sizeof(struct spdk_srv_transport_pg_cache_buf *));
+
+		if (!bufs)
+		{
+			SPDK_ERRLOG("Memory allocation failed, can't reserve buffers for the pg buffer cache\n");
+			return group;
+		}
+
+		if (spdk_mempool_get_bulk(transport->data_buf_pool, (void **)bufs, group->buf_cache_size))
+		{
+			group->buf_cache_size = (uint32_t)spdk_mempool_count(transport->data_buf_pool);
+			SPDK_NOTICELOG("Unable to reserve the full number of buffers for the pg buffer cache. "
+						   "Decrease the number of cached buffers from %u to %u\n",
+						   transport->opts.buf_cache_size, group->buf_cache_size);
+			/* Sanity check */
+			assert(group->buf_cache_size <= transport->opts.buf_cache_size);
+			/* Try again with less number of buffers */
+			if (spdk_mempool_get_bulk(transport->data_buf_pool, (void **)bufs, group->buf_cache_size))
+			{
+				SPDK_NOTICELOG("Failed to reserve %u buffers\n", group->buf_cache_size);
+				group->buf_cache_size = 0;
+			}
+		}
+
+		for (i = 0; i < group->buf_cache_size; i++)
+		{
+			STAILQ_INSERT_HEAD(&group->buf_cache, bufs[i], link);
+		}
+		group->buf_cache_count = group->buf_cache_size;
+
+		free(bufs);
+	}
+
+	return group;
+}
+
+struct spdk_srv_transport_poll_group *
+srv_transport_get_optimal_poll_group(struct spdk_srv_transport *transport,
+									 struct spdk_srv_conn *conn)
+{
+	if (transport->ops->get_optimal_poll_group)
+	{
+		return transport->ops->get_optimal_poll_group(conn);
+	}
+	else
+	{
+		return NULL;
+	}
+}
+
+void srv_transport_poll_group_destroy(struct spdk_srv_transport_poll_group *group)
+{
+	struct spdk_srv_transport_pg_cache_buf *buf, *tmp;
+
+	if (!STAILQ_EMPTY(&group->pending_buf_queue))
+	{
+		SPDK_ERRLOG("Pending I/O list wasn't empty on poll group destruction\n");
+	}
+
+	STAILQ_FOREACH_SAFE(buf, &group->buf_cache, link, tmp)
+	{
+		STAILQ_REMOVE(&group->buf_cache, buf, spdk_srv_transport_pg_cache_buf, link);
+		spdk_mempool_put(group->transport->data_buf_pool, buf);
+	}
+	group->transport->ops->poll_group_destroy(group);
+}
+
+int srv_transport_poll_group_add(struct spdk_srv_transport_poll_group *group,
+								 struct spdk_srv_conn *conn)
+{
+	if (conn->transport)
+	{
+		assert(conn->transport == group->transport);
+		if (conn->transport != group->transport)
+		{
+			return -1;
+		}
+	}
+	else
+	{
+		conn->transport = group->transport;
+	}
+
+	SPDK_DTRACE_PROBE3(srv_transport_poll_group_add, conn, conn->qid,
+					   spdk_thread_get_id(group->group->thread));
+
+	return group->transport->ops->poll_group_add(group, conn);
+}
+
+int srv_transport_poll_group_remove(struct spdk_srv_transport_poll_group *group,
+									struct spdk_srv_conn *conn)
+{
+	int rc = ENOTSUP;
+
+	SPDK_DTRACE_PROBE3(srv_transport_poll_group_remove, conn, conn->qid,
+					   spdk_thread_get_id(group->group->thread));
+
+	assert(conn->transport == group->transport);
+	if (group->transport->ops->poll_group_remove)
+	{
+		rc = group->transport->ops->poll_group_remove(group, conn);
+	}
+
+	return rc;
+}
+
+int srv_transport_poll_group_poll(struct spdk_srv_transport_poll_group *group)
+{
+	return group->transport->ops->poll_group_poll(group);
+}
+
+static int
+srv_transport_req_free(struct spdk_srv_request *req)
+{
+	return req->conn->transport->ops->req_free(req);
+}
+
+static int
+srv_transport_req_complete(struct spdk_srv_request *req)
+{
+	return req->conn->transport->ops->req_complete(req);
+}
+
+static void
+srv_transport_conn_fini(struct spdk_srv_conn *conn,
+						spdk_srv_transport_conn_fini_cb cb_fn,
+						void *cb_arg)
+{
+	SPDK_DTRACE_PROBE1(srv_transport_conn_fini, conn);
+
+	conn->transport->ops->conn_fini(conn, cb_fn, cb_arg);
+}
+
+int srv_transport_conn_get_peer_trid(struct spdk_srv_conn *conn,
+									 struct spdk_srv_transport_id *trid)
+{
+	return conn->transport->ops->conn_get_peer_trid(conn, trid);
+}
+
+int srv_transport_conn_get_local_trid(struct spdk_srv_conn *conn,
+									  struct spdk_srv_transport_id *trid)
+{
+	return conn->transport->ops->conn_get_local_trid(conn, trid);
+}
+
+int srv_transport_conn_get_listen_trid(struct spdk_srv_conn *conn,
+									   struct spdk_srv_transport_id *trid)
+{
+	return conn->transport->ops->conn_get_listen_trid(conn, trid);
+}
+
+static void
+srv_transport_conn_abort_request(struct spdk_srv_conn *conn,
+								 struct spdk_srv_request *req)
+{
+	if (conn->transport->ops->conn_abort_request)
+	{
+		conn->transport->ops->conn_abort_request(conn, req);
+	}
+}
+
+bool spdk_srv_transport_opts_init(const char *transport_name,
+								  struct spdk_srv_transport_opts *opts, size_t opts_size)
+{
+	const struct spdk_srv_transport_ops *ops;
+	struct spdk_srv_transport_opts opts_local = {};
+
+	ops = srv_get_transport_ops(transport_name);
+	if (!ops)
+	{
+		SPDK_ERRLOG("Transport type %s unavailable.\n", transport_name);
+		return false;
+	}
+
+	if (!opts)
+	{
+		SPDK_ERRLOG("opts should not be NULL\n");
+		return false;
+	}
+
+	if (!opts_size)
+	{
+		SPDK_ERRLOG("opts_size inside opts should not be zero value\n");
+		return false;
+	}
+
+	opts_local.association_timeout = SRV_TRANSPORT_DEFAULT_ASSOCIATION_TIMEOUT_IN_MS;
+	opts_local.acceptor_poll_rate = SPDK_SRV_DEFAULT_ACCEPT_POLL_RATE_US;
+	ops->opts_init(&opts_local);
+
+	srv_transport_opts_copy(opts, &opts_local, opts_size);
+
+	return true;
+}
+
+static int
+cmp_int(int a, int b)
+{
+	return a - b;
+}
+
+int spdk_srv_transport_id_compare(const struct spdk_srv_transport_id *trid1,
+								  const struct spdk_srv_transport_id *trid2)
+{
+	int cmp;
+
+	if (trid1->trtype == SPDK_SRV_TRANSPORT_CUSTOM)
+	{
+		cmp = strcasecmp(trid1->trstring, trid2->trstring);
+	}
+	else
+	{
+		cmp = cmp_int(trid1->trtype, trid2->trtype);
+	}
+
+	if (cmp)
+	{
+		return cmp;
+	}
+
+	cmp = strcasecmp(trid1->traddr, trid2->traddr);
+	if (cmp)
+	{
+		return cmp;
+	}
+
+	cmp = cmp_int(trid1->adrfam, trid2->adrfam);
+	if (cmp)
+	{
+		return cmp;
+	}
+
+	cmp = strcasecmp(trid1->trsvcid, trid2->trsvcid);
+	if (cmp)
+	{
+		return cmp;
+	}
+
+	return 0;
+}
+
+void spdk_srv_request_free_buffers(struct spdk_srv_request *req,
+								   struct spdk_srv_transport_poll_group *group,
+								   struct spdk_srv_transport *transport)
+{
+	uint32_t i;
+
+	for (i = 0; i < req->iovcnt; i++)
+	{
+		if (group->buf_cache_count < group->buf_cache_size)
+		{
+			STAILQ_INSERT_HEAD(&group->buf_cache,
+							   (struct spdk_srv_transport_pg_cache_buf *)req->buffers[i],
+							   link);
+			group->buf_cache_count++;
+		}
+		else
+		{
+			spdk_mempool_put(transport->data_buf_pool, req->buffers[i]);
+		}
+		req->iov[i].iov_base = NULL;
+		req->buffers[i] = NULL;
+		req->iov[i].iov_len = 0;
+	}
+	req->data_from_pool = false;
+}
+
+static inline int
+srv_request_set_buffer(struct spdk_srv_request *req, void *buf, uint32_t length,
+					   uint32_t io_unit_size)
+{
+	req->buffers[req->iovcnt] = buf;
+	req->iov[req->iovcnt].iov_base = (void *)((uintptr_t)(buf + SRV_DATA_BUFFER_MASK) &
+											  ~SRV_DATA_BUFFER_MASK);
+	req->iov[req->iovcnt].iov_len = spdk_min(length, io_unit_size);
+	length -= req->iov[req->iovcnt].iov_len;
+	req->iovcnt++;
+
+	return length;
+}
+
+static int
+srv_request_get_buffers(struct spdk_srv_request *req,
+						struct spdk_srv_transport_poll_group *group,
+						struct spdk_srv_transport *transport,
+						uint32_t length)
+{
+	uint32_t io_unit_size = transport->opts.io_unit_size;
+	uint32_t num_buffers;
+	uint32_t i = 0, j;
+	void *buffer, *buffers[SRV_REQ_MAX_BUFFERS];
+
+	/* If the number of buffers is too large, then we know the I/O is larger than allowed.
+	 *  Fail it.
+	 */
+	num_buffers = SPDK_CEIL_DIV(length, io_unit_size);
+	if (num_buffers > SRV_REQ_MAX_BUFFERS)
+	{
+		return -EINVAL;
+	}
+
+	while (i < num_buffers)
+	{
+		if (!(STAILQ_EMPTY(&group->buf_cache)))
+		{
+			group->buf_cache_count--;
+			buffer = STAILQ_FIRST(&group->buf_cache);
+			STAILQ_REMOVE_HEAD(&group->buf_cache, link);
+			assert(buffer != NULL);
+
+			length = srv_request_set_buffer(req, buffer, length, io_unit_size);
+			i++;
+		}
+		else
+		{
+			if (spdk_mempool_get_bulk(transport->data_buf_pool, buffers,
+									  num_buffers - i))
+			{
+				return -ENOMEM;
+			}
+			for (j = 0; j < num_buffers - i; j++)
+			{
+				length = srv_request_set_buffer(req, buffers[j], length, io_unit_size);
+			}
+			i += num_buffers - i;
+		}
+	}
+
+	assert(length == 0);
+
+	req->data_from_pool = true;
+	return 0;
+}
+
+int spdk_srv_request_get_buffers(struct spdk_srv_request *req,
+								 struct spdk_srv_transport_poll_group *group,
+								 struct spdk_srv_transport *transport,
+								 uint32_t length)
+{
+	int rc;
+
+	req->iovcnt = 0;
+
+	rc = srv_request_get_buffers(req, group, transport, length);
+	if (rc == -ENOMEM)
+	{
+		spdk_srv_request_free_buffers(req, group, transport);
+	}
+
+	return rc;
+}
+
+static void
+srv_conn_request_cleanup(struct spdk_srv_conn *conn)
+{
+	if (conn->state == SPDK_SRV_CONN_DEACTIVATING)
+	{
+		assert(conn->state_cb != NULL);
+
+		if (TAILQ_EMPTY(&conn->outstanding))
+		{
+			conn->state_cb(conn->state_cb_arg, 0);
+		}
+	}
+}
+
+static void
+_srv_request_complete(void *ctx)
+{
+	struct spdk_srv_request *req = ctx;
+	struct spdk_req_cpl *rsp = req->rsp;
+	struct spdk_srv_conn *conn;
+	struct spdk_srv_subsystem_poll_group *sgroup = NULL;
+	struct spdk_srv_subsystem_pg_ns_info *ns_info;
+	bool is_aer = false;
+	uint32_t nsid;
+	bool paused;
+	uint8_t opcode;
+
+	rsp->sqid = 0;
+	rsp->status.p = 0;
+	rsp->cid = req->cmd->cid;
+	opcode = req->cmd->opc;
+
+	conn = req->conn;
+
+	if (srv_transport_req_complete(req))
+	{
+		SPDK_ERRLOG("Transport request completion error!\n");
+	}
+
+	srv_conn_request_cleanup(conn);
+}
+
+void spdk_srv_request_exec(struct spdk_srv_request *req)
+{
+	struct spdk_srv_conn *conn = req->conn;
+	struct spdk_srv_transport *transport = conn->transport;
+	enum spdk_srv_request_exec_status status;
+
+	// TODO: handle req
+	status = SPDK_SRV_REQUEST_EXEC_STATUS_COMPLETE;
+	SPDK_DEBUGLOG(srv, "handle a request complete\n");
+
+	if (status == SPDK_SRV_REQUEST_EXEC_STATUS_COMPLETE)
+	{
+		_srv_request_complete(req);
+	}
+}
+
+int spdk_srv_request_complete(struct spdk_srv_request *req)
+{
+	struct spdk_srv_conn *conn = req->conn;
+
+	if (spdk_likely(conn->group->thread == spdk_get_thread()))
+	{
+		_srv_request_complete(req);
+	}
+	else
+	{
+		spdk_thread_send_msg(conn->group->thread,
+							 _srv_request_complete, req);
+	}
+
+	return 0;
+}
+
+void spdk_srv_trid_populate_transport(struct spdk_srv_transport_id *trid,
+									  enum spdk_srv_transport_type trtype)
+{
+	const char *trstring = "";
+
+	trid->trtype = trtype;
+	switch (trtype)
+	{
+	case SPDK_SRV_TRANSPORT_RDMA:
+		trstring = SPDK_SRV_TRANSPORT_NAME_RDMA;
+		break;
+	case SPDK_SRV_TRANSPORT_TCP:
+		trstring = SPDK_SRV_TRANSPORT_NAME_TCP;
+		break;
+	case SPDK_SRV_TRANSPORT_CUSTOM:
+		trstring = SPDK_SRV_TRANSPORT_NAME_CUSTOM;
+		break;
+	default:
+		SPDK_ERRLOG("no available transports\n");
+		assert(0);
+		return;
+	}
+	snprintf(trid->trstring, SPDK_SRV_TRSTRING_MAX_LEN, "%s", trstring);
+}
+
+/* supplied to a single call to srv_conn_disconnect */
+struct srv_conn_disconnect_ctx
+{
+	struct spdk_srv_conn *conn;
+	srv_conn_disconnect_cb cb_fn;
+	struct spdk_thread *thread;
+	void *ctx;
+	uint16_t qid;
+};
+
+static void
+_srv_transport_conn_fini_complete(void *cb_ctx)
+{
+	struct srv_conn_disconnect_ctx *conn_ctx = cb_ctx;
+	/* Store cb args since cb_ctx can be freed in _srv_ctrlr_free_from_conn */
+	srv_conn_disconnect_cb cb_fn = conn_ctx->cb_fn;
+	void *cb_arg = conn_ctx->ctx;
+	struct spdk_thread *cb_thread = conn_ctx->thread;
+
+	SPDK_DEBUGLOG(srv, "Finish destroying qid %u\n", conn_ctx->qid);
+
+	free(conn_ctx);
+
+	if (cb_fn)
+	{
+		spdk_thread_send_msg(cb_thread, cb_fn, cb_arg);
+	}
+}
+
+static void
+_srv_conn_destroy(void *ctx, int status)
+{
+	struct srv_conn_disconnect_ctx *conn_ctx = ctx;
+	struct spdk_srv_conn *conn = conn_ctx->conn;
+	struct spdk_srv_request *req, *tmp;
+	struct spdk_srv_subsystem_poll_group *sgroup;
+
+	assert(conn->state == SPDK_SRV_CONN_DEACTIVATING);
+	conn_ctx->qid = conn->qid;
+
+	spdk_srv_poll_group_remove(conn);
+	srv_transport_conn_fini(conn, _srv_transport_conn_fini_complete, conn_ctx);
+}
+
+static void
+_srv_conn_disconnect_msg(void *ctx)
+{
+	struct srv_conn_disconnect_ctx *conn_ctx = ctx;
+
+	spdk_srv_conn_disconnect(conn_ctx->conn, conn_ctx->cb_fn, conn_ctx->ctx);
+	free(ctx);
+}
+
+void srv_conn_set_state(struct spdk_srv_conn *conn,
+						enum spdk_srv_conn_state state)
+{
+	assert(conn != NULL);
+	assert(conn->group->thread == spdk_get_thread());
+
+	conn->state = state;
+}
+
+int spdk_srv_conn_disconnect(struct spdk_srv_conn *conn, srv_conn_disconnect_cb cb_fn, void *ctx)
+{
+	struct spdk_srv_poll_group *group = conn->group;
+	struct srv_conn_disconnect_ctx *conn_ctx;
+
+	if (__atomic_test_and_set(&conn->disconnect_started, __ATOMIC_RELAXED))
+	{
+		if (cb_fn)
+		{
+			cb_fn(ctx);
+		}
+		return 0;
+	}
+
+	/* If we get a conn in the uninitialized state, we can just destroy it immediately */
+	if (conn->state == SPDK_SRV_CONN_UNINITIALIZED)
+	{
+		srv_transport_conn_fini(conn, NULL, NULL);
+		if (cb_fn)
+		{
+			cb_fn(ctx);
+		}
+		return 0;
+	}
+
+	assert(group != NULL);
+	if (spdk_get_thread() != group->thread)
+	{
+		/* clear the atomic so we can set it on the next call on the proper thread. */
+		__atomic_clear(&conn->disconnect_started, __ATOMIC_RELAXED);
+		conn_ctx = calloc(1, sizeof(struct srv_conn_disconnect_ctx));
+		if (!conn_ctx)
+		{
+			SPDK_ERRLOG("Unable to allocate context for srv_conn_disconnect\n");
+			return -ENOMEM;
+		}
+		conn_ctx->conn = conn;
+		conn_ctx->cb_fn = cb_fn;
+		conn_ctx->thread = group->thread;
+		conn_ctx->ctx = ctx;
+		spdk_thread_send_msg(group->thread, _srv_conn_disconnect_msg, conn_ctx);
+		return 0;
+	}
+
+	SPDK_DTRACE_PROBE2(srv_conn_disconnect, conn, spdk_thread_get_id(group->thread));
+	assert(conn->state == SPDK_SRV_CONN_ACTIVE);
+	srv_conn_set_state(conn, SPDK_SRV_CONN_DEACTIVATING);
+
+	conn_ctx = calloc(1, sizeof(struct srv_conn_disconnect_ctx));
+	if (!conn_ctx)
+	{
+		SPDK_ERRLOG("Unable to allocate context for srv_conn_disconnect\n");
+		return -ENOMEM;
+	}
+
+	conn_ctx->conn = conn;
+	conn_ctx->cb_fn = cb_fn;
+	conn_ctx->thread = group->thread;
+	conn_ctx->ctx = ctx;
+
+	/* Check for outstanding I/O */
+	if (!TAILQ_EMPTY(&conn->outstanding))
+	{
+		SPDK_DTRACE_PROBE2(srv_poll_group_drain_conn, conn, spdk_thread_get_id(group->thread));
+		conn->state_cb = _srv_conn_destroy;
+		conn->state_cb_arg = conn_ctx;
+		return 0;
+	}
+
+	_srv_conn_destroy(conn_ctx, 0);
+
+	return 0;
+}
\ No newline at end of file
diff --git a/lib/rdma_server/utils.c b/lib/rdma_server/utils.c
new file mode 100644
index 000000000..bb7ae60a6
--- /dev/null
+++ b/lib/rdma_server/utils.c
@@ -0,0 +1,487 @@
+#include "spdk/stdinc.h"
+#include "spdk/log.h"
+#include "spdk/string.h"
+#include "spdk/env.h"
+#include "spdk/rdma_client.h"
+
+void spdk_client_trid_populate_transport(struct spdk_client_transport_id *trid,
+										 enum spdk_client_transport_type trtype)
+{
+	const char *trstring = "";
+
+	trid->trtype = trtype;
+	switch (trtype)
+	{
+	case SPDK_CLIENT_TRANSPORT_PCIE:
+		trstring = SPDK_CLIENT_TRANSPORT_NAME_PCIE;
+		break;
+	case SPDK_CLIENT_TRANSPORT_RDMA:
+		trstring = SPDK_CLIENT_TRANSPORT_NAME_RDMA;
+		break;
+	case SPDK_CLIENT_TRANSPORT_TCP:
+		trstring = SPDK_CLIENT_TRANSPORT_NAME_TCP;
+		break;
+	case SPDK_CLIENT_TRANSPORT_VFIOUSER:
+		trstring = SPDK_CLIENT_TRANSPORT_NAME_VFIOUSER;
+		break;
+	case SPDK_CLIENT_TRANSPORT_CUSTOM:
+		trstring = SPDK_CLIENT_TRANSPORT_NAME_CUSTOM;
+		break;
+	default:
+		SPDK_ERRLOG("no available transports\n");
+		assert(0);
+		return;
+	}
+	snprintf(trid->trstring, SPDK_SRV_TRSTRING_MAX_LEN, "%s", trstring);
+}
+
+int spdk_client_transport_id_populate_trstring(struct spdk_client_transport_id *trid, const char *trstring)
+{
+	int len, i, rc;
+
+	if (trstring == NULL)
+	{
+		return -EINVAL;
+	}
+
+	len = strnlen(trstring, SPDK_SRV_TRSTRING_MAX_LEN);
+	if (len == SPDK_SRV_TRSTRING_MAX_LEN)
+	{
+		return -EINVAL;
+	}
+
+	rc = snprintf(trid->trstring, SPDK_SRV_TRSTRING_MAX_LEN, "%s", trstring);
+	if (rc < 0)
+	{
+		return rc;
+	}
+
+	/* cast official trstring to uppercase version of input. */
+	for (i = 0; i < len; i++)
+	{
+		trid->trstring[i] = toupper(trid->trstring[i]);
+	}
+	return 0;
+}
+
+int spdk_client_transport_id_parse_trtype(enum spdk_client_transport_type *trtype, const char *str)
+{
+	if (trtype == NULL || str == NULL)
+	{
+		return -EINVAL;
+	}
+
+	if (strcasecmp(str, "PCIe") == 0)
+	{
+		*trtype = SPDK_CLIENT_TRANSPORT_PCIE;
+	}
+	else if (strcasecmp(str, "RDMA") == 0)
+	{
+		*trtype = SPDK_CLIENT_TRANSPORT_RDMA;
+	}
+	else if (strcasecmp(str, "TCP") == 0)
+	{
+		*trtype = SPDK_CLIENT_TRANSPORT_TCP;
+	}
+	else if (strcasecmp(str, "VFIOUSER") == 0)
+	{
+		*trtype = SPDK_CLIENT_TRANSPORT_VFIOUSER;
+	}
+	else
+	{
+		*trtype = SPDK_CLIENT_TRANSPORT_CUSTOM;
+	}
+	return 0;
+}
+
+const char *
+spdk_client_transport_id_trtype_str(enum spdk_client_transport_type trtype)
+{
+	switch (trtype)
+	{
+	case SPDK_CLIENT_TRANSPORT_PCIE:
+		return "PCIe";
+	case SPDK_CLIENT_TRANSPORT_RDMA:
+		return "RDMA";
+	case SPDK_CLIENT_TRANSPORT_TCP:
+		return "TCP";
+	case SPDK_CLIENT_TRANSPORT_VFIOUSER:
+		return "VFIOUSER";
+	case SPDK_CLIENT_TRANSPORT_CUSTOM:
+		return "CUSTOM";
+	default:
+		return NULL;
+	}
+}
+
+int spdk_client_transport_id_parse_adrfam(enum spdk_srv_adrfam *adrfam, const char *str)
+{
+	if (adrfam == NULL || str == NULL)
+	{
+		return -EINVAL;
+	}
+
+	if (strcasecmp(str, "IPv4") == 0)
+	{
+		*adrfam = SPDK_SRV_ADRFAM_IPV4;
+	}
+	else if (strcasecmp(str, "IPv6") == 0)
+	{
+		*adrfam = SPDK_SRV_ADRFAM_IPV6;
+	}
+	else if (strcasecmp(str, "IB") == 0)
+	{
+		*adrfam = SPDK_SRV_ADRFAM_IB;
+	}
+	else if (strcasecmp(str, "FC") == 0)
+	{
+		*adrfam = SPDK_SRV_ADRFAM_FC;
+	}
+	else
+	{
+		return -ENOENT;
+	}
+	return 0;
+}
+
+const char *
+spdk_client_transport_id_adrfam_str(enum spdk_srv_adrfam adrfam)
+{
+	switch (adrfam)
+	{
+	case SPDK_SRV_ADRFAM_IPV4:
+		return "IPv4";
+	case SPDK_SRV_ADRFAM_IPV6:
+		return "IPv6";
+	case SPDK_SRV_ADRFAM_IB:
+		return "IB";
+	case SPDK_SRV_ADRFAM_FC:
+		return "FC";
+	default:
+		return NULL;
+	}
+}
+
+static size_t
+parse_next_key(const char **str, char *key, char *val, size_t key_buf_size, size_t val_buf_size)
+{
+
+	const char *sep, *sep1;
+	const char *whitespace = " \t\n";
+	size_t key_len, val_len;
+
+	*str += strspn(*str, whitespace);
+
+	sep = strchr(*str, ':');
+	if (!sep)
+	{
+		sep = strchr(*str, '=');
+		if (!sep)
+		{
+			SPDK_ERRLOG("Key without ':' or '=' separator\n");
+			return 0;
+		}
+	}
+	else
+	{
+		sep1 = strchr(*str, '=');
+		if ((sep1 != NULL) && (sep1 < sep))
+		{
+			sep = sep1;
+		}
+	}
+
+	key_len = sep - *str;
+	if (key_len >= key_buf_size)
+	{
+		SPDK_ERRLOG("Key length %zu greater than maximum allowed %zu\n",
+					key_len, key_buf_size - 1);
+		return 0;
+	}
+
+	memcpy(key, *str, key_len);
+	key[key_len] = '\0';
+
+	*str += key_len + 1; /* Skip key: */
+	val_len = strcspn(*str, whitespace);
+	if (val_len == 0)
+	{
+		SPDK_ERRLOG("Key without value\n");
+		return 0;
+	}
+
+	if (val_len >= val_buf_size)
+	{
+		SPDK_ERRLOG("Value length %zu greater than maximum allowed %zu\n",
+					val_len, val_buf_size - 1);
+		return 0;
+	}
+
+	memcpy(val, *str, val_len);
+	val[val_len] = '\0';
+
+	*str += val_len;
+
+	return val_len;
+}
+
+int spdk_client_transport_id_parse(struct spdk_client_transport_id *trid, const char *str)
+{
+	size_t val_len;
+	char key[32];
+	char val[1024];
+
+	if (trid == NULL || str == NULL)
+	{
+		return -EINVAL;
+	}
+
+	while (*str != '\0')
+	{
+
+		val_len = parse_next_key(&str, key, val, sizeof(key), sizeof(val));
+
+		if (val_len == 0)
+		{
+			SPDK_ERRLOG("Failed to parse transport ID\n");
+			return -EINVAL;
+		}
+
+		if (strcasecmp(key, "trtype") == 0)
+		{
+			if (spdk_client_transport_id_populate_trstring(trid, val) != 0)
+			{
+				SPDK_ERRLOG("invalid transport '%s'\n", val);
+				return -EINVAL;
+			}
+			if (spdk_client_transport_id_parse_trtype(&trid->trtype, val) != 0)
+			{
+				SPDK_ERRLOG("Unknown trtype '%s'\n", val);
+				return -EINVAL;
+			}
+		}
+		else if (strcasecmp(key, "adrfam") == 0)
+		{
+			if (spdk_client_transport_id_parse_adrfam(&trid->adrfam, val) != 0)
+			{
+				SPDK_ERRLOG("Unknown adrfam '%s'\n", val);
+				return -EINVAL;
+			}
+		}
+		else if (strcasecmp(key, "traddr") == 0)
+		{
+			if (val_len > SPDK_SRV_TRADDR_MAX_LEN)
+			{
+				SPDK_ERRLOG("traddr length %zu greater than maximum allowed %u\n",
+							val_len, SPDK_SRV_TRADDR_MAX_LEN);
+				return -EINVAL;
+			}
+			memcpy(trid->traddr, val, val_len + 1);
+		}
+		else if (strcasecmp(key, "trsvcid") == 0)
+		{
+			if (val_len > SPDK_SRV_TRSVCID_MAX_LEN)
+			{
+				SPDK_ERRLOG("trsvcid length %zu greater than maximum allowed %u\n",
+							val_len, SPDK_SRV_TRSVCID_MAX_LEN);
+				return -EINVAL;
+			}
+			memcpy(trid->trsvcid, val, val_len + 1);
+		}
+		else if (strcasecmp(key, "priority") == 0)
+		{
+			if (val_len > SPDK_SRV_PRIORITY_MAX_LEN)
+			{
+				SPDK_ERRLOG("priority length %zu greater than maximum allowed %u\n",
+							val_len, SPDK_SRV_PRIORITY_MAX_LEN);
+				return -EINVAL;
+			}
+			trid->priority = spdk_strtol(val, 10);
+		}
+		else if (strcasecmp(key, "hostaddr") == 0)
+		{
+			continue;
+		}
+		else if (strcasecmp(key, "hostsvcid") == 0)
+		{
+			continue;
+		}
+		else if (strcasecmp(key, "hostnqn") == 0)
+		{
+			continue;
+		}
+		else if (strcasecmp(key, "ns") == 0)
+		{
+			/*
+			 * Special case.  The namespace id parameter may
+			 * optionally be passed in the transport id string
+			 * for an SPDK application (e.g. client/perf)
+			 * and additionally parsed therein to limit
+			 * targeting a specific namespace.  For this
+			 * scenario, just silently ignore this key
+			 * rather than letting it default to logging
+			 * it as an invalid key.
+			 */
+			continue;
+		}
+		else if (strcasecmp(key, "alt_traddr") == 0)
+		{
+			/*
+			 * Used by applications for enabling transport ID failover.
+			 * Please see the case above for more information on custom parameters.
+			 */
+			continue;
+		}
+		else
+		{
+			SPDK_ERRLOG("Unknown transport ID key '%s'\n", key);
+		}
+	}
+
+	return 0;
+}
+
+int spdk_client_host_id_parse(struct spdk_client_host_id *hostid, const char *str)
+{
+
+	size_t key_size = 32;
+	size_t val_size = 1024;
+	size_t val_len;
+	char key[key_size];
+	char val[val_size];
+
+	if (hostid == NULL || str == NULL)
+	{
+		return -EINVAL;
+	}
+
+	while (*str != '\0')
+	{
+
+		val_len = parse_next_key(&str, key, val, key_size, val_size);
+
+		if (val_len == 0)
+		{
+			SPDK_ERRLOG("Failed to parse host ID\n");
+			return val_len;
+		}
+
+		/* Ignore the rest of the options from the transport ID. */
+		if (strcasecmp(key, "trtype") == 0)
+		{
+			continue;
+		}
+		else if (strcasecmp(key, "adrfam") == 0)
+		{
+			continue;
+		}
+		else if (strcasecmp(key, "traddr") == 0)
+		{
+			continue;
+		}
+		else if (strcasecmp(key, "trsvcid") == 0)
+		{
+			continue;
+		}
+		else if (strcasecmp(key, "subnqn") == 0)
+		{
+			continue;
+		}
+		else if (strcasecmp(key, "priority") == 0)
+		{
+			continue;
+		}
+		else if (strcasecmp(key, "ns") == 0)
+		{
+			continue;
+		}
+		else if (strcasecmp(key, "hostaddr") == 0)
+		{
+			if (val_len > SPDK_SRV_TRADDR_MAX_LEN)
+			{
+				SPDK_ERRLOG("hostaddr length %zu greater than maximum allowed %u\n",
+							val_len, SPDK_SRV_TRADDR_MAX_LEN);
+				return -EINVAL;
+			}
+			memcpy(hostid->hostaddr, val, val_len + 1);
+		}
+		else if (strcasecmp(key, "hostsvcid") == 0)
+		{
+			if (val_len > SPDK_SRV_TRSVCID_MAX_LEN)
+			{
+				SPDK_ERRLOG("trsvcid length %zu greater than maximum allowed %u\n",
+							val_len, SPDK_SRV_TRSVCID_MAX_LEN);
+				return -EINVAL;
+			}
+			memcpy(hostid->hostsvcid, val, val_len + 1);
+		}
+		else
+		{
+			SPDK_ERRLOG("Unknown transport ID key '%s'\n", key);
+		}
+	}
+
+	return 0;
+}
+
+static int
+cmp_int(int a, int b)
+{
+	return a - b;
+}
+
+int spdk_client_transport_id_compare(const struct spdk_client_transport_id *trid1,
+									 const struct spdk_client_transport_id *trid2)
+{
+	int cmp;
+
+	if (trid1->trtype == SPDK_CLIENT_TRANSPORT_CUSTOM)
+	{
+		cmp = strcasecmp(trid1->trstring, trid2->trstring);
+	}
+	else
+	{
+		cmp = cmp_int(trid1->trtype, trid2->trtype);
+	}
+
+	if (cmp)
+	{
+		return cmp;
+	}
+
+	if (trid1->trtype == SPDK_CLIENT_TRANSPORT_PCIE)
+	{
+		struct spdk_pci_addr pci_addr1 = {};
+		struct spdk_pci_addr pci_addr2 = {};
+
+		/* Normalize PCI addresses before comparing */
+		if (spdk_pci_addr_parse(&pci_addr1, trid1->traddr) < 0 ||
+			spdk_pci_addr_parse(&pci_addr2, trid2->traddr) < 0)
+		{
+			return -1;
+		}
+
+		/* PCIe transport ID only uses trtype and traddr */
+		return spdk_pci_addr_compare(&pci_addr1, &pci_addr2);
+	}
+
+	cmp = strcasecmp(trid1->traddr, trid2->traddr);
+	if (cmp)
+	{
+		return cmp;
+	}
+
+	cmp = cmp_int(trid1->adrfam, trid2->adrfam);
+	if (cmp)
+	{
+		return cmp;
+	}
+
+	cmp = strcasecmp(trid1->trsvcid, trid2->trsvcid);
+	if (cmp)
+	{
+		return cmp;
+	}
+
+	return 0;
+}
diff --git a/python/spdk/rpc/bdev.py b/python/spdk/rpc/bdev.py
index c722e7d68..4597d9fcf 100644
--- a/python/spdk/rpc/bdev.py
+++ b/python/spdk/rpc/bdev.py
@@ -897,6 +897,91 @@ def bdev_zone_block_delete(client, name):
     params = {'name': name}
     return client.call('bdev_zone_block_delete', params)
 
+def size_str_to_num(size):
+    match = False
+    units = {
+        'GB': 1024 * 1024 * 1024,
+        'G' : 1024 * 1024 * 1024,
+        'M' : 1024 * 1024,
+        'MB': 1024 * 1024, 
+        'K' : 1024,
+        'KB': 1024,
+        'B' : 1
+    }
+    for key, val in units.items():
+        if size.upper().endswith(key):
+            size = size[:-1*len(key)]
+            if (len(size) == 0):
+                return 0
+            try:
+                size = int(size) * val
+            except:
+                return 0
+            match=True
+            break
+    if(match is False):
+        return 0
+    return size
+
+@deprecated_alias('construct_fastblock_bdev')
+def bdev_fastblock_create(client, pool_id, pool_name, image_name, block_size, image_size, monitor_address, name, object_size=0):
+    """Create a fastblock image block device.
+
+    Args:
+        pool_id: fastblock pool id
+        pool_name: fastblock pool name
+        image_name: fastblock image name
+        block_size: block size of RBD volume
+        image_size: image size , The units it follows: GB, G, M, MB, K, KB, B
+        object_size: object size
+        monitor_address: monitor_address
+        name: name of block device (optional)
+
+    Returns:
+        Name of created block device.
+    """
+    image_size = size_str_to_num(image_size)
+    if image_size == 0:
+        return "Illegal image_size"
+        
+    params = {
+        'pool_id': pool_id,
+        'pool_name': pool_name,
+        'image_name': image_name,
+        'block_size': block_size,
+        'image_size': image_size,
+        'object_size': object_size,
+        'monitor_address': monitor_address,
+        'name': name
+    }
+
+    return client.call('bdev_fastblock_create', params)
+
+@deprecated_alias('delete_fastblock_bdev')
+def bdev_fastblock_delete(client, name):
+    """Remove fastblock bdev from the system.
+
+    Args:
+        name: name of fastblock bdev to delete
+    """
+    params = {'name': name}
+    return client.call('bdev_fastblock_delete', params)
+
+def bdev_fastblock_resize(client, name, new_size):
+    """Resize fastblock bdev in the system.
+
+    Args:
+        name: name of fastblock bdev to resize
+        new_size: new bdev size of resize operation, The units it follows: GB, G, M, MB, K, KB, B
+    """
+    new_size = size_str_to_num(new_size)
+    if new_size == 0:
+        return "Illegal new_size"
+    params = {
+            'name': name,
+            'new_size': new_size,
+            }
+    return client.call('bdev_fastblock_resize', params)
 
 def bdev_rbd_register_cluster(client, name, user=None, config_param=None, config_file=None, key_file=None):
     """Create a Rados Cluster object of the Ceph RBD backend.
diff --git a/scripts/rpc.py b/scripts/rpc.py
index 515106b4a..1083cba66 100755
--- a/scripts/rpc.py
+++ b/scripts/rpc.py
@@ -826,6 +826,50 @@ if __name__ == "__main__":
     p.add_argument('name', help='Virtual zone bdev name')
     p.set_defaults(func=bdev_zone_block_delete)
 
+    def bdev_fastblock_create(args):
+        print_json(rpc.bdev.bdev_fastblock_create(args.client,
+                                            name=args.name,
+                                            pool_id=args.pool_id,
+                                            pool_name=args.pool_name,
+                                            image_name=args.image_name,
+                                            block_size=args.block_size,
+                                            image_size=args.image_size,
+                                            object_size=args.object_size,
+                                            monitor_address=args.monitor_address))
+
+    p = subparsers.add_parser('bdev_fastblock_create', aliases=['construct_fastblock_bdev'],
+                              help='Add a bdev with fastblock image backend')
+    p.add_argument('-b', '--name', help="Name of the bdev")
+    p.add_argument('-S', '--object_size', help='object size', required=False, type=int, default=1024*4096)
+    p.add_argument('-P', '--pool_id', help='pool id', type=int)
+    p.add_argument('-p', '--pool_name', help='pool name')
+    p.add_argument('-i', '--image_name', help='image name')
+    p.add_argument('-k', '--block_size', help='image block size', required=False, type=int, default=4096)
+    p.add_argument('-I', '--image_size', help='image size, The units it follows: GB, G, M, MB, K, KB, B')
+    p.add_argument('-m', '--monitor_address', help='monitor address')
+    p.set_defaults(func=bdev_fastblock_create)
+
+    def bdev_fastblock_delete(args):
+        rpc.bdev.bdev_fastblock_delete(args.client,
+                                 name=args.name)
+
+    p = subparsers.add_parser('bdev_fastblock_delete', aliases=['delete_fastblock_bdev'],
+                              help='Delete a fastblock image bdev')
+    p.add_argument('name', help='fastblock bdev name')
+    p.set_defaults(func=bdev_fastblock_delete)
+
+    def bdev_fastblock_resize(args):
+        print_json(rpc.bdev.bdev_fastblock_resize(args.client,
+                                 name=args.name,
+                                 new_size=args.new_size))
+
+    p = subparsers.add_parser('bdev_fastblock_resize',
+                              help='Resize a fastblock image bdev')
+    p.add_argument('name', help='fastblock bdev name')
+    p.add_argument('new_size', help='new bdev size for resize operation, The units it follows: GB, G, M, MB, K, KB, B')
+    p.set_defaults(func=bdev_fastblock_resize)
+
+
     def bdev_rbd_register_cluster(args):
         config_param = None
         if args.config_param:
-- 
2.32.0

